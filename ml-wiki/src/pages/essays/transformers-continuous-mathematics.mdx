import EssayLayout from "../../components/EssayLayout";

export const meta = {
  title: "Transformers as Continuous Mathematics: A Journey from Discrete Layers to Differential Equations",
  description: "An exploration of how Transformers can be rigorously understood as discretizations of continuous integro-differential equations, revealing the deep mathematical structure underlying modern AI.",
  readingTime: "25 min read",
  publishDate: "2026-01-15",
  audioPath: "/audio/transformers-continuous-mathematics.mp3",
  relatedPapers: [
    { title: "A Mathematical Explanation of Transformers for Large Language Models and GPTs", slug: "mathematical-transformers" },
    { title: "Attention Is All You Need", slug: "attention-is-all-you-need" }
  ],
  relatedConcepts: [
    { name: "Integro-Differential Equations for Transformers", slug: "integro-differential-equations-transformers" },
    { name: "Operator Splitting Methods", slug: "operator-splitting" },
    { name: "Transformer Architecture", slug: "transformer-architecture" }
  ]
};

<EssayLayout {...meta}>

## Prologue: The Mystery of Attention

When I first encountered the Transformer architecture in 2017, I was struck by its elegance. Here was a model that dispensed with the recurrent connections that had dominated sequence modeling, replacing them with something called "attention." The architecture worked brilliantly—powering breakthroughs from GPT to BERT to everything that came after. But why did it work?

The standard explanation goes something like this: attention allows the model to focus on relevant parts of the input, multi-head attention captures different types of relationships, and layer normalization keeps everything stable. These are useful intuitions, but they don't quite satisfy. They describe *what* the Transformer does, but not *why* this particular arrangement of operations should be so effective.

In October 2025, a team of researchers led by Xue-Cheng Tai published a paper that changed how I think about Transformers entirely [Tai et al., 2025]. They showed that the Transformer isn't just an arbitrary stack of attention and feedforward layers. It's a discretization of a continuous dynamical system—an integro-differential equation, to be precise. Every component we thought we understood—attention, normalization, feedforward networks—emerges naturally from this continuous formulation.

This essay is about that discovery. It's about seeing the Transformer not as a discrete collection of layers, but as a continuous flow of information governed by differential equations. It's a story about how mathematics reveals the hidden structure in things we thought we already understood.

## Part I: The Discrete World We Know

### Transformers as Layer Stacks

Let's start with what we know. The Transformer, as introduced by Vaswani et al. in 2017, processes sequences through a series of discrete layers [Vaswani et al., 2017, p.3]. Each layer consists of:

1. **Multi-head self-attention**: Computing relationships between all positions in the sequence
2. **Add & Normalize**: Combining with a skip connection and normalizing
3. **Feedforward network**: Two linear transformations with ReLU activation
4. **Add & Normalize**: Another skip connection and normalization

We stack these layers—6, 12, 24, sometimes hundreds—and somehow this architecture learns to understand and generate language with remarkable sophistication.

The operations are straightforward to implement. Given an input matrix **X** ∈ ℝ^(n×d) representing n tokens with d-dimensional embeddings:

**Attention**: Compute queries Q = XW^Q, keys K = XW^K, values V = XW^V. Then calculate attention scores with softmax((QK^T)/√d_k) and multiply by V.

**Normalization**: For each token, standardize its feature vector to have mean 0 and variance 1, then rescale.

**Feedforward**: Two linear layers with ReLU: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂.

These are discrete operations on discrete matrices. We apply them sequentially, layer by layer, until we reach the output. The question that nagged at me was: *Why this sequence? Why not some other arrangement?*

### The Discreteness Problem

There's something unsatisfying about viewing the Transformer purely as a discrete system. Consider these questions:

- Why do we need skip connections (residual connections) between layers? They seem essential, but why?
- Why does layer normalization appear twice in each block, in those specific locations?
- Why does increasing the number of layers improve performance, but only up to a point?
- What determines the "right" depth for a Transformer?

These questions suggest that the layer-by-layer view is missing something. The layers aren't independent modules; they're connected in a specific way that matters. But what is that structure?

## Part II: Enter the Continuous Viewpoint

### From Layers to Time

Here's the key insight from Tai et al.: What if we think of each Transformer layer not as a discrete step, but as a discretization of continuous time?

Imagine the Transformer not as 12 separate layers, but as a continuous evolution of the hidden state from time t=0 to time t=T [Tai et al., 2025, p.3]. The "layers" are just snapshots we take at regular time intervals Δt = T/12.

This isn't just a metaphor. Tai et al. show that we can write down a precise differential equation whose solution, when discretized, gives us exactly the Transformer architecture.

Let u(x, y, t) be a continuous function where:
- **x** represents the token position (where in the sequence we are)
- **y** represents the feature dimension (which component of the embedding vector)
- **t** represents time (which "layer" we're at, but now continuous)

The Transformer solves this integro-differential equation [Tai et al., 2025, p.3]:

```
u_t = ⟨γ(x,·,t;u), V(·,y,t;u)⟩_Ωx + ∂I_S1(u) + Σ_j[⟨W_j(·,y,t), u(x,·,t)⟩_Ωy + b_j(x,t)] + ∂I_S2(u)
```

Don't panic at the notation. Let's break down what each term means.

### The Three Components

The right side of this equation has three main parts, and each corresponds exactly to a component of the Transformer:

**Component I: Attention as a Non-Local Integral Operator**

The term ⟨γ(x,·,t;u), V(·,y,t;u)⟩_Ωx is attention. But now we see what attention really *is*: a non-local integral operator.

What does "non-local" mean? In most differential equations, the change in u at position x depends only on u and its derivatives at or near x. That's "local." But attention is different—the update at position x depends on the values of u at *all* positions x̃. That's "non-local" [Tai et al., 2025, p.3].

The attention score γ(x, x̃, t; u) determines how much position x̃ influences position x. It's computed by:

1. Apply integral transforms with three kernels W^Q, W^K, W^V to get query Q, key K, and value V
2. Compute γ = softmax((⟨Q(x,·), K(x̃,·)⟩)/√|Ωy|)
3. Apply γ as weights to V

This is *exactly* the self-attention mechanism from Vaswani et al. [2017, p.4]. But now we see it as an integral operator—a continuous version of the discrete matrix multiplication we're used to.

**Component II: Layer Normalization as Projection**

The term ∂I_S1(u) enforces a constraint. Specifically, it projects u onto the set S1 of functions that have specified mean σ₁ and variance σ₂² along the y direction [Tai et al., 2025, p.5].

This is layer normalization! But now we understand *why* it appears: it's a constraint that keeps the solution well-behaved as time evolves. The continuous equation naturally includes this normalization as part of its structure.

Tai et al. even prove a theorem giving the closed-form solution for this projection [Tai et al., 2025, p.9]:

```
u(x,y) = (v(x,y) - mean(v)) / √variance(v) · σ₂ + σ₁
```

This is exactly the layer normalization formula we implement in code. It emerges from the mathematics, not from heuristic design choices.

**Component III: Feedforward as Linear Transform Plus Constraint**

The final terms Σ_j[⟨W_j, u⟩ + b_j] + ∂I_S2(u) combine linear transformations with a projection onto S2 = `{u : u ≥ 0}` [Tai et al., 2025, p.6].

The linear part is obvious—these are the feedforward layers. But what's ∂I_S2(u)? It's the ReLU activation function! The projection onto non-negative functions is exactly max(u, 0) = ReLU(u) [Tai et al., 2025, p.9].

Again, we see that what we thought was an architectural choice—using ReLU activation—is actually a natural consequence of the mathematical structure.

## Part III: Discretization and Architecture

### Operator Splitting: From Continuous to Discrete

Now comes the crucial step: how do we go from the continuous equation to the discrete Transformer architecture we implement?

The answer is **operator splitting**, a classical technique in numerical analysis [Tai et al., 2025, p.7]. The idea is simple: if you have a complicated equation with multiple terms, solve each term separately in sequence.

For the Transformer equation, we split time step Δt into M substeps:

**Substep 1** (Attention): Solve u_t = ⟨γ(x,·,t;u), V(·,y,t;u)⟩_Ωx

**Substep 2** (LayerNorm): Solve u - u_prev = ∂I_S1(u)

**Substeps 3 to 2+J** (Feedforward): For each layer j, solve u - u_prev = ⟨W_j, u_prev⟩ + b_j + ∂I_S2(u)

**Substep 3+J** (Skip): Compute u = (1/2)(u_after_FF + u_after_first_norm)

**Substep 4+J** (LayerNorm): Solve u - u_prev = ∂I_S1(u)

This is called Lie splitting, and it's a first-order accurate method for solving differential equations [Tai et al., 2025, p.19].

Now here's the beautiful part: when you work out what these substeps look like in discrete form, you get *exactly* the Transformer architecture [Tai et al., 2025, p.12]:

```
Input → Attention → LayerNorm → FF Layer 1 → FF Layer 2 → Skip → LayerNorm → Output
```

With M=6 (corresponding to J=2 feedforward layers), this is precisely the Transformer encoder block from Vaswani et al. [2017, p.3].

The architecture we thought was carefully designed actually emerges automatically from discretizing a continuous equation.

### Why Skip Connections?

Remember my earlier question about skip connections? Now we have an answer.

In Substep 3+J, we compute u = (1/2)(u_after_feedforward + u_after_first_norm). This is the skip connection—averaging the input and output of the feedforward block [Tai et al., 2025, p.7].

Why average? This comes from the operator splitting scheme. It's a "relaxation" step that improves stability and accuracy of the numerical method. The skip connection isn't an ad-hoc trick to help gradients flow—it's part of the mathematical structure of the discretization.

Similarly, the skip connection around the attention block (Substep 1 adds to u^0, not replaces it) emerges from how we discretize u_t [Tai et al., 2025, p.7].

### Network Depth and Time Step Size

What about network depth? The relationship is now clear: depth N_layers corresponds to the number of time steps, which equals T/Δt where T is the total integration time and Δt is the step size [Tai et al., 2025, p.7].

A deeper network means smaller Δt, which means a more accurate discretization of the continuous dynamics. But there's a tradeoff: smaller Δt requires more computational steps. This explains why there's a "sweet spot" for network depth—it's balancing approximation accuracy against computational cost.

It also suggests new possibilities: we could use adaptive time stepping (like in ODE solvers) to adjust depth based on input complexity. Harder problems might need more layers; easier ones might need fewer.

## Part IV: What This Tells Us

### A Unified View

The continuous formulation unifies concepts that seemed separate:

**Attention is an integral operator.** Not just "paying attention to relevant parts," but a precise mathematical operation that integrates information across space (token positions) using a data-dependent kernel (the attention scores).

**Normalization is a projection.** Not just "making things stable," but enforcing geometric constraints that keep the solution on a well-defined manifold.

**Feedforward networks are differential operators.** Not just "nonlinear transformations," but discretized flows along specific directions in function space.

**Skip connections are relaxation steps.** Not just "helping gradients," but improving the accuracy of the numerical discretization scheme.

This unified view is intellectually satisfying. But does it give us practical insights?

### Connections to Physics

One striking aspect of this formulation is how it connects Transformers to physics. The integro-differential equation governing the Transformer is similar to equations that appear in:

- **Fluid dynamics** (non-local interactions between particles)
- **Quantum mechanics** (non-local potentials)
- **Population dynamics** (organisms influencing each other at a distance)

This isn't coincidental. Attention models long-range dependencies in sequences—just like these physical systems model long-range interactions in space.

The mathematical framework even suggests physical interpretations: What if we think of tokens as particles? Attention as interaction forces? Layer normalization as constraints keeping the system in equilibrium?

These aren't just analogies. They suggest we could borrow techniques from physics—conservation laws, symmetries, Hamiltonian formulations—and apply them to Transformer design [Tai et al., 2025, p.3].

### Multi-Head Attention

The continuous framework extends naturally to multi-head attention. Instead of running multiple attention mechanisms in parallel (the discrete view), we add a continuous "head dimension" h and integrate over it [Tai et al., 2025, p.13]:

```
u_t = ∫_Ωh ⟨γ(x,·,h,t;u), V(·,y,h,t;u)⟩_Ωx dh + ...
```

When discretized with N_h grid points, this recovers exactly the multi-head attention mechanism. Each "head" is a discretization point in the continuous head space.

This perspective suggests interesting questions: What if we used non-uniform discretization in head space? What if we adapted the number of heads dynamically? The continuous view makes these natural to consider.

### Convolutional Transformers

For structured data like images, we can specialize the integral operators to convolutions [Tai et al., 2025, p.15]:

```
Q(x,y,t;u) = W^Q(·,t) * u(x,·,t)
```

where * denotes convolution. This gives us **Convolutional Vision Transformers** (CvT) as a special case of the general framework.

Again, an architecture that seemed like a separate idea—combining convolutions with transformers—emerges as a natural specialization of the continuous formulation.

## Part V: Training as Optimal Control

### The Control Problem

There's one more beautiful insight in Tai et al.'s framework: training can be viewed as an optimal control problem [Tai et al., 2025, p.6].

The continuous Transformer equation has "control variables":
`θ = {W^Q, W^K, W^V, {W_j, b_j}, σ₁(t), σ₂(t)}`

These are the parameters we learn during training. Training seeks to find θ that minimizes the loss over a dataset `{(u_i, v_i)}`:

```
min_θ (1/B)Σ_i ℓ(N_θ(u_i), v_i)

subject to: N_θ(u_i) solves the Transformer equation
```

This is a **PDE-constrained optimization problem**—a classical formulation in optimal control theory [Tai et al., 2025, p.6]. We're not just minimizing a loss; we're finding control variables for a dynamical system constrained by differential equations.

This connection to control theory is powerful because it brings decades of mathematical machinery to bear on understanding neural network training:

- **Pontryagin's maximum principle** might give insights into optimal architectures
- **Dynamic programming** might suggest better training algorithms
- **Sensitivity analysis** might explain why some parameters matter more than others

### Implications for Training

The control perspective also suggests practical improvements:

**Continuous normalization parameters**: Instead of learning fixed σ₁ and σ₂, we could learn functions σ₁(t) and σ₂(t) that vary with depth. The continuous formulation makes this natural [Tai et al., 2025, p.6].

**Stability constraints**: Control theory has well-developed methods for ensuring system stability. We could incorporate these directly into training to prevent the instabilities that plague deep networks.

**Optimal time horizons**: In control theory, the terminal time T is often chosen to balance solution quality against computation. For Transformers, this translates to choosing network depth—suggesting principled methods for architectural search.

## Part VI: What It Means to Understand

### From Engineering to Mathematics

There's a philosophical point worth making. Before Tai et al.'s work, we understood the Transformer as an engineering artifact: a clever combination of components that works well empirically.

The continuous formulation gives us a different kind of understanding: mathematical understanding. We now see the Transformer as the discretization of a well-defined continuous system with geometric and analytical structure.

This doesn't make the engineering view wrong. But it reveals a deeper layer of explanation. It's like the difference between knowing that an airplane flies because its wings generate lift, versus understanding the Navier-Stokes equations governing fluid flow around the wing. Both are true; one goes deeper.

### What Is Attention, Really?

Consider self-attention. The standard explanation: "The model learns to focus on relevant parts of the input."

The continuous formulation: "Attention is a non-local integral operator with a data-dependent kernel computed via integral transformations and softmax normalization" [Tai et al., 2025, p.5].

These sound very different. But they're describing the same thing at different levels of precision. The first is intuitive; the second is rigorous.

The power of the mathematical view is that it enables:
- **Precise predictions** (stability conditions, convergence rates)
- **Systematic variations** (what happens if we change the kernel structure?)
- **Connections to other domains** (fluid dynamics, quantum mechanics, control theory)

### The Limits of Continuous Models

It's worth noting what this framework doesn't explain. It shows that the Transformer architecture can be derived from a continuous equation, but it doesn't tell us:

- *Why* this particular equation should be effective for language
- *How* the learned parameters relate to linguistic structure
- *When* this architecture will succeed versus fail

These are questions about the *content* of what Transformers learn, not the *form* of the architecture. The continuous framework is powerful, but it's not a complete theory of how language models work.

## Part VII: Looking Forward

### Research Directions

Tai et al.'s framework opens numerous research directions:

**1. Architecture design via continuous modeling**: Instead of proposing architectures heuristically, we could design them by writing down continuous equations with desired properties, then discretizing. Want better long-range dependencies? Modify the integral operator. Want different inductive biases? Change the constraint sets [Tai et al., 2025, p.3].

**2. Analysis using PDE theory**: We can apply the vast theory of differential equations to analyze Transformers: existence and uniqueness of solutions, stability analysis, approximation theory, regularity results. Each of these could yield insights into when and why Transformers work [Tai et al., 2025, p.3].

**3. Continuous training methods**: Current training discretizes in both space (parameters) and time (layers). What if we used continuous methods—shooting methods, adjoint methods, variational integrators—for parts of the training process?

**4. Bridging continuous and discrete**: The framework assumes we discretize uniformly in time. But what about adaptive methods? Variable time steps? Higher-order discretizations? Each of these has discrete analogues as architectural variations [Tai et al., 2025, p.7].

**5. Physics-informed Transformers**: Can we incorporate physical laws (conservation, symmetry) directly into the continuous formulation? This could enable Transformers that respect known structure in scientific data.

### Broader Impact

Beyond specific research directions, this work represents a shift in how we think about neural architectures.

The dominant paradigm in deep learning has been empirical: try many architectures, see what works, iterate. This has been incredibly successful. But it's also somewhat unsatisfying—we're searching in architecture space without clear principles.

The continuous framework suggests an alternative: start with mathematical structures (differential equations, integral operators, variational principles) and derive architectures from them. This is how scientific fields typically mature—from empirical observation to mathematical theory.

We're not there yet for deep learning. But work like Tai et al.'s shows it's possible. The Transformer, our most important architecture, can be understood rigorously through mathematics. That's a promising sign.

## Epilogue: The Beauty of Hidden Structure

I began by asking why the Transformer works. Tai et al. haven't given us a complete answer—that would require understanding not just the architecture but also what it learns and why that's useful for language.

But they've given us something perhaps more valuable: they've shown us the hidden mathematical structure underlying the architecture. The Transformer isn't an arbitrary collection of operations. It's a discretization of a continuous dynamical system, with attention as integral operators, normalization as projections, and feedforward layers as differential flows.

This structure was there all along, implicit in the architecture. Tai et al. made it explicit. In doing so, they've given us new tools—the tools of continuous mathematics—for understanding, analyzing, and designing neural networks.

There's a kind of beauty in discovering hidden structure. The Transformer seemed complicated: attention mechanisms, normalization, skip connections, feedforward layers, all carefully arranged. But underneath, there's something simpler: a continuous evolution equation governing the flow of information through time and space.

Complexity often hides simplicity. Mathematics is how we uncover it.

## References

- Tai, X., Liu, H., Li, L., & Chan, R. H. (2025). A Mathematical Explanation of Transformers for Large Language Models and GPTs. *arXiv preprint* arXiv:2510.03989.

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. *Advances in Neural Information Processing Systems*, 30.

- Glowinski, R., Pan, T.-W., & Tai, X.-C. (2016). Some facts about operator-splitting and alternating direction methods. In *Splitting Methods in Communication, Imaging, Science, and Engineering* (pp. 19-94). Springer.

</EssayLayout>
