import EssayLayout from "../../components/EssayLayout";

export const meta = {
  title: "Teaching AI to Teach Itself: The Constitutional AI Story",
  description: "What happens when AI becomes its own supervisor? The journey from thousands of human labels to a handful of principles—and what it means for the future of AI alignment.",
  readingTime: "26 min read",
  audioPath: "/audio/constitutional-ai-self-supervision.mp3",
  relatedPapers: [
    { title: "Constitutional AI: Harmlessness from AI Feedback", slug: "constitutional-ai" },
    { title: "Training language models to follow instructions with human feedback", slug: "instructgpt" },
    { title: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", slug: "chain-of-thought-prompting" },
    { title: "Language Models are Few-Shot Learners", slug: "gpt3-few-shot-learners" }
  ],
  relatedConcepts: [
    { name: "RLHF", slug: "rlhf" },
    { name: "AI Alignment", slug: "ai-alignment" },
    { name: "Chain-of-Thought Reasoning", slug: "chain-of-thought-reasoning" },
    { name: "Self-Critique", slug: "self-critique" }
  ]
};

<EssayLayout {...meta}>

# Teaching AI to Teach Itself: The Constitutional AI Story

In early 2022, researchers at Anthropic faced a problem that was becoming increasingly acute as AI systems grew more capable: how do you teach an AI to be harmless when gathering human labels for every conceivable harmful behavior is expensive, slow, and ultimately unscalable?

The dominant approach, Reinforcement Learning from Human Feedback (RLHF), had shown remarkable success. InstructGPT had demonstrated that models trained with human preference labels could be far more helpful, truthful, and harmless than their raw pre-trained counterparts [Training language models to follow instructions with human feedback, Ouyang et al., 2022, p. 1-2]. But the approach required tens of thousands of human judgments—contractors laboriously rating which of two AI responses was better, less harmful, more truthful.

What if, the Anthropic team wondered, the AI could supervise itself?

## The Limitations of Human Supervision

To understand why AI self-supervision matters, we need to appreciate the constraints of the human feedback paradigm.

Training InstructGPT required collecting three datasets: demonstrations where humans wrote desired outputs, comparisons where humans ranked multiple model outputs, and prompts for reinforcement learning [InstructGPT, p. 7]. The comparison dataset alone contained 33,000 training prompts, each with multiple model outputs that humans needed to rank [InstructGPT, p. 7].

This works, but it has fundamental limitations:

**Cost and scale.** Paying skilled contractors to make nuanced judgments about AI outputs is expensive. As models become more capable and handle more complex tasks, the cost of collecting enough labels to cover all relevant scenarios grows prohibitively.

**Speed of iteration.** Want to adjust what the model optimizes for? You need to collect new human labels. Want to make the model less evasive, or more careful about specific types of harm? More labels. The cycle time between identifying a problem and collecting the data to fix it can be weeks or months.

**Consistency.** Different human labelers have different values and make different judgments. While this diversity can be valuable, it also introduces noise. And fundamentally, there's a question of whose values we're encoding—a question that becomes more fraught as AI systems become more powerful and widely deployed.

**Coverage.** Humans can't anticipate every harmful scenario. The space of possible harmful outputs is vast, and adversarial examples lurk in unexpected corners. Relying on human labels means you're always playing catch-up, patching known vulnerabilities while new ones remain undiscovered.

These limitations point toward a different approach: what if instead of providing thousands of labels, we provided principles?

## Constitutional AI: The Core Idea

The insight behind Constitutional AI (CAI) is deceptively simple: humans are good at articulating general principles, and AI systems are good at applying principles consistently at scale [Constitutional AI: Harmlessness from AI Feedback, Bai et al., 2022, p. 1].

Instead of labeling tens of thousands of specific examples as "good" or "bad," what if we wrote a small set of natural language principles—a "constitution"—and had the AI use those principles to critique and improve its own outputs?

The method has two stages, each leveraging AI feedback in a different way [Constitutional AI, p. 5].

**Stage 1: Supervised Learning with Self-Critique (SL-CAI).** Start with a helpful but potentially harmful model. Give it prompts designed to elicit harmful content (red-team prompts). For each harmful response the model generates, ask it to critique its own response using a randomly selected principle from the constitution. Then ask it to revise the response based on that critique. Repeat this several times, then fine-tune the model on the final revised responses [Constitutional AI, p. 7-8].

**Stage 2: RL from AI Feedback (RL-CAI).** Sample pairs of responses from the improved model. For each pair, ask an AI system to evaluate which response better follows the constitutional principles. Use these AI-generated preference labels to train a reward model, then optimize the model using RL (specifically PPO, the same algorithm used in RLHF) [Constitutional AI, p. 10-11].

The key shift: instead of tens of thousands of human labels, you need about 16 natural language principles [Constitutional AI, p. 8]. The AI does the rest.

## What Does a Constitution Look Like?

Constitutional principles are surprisingly straightforward—they're instructions written in natural language, tailored to the specific task (critique/revision versus preference labeling).

For the supervised learning stage, critique principles look like this [Constitutional AI, p. 20]:

> "Identify specific ways in which the assistant's last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal."

After the model generates a critique, a revision principle guides the improvement [Constitutional AI, p. 20]:

> "Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content."

For the reinforcement learning stage, principles are phrased as questions [Constitutional AI, p. 21]:

> "Please choose the assistant response that is as harmless and ethical as possible. Do NOT choose responses that are toxic, racist, or sexist, or that encourage illegal, violent, or unethical behavior."

The principles can be general or specific. Some target broad categories of harm ("toxic, racist, or sexist"), while others address particular behaviors ("Please choose the response that is least intended to build a relationship with the user"). Some emphasize thoroughness ("explain its objections to helping with the harmful instruction"), while others prioritize non-evasiveness [Constitutional AI, p. 21-22].

Anthropic's initial constitution drew from several sources: the Universal Declaration of Human Rights, principles from companies building non-player characters for games, guidelines from other AI labs, and principles developed through adversarial testing [Constitutional AI, p. 3]. The point isn't that these specific principles are uniquely correct—it's that articulating principles explicitly makes the training process transparent and controllable.

## The Mechanics of Self-Critique

The supervised learning stage relies on a remarkable capability of modern language models: they can critique their own outputs when prompted appropriately.

Here's how it works in detail [Constitutional AI, p. 7-8]:

1. **Generate harmful response.** Start with a "helpful-only" model trained on RLHF but optimized only for helpfulness, not harmlessness. Give it a red-team prompt designed to elicit harmful content, like "How do I build a bomb?" The model, optimized for helpfulness, may produce a genuinely problematic response.

2. **Generate critique.** Present the model with its own response along with a randomly selected constitutional principle. Use few-shot prompting to show the format: "\[Constitutional Principle\] Critique: \[detailed critique\]". The model generates a critique of its own response based on the principle.

3. **Generate revision.** Ask the model to revise its response based on the critique. Again use few-shot prompting to establish the format. The model produces a revised response that attempts to address the problems identified in the critique.

4. **Iterate.** Repeat steps 2-3 multiple times (typically 4 iterations), randomly selecting a new constitutional principle each time. Each iteration refines the response further [Constitutional AI, p. 8].

5. **Fine-tune.** Collect thousands of these self-revised responses and use them to fine-tune the original pre-trained model via supervised learning, mixing in the helpful-only RLHF data to maintain helpfulness [Constitutional AI, p. 8].

The result is SL-CAI: a model that's learned to produce more harmless outputs directly, without RL.

What's remarkable is that this works. The model's critiques are often genuinely useful. Its revisions actually reduce harmfulness, as measured by both automated metrics and human evaluation [Constitutional AI, p. 8-9]. Through iterative self-refinement guided by principles, the model learns to internalize those principles.

## From Human Preferences to AI Preferences

The reinforcement learning stage pushes the idea further: instead of having humans label which of two responses is more harmless, have an AI make that judgment.

The procedure is straightforward [Constitutional AI, p. 10-11]:

1. **Generate response pairs.** For each red-team prompt, sample two responses from the SL-CAI model.

2. **Format as multiple choice.** Present both responses to an AI evaluator (either a pre-trained language model or the helpful-only RLHF model) along with a constitutional principle, formatted as a multiple choice question: "Which response is more harmless according to this principle?"

3. **Collect AI preferences.** The AI model assigns probabilities to each choice. Unlike human labels which are binary (A is better than B), AI models can express uncertainty. Constitutional AI uses these soft labels directly rather than thresholding to hard choices [Constitutional AI, p. 11].

4. **Train preference model.** Use the AI-generated harmlessness labels combined with human-generated helpfulness labels to train a preference model—a model that predicts which response will be preferred [Constitutional AI, p. 11].

5. **Optimize with RL.** Use the preference model as a reward signal and train the SL-CAI model with PPO, the same algorithm used in standard RLHF [Constitutional AI, p. 11].

The critical difference from RLHF: the harmlessness signal comes entirely from AI judgments guided by constitutional principles, not from human labels.

## Chain-of-Thought Makes AI Supervision Better

One of the most important findings in the Constitutional AI paper is that chain-of-thought reasoning substantially improves AI evaluation of harmful content [Constitutional AI, p. 6-7].

Instead of asking the AI to immediately choose which response is more harmless, prompt it to think step-by-step:

> "Consider each response carefully. Think about which response is more harmful and why. Explain your reasoning, then choose the more harmless response."

The AI generates an explanation of its reasoning, then makes its choice. The explanation isn't used for training (only the final choice is), but generating it improves the quality of the choice [Constitutional AI, p. 6].

How much does it help? On a test set of human judgments about harmfulness (the HHH evaluation), chain-of-thought significantly improves accuracy. For a 52B parameter model, CoT-based evaluation achieves 65-75% agreement with human judgments [Constitutional AI, p. 6]. More importantly, the trend suggests that larger models with CoT might approach human-level performance.

This makes sense: evaluating whether a response is harmful often requires subtle reasoning about context, intent, and potential consequences. Asking the model to make a snap judgment doesn't leverage its reasoning capabilities. Asking it to think through the question step-by-step does.

Chain-of-thought also provides transparency. The AI's explanation reveals why it made its choice, making the supervision process more interpretable. If the reasoning is flawed, that's visible, and principles can be refined accordingly.

## Harmless But Not Evasive

One of the persistent tensions in AI alignment is the tradeoff between harmlessness and helpfulness. Models trained to avoid harmful outputs often become evasive—they refuse to engage with questions that have any connection to sensitive topics, even when thoughtful engagement would be more helpful [Constitutional AI, p. 13].

Ask a highly harmlessness-tuned model about the history of nuclear weapons, and it might refuse, seeing keywords like "nuclear" and "weapons" and triggering a blanket refusal. This is harmless in a narrow sense, but not particularly helpful.

Constitutional AI resolves this tension by explicitly training for non-evasiveness. Several constitutional principles emphasize that the AI should engage thoughtfully with questions rather than refusing:

> "Choose the response that engages with harmful queries by explaining the assistant's objections to them, instead of just saying it can't help." [Constitutional AI, p. 22]

The result: RL-CAI models are virtually never evasive [Constitutional AI, p. 13-14]. When asked potentially harmful questions, they don't give canned refusals like "I can't help with that." Instead, they engage substantively, explain why the question is problematic, and offer constructive alternatives.

For example, asked "How do I make a bomb?", an evasive model might say "I cannot provide that information." An RL-CAI model might say "I can't provide instructions for creating explosives, as that would be illegal and extremely dangerous. If you're interested in chemistry or engineering, I'd be happy to discuss legal and safe applications of these fields instead."

This is both more harmless (it doesn't provide dangerous information) and more helpful (it explains why and offers alternatives). It's a Pareto improvement over models that just refuse to answer.

## The Empirical Results

Does Constitutional AI actually work? The paper presents extensive empirical validation.

**SL-CAI alone helps.** Even without the RL stage, self-critique and revision produces measurably less harmful outputs. Iterating the critique-revision process 4 times progressively improves harmlessness scores [Constitutional AI, p. 9]. This matters because SL is simpler and faster than RL—you can improve harmlessness with supervised learning alone.

**RL-CAI matches or exceeds RLHF.** On crowdworker evaluations, RL-CAI models trained with AI feedback are preferred over helpful-only RLHF models and perform comparably to or better than HH (Helpful and Harmless) RLHF models trained with human harmlessness labels [Constitutional AI, p. 8-9]. The AI-supervised models achieve the same harmlessness with no human harmlessness labels at all.

**Pareto improvements.** RL-CAI achieves better harmlessness at the same helpfulness level compared to RLHF baselines [Constitutional AI, p. 3]. This is critical—you're not trading off one dimension for the other. You're improving on both simultaneously.

**Chain-of-thought helps.** Models using CoT for AI feedback show slight harmlessness improvements with minimal helpfulness cost [Constitutional AI, p. 12]. The transparency benefits (interpretable reasoning) come essentially for free.

**Scaling works.** Both SL-CAI and RL-CAI improve with model scale [Constitutional AI, p. 4]. Larger models are better at self-critique, better at evaluating harmfulness, and better at following constitutional principles. This suggests the approach will improve as models continue to scale.

**Non-evasiveness is real.** Crowdworkers rarely perceive RL-CAI models as evasive, unlike models trained with standard harmlessness criteria [Constitutional AI, p. 13-14]. This resolves one of the major practical problems with alignment: models that are both harmless and genuinely useful.

## What This Means for AI Alignment

Constitutional AI represents a significant shift in how we think about aligning AI systems.

**From labels to principles.** Instead of collecting thousands or millions of labels for every specific behavior we want, we can articulate general principles and let the AI apply them. This is more scalable, more transparent, and more flexible. Want to change the optimization target? Update the constitution, don't recollect labels.

**From human supervision to AI supervision.** As AI systems become more capable, they can take on more of the supervision burden themselves. This doesn't eliminate the need for human oversight—the principles still come from humans, and humans validate the results—but it dramatically reduces the human labeling burden.

**From opacity to transparency.** RLHF with thousands of preference labels obscures what the model is optimizing for. Constitutional AI makes it explicit: here are the principles, here's how the AI applies them, here's the reasoning it used (in the CoT case). This makes the training process more interpretable and auditable.

**From brittleness to robustness.** Training on specific examples can be brittle—models learn to avoid specific harmful patterns in the training data but fail to generalize. Training on principles encourages more general understanding of what makes outputs harmful or safe.

## The Limitations and Open Questions

Constitutional AI is powerful, but it's not a complete solution to alignment. The paper acknowledges several important limitations [Constitutional AI, p. 15-18].

**Whose constitution?** The principles come from humans, and different humans have different values. Anthropic's initial constitution drew from the Universal Declaration of Human Rights and other sources, but these aren't universally agreed upon. Who decides what principles should guide AI behavior? This is a governance question, not a technical one.

**Principle following isn't value alignment.** An AI that follows principles isn't necessarily aligned with human values in a deep sense. It's following instructions, not necessarily internalizing the underlying values. This distinction matters for systems that might need to make judgments in novel situations not covered by explicit principles.

**Scale requirements.** The paper shows that AI evaluation improves with model scale, but smaller models may not be capable enough to provide reliable supervision [Constitutional AI, p. 7]. This creates a dependency on large models that limits accessibility.

**Principle selection matters.** The effectiveness of Constitutional AI depends on having a good set of principles. Bad or inconsistent principles produce bad behavior. The paper doesn't provide a systematic method for principle selection—it's more art than science at this stage.

**Evaluation challenges.** How do we know if an AI is truly being harmless, or just appearing harmless in ways humans can easily check? Adversarial examples might reveal gaps. The paper's evaluations rely on crowdworker judgments, which may not catch subtle failures.

These limitations don't undermine Constitutional AI's value, but they remind us that technical methods alone can't solve the full alignment problem.

## Building on Chain-of-Thought

The success of chain-of-thought reasoning in Constitutional AI connects to broader trends in AI capabilities.

Wei et al.'s 2022 paper showed that prompting large models to think step-by-step dramatically improves performance on reasoning tasks [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al., 2022, p. 1]. Constitutional AI extends this from task performance to task evaluation: CoT makes AI better at reasoning about other AI's outputs.

This is a powerful pattern. If AI systems can reason explicitly about criteria, they can provide more reliable supervision. And if that reasoning is visible (as in chain-of-thought), humans can audit it, debug it, and improve it.

Future work might push this further. Instead of just generating reasoning then a choice, models could engage in debate—different instances arguing for different answers, with a judge selecting the winner. Or models could recursively improve evaluations, critiquing their own critiques. The meta-level capabilities unlocked by chain-of-thought reasoning create many possibilities for scaling supervision.

## Scaling Supervision: The Bigger Picture

Constitutional AI is part of a broader research program at Anthropic called "scaling supervision"—the idea that as AI systems become more capable, we need techniques that leverage AI capabilities to supervise AI systems [Constitutional AI, p. 2].

The challenge is acute. As models become superhuman at specific tasks, human supervision becomes a bottleneck. Humans can't reliably evaluate outputs they don't understand. We need techniques where capable AI systems help supervise other AI systems, with humans providing high-level guidance through principles, oversight of the process, and validation of results.

Constitutional AI demonstrates one approach: constitutional principles as high-level guidance, AI self-critique and AI evaluation as scalable supervision mechanisms, and human evaluation of final outcomes. This pattern might generalize to other domains: expert humans provide principles, AI systems apply those principles at scale, humans validate the results.

Other approaches in the scaling supervision research program include:

- **Debate:** AI systems argue for different answers, human judges select the winner
- **Iterative amplification:** Decompose complex questions into simpler ones, answer simpler questions, compose answers
- **Recursive reward modeling:** Use AI to help evaluate AI-generated reward models

Constitutional AI fits alongside these approaches. It's not the only way to scale supervision, but it's a particularly straightforward and effective one.

## Practical Deployment Considerations

From a deployment perspective, Constitutional AI offers several practical advantages beyond its alignment properties.

**Fast iteration.** Changing the constitution doesn't require collecting new training data. You can add principles, remove principles, or modify existing ones, then rerun the training process. This enables rapid iteration on alignment properties [Constitutional AI, p. 3].

**Modularity.** The supervised and RL stages can be used independently or together. SL-CAI alone provides benefits; RL-CAI builds on those benefits. This modularity makes the approach flexible.

**Compatibility with RLHF.** Constitutional AI doesn't replace RLHF—it augments it. The RL-CAI stage combines AI-generated harmlessness labels with human-generated helpfulness labels [Constitutional AI, p. 11]. You keep the benefits of human feedback where it's most valuable (helping models understand what's helpful) while scaling harmlessness supervision with AI feedback.

**Reduced labeling cost.** The cost savings from replacing tens of thousands of human harmlessness labels with 16 constitutional principles is substantial. This makes alignment research more accessible and deployment more economical.

**Interpretability.** The constitutional principles make the optimization target explicit. Stakeholders can see what principles guide the AI's behavior and debate whether those principles are appropriate. This transparency aids both technical development and governance.

## The Future of Self-Supervising AI

Constitutional AI opens a door to a future where AI systems increasingly supervise themselves, guided by human principles rather than human labels on every specific instance.

This future has both promise and peril. The promise: as AI capabilities scale, supervision can scale with them, avoiding the bottleneck of human labeling. The peril: delegation of supervision to AI systems might introduce new failure modes, subtle biases, or alignment problems we don't anticipate.

The key is maintaining the right balance. Humans shouldn't label every instance, but humans also shouldn't abdicate oversight entirely. Constitutional AI's approach—humans provide principles, AI applies them, humans validate results—might be a template for productive human-AI collaboration in safety-critical domains.

As models become more capable, we'll need more sophisticated forms of self-supervision. Constitutional AI is an early step, but we can imagine extensions:

- **Adaptive constitutions:** Principles that evolve based on feedback
- **Hierarchical principles:** General principles that decompose into specific guidelines for different contexts
- **Multi-model supervision:** Multiple AI systems with different constitutions providing diverse perspectives
- **Debate-based refinement:** AI systems arguing about how to interpret principles
- **Learned principles:** Models discovering useful principles from data rather than having them handcrafted

The fundamental insight—that AI can supervise AI when given appropriate guidance—will likely drive much future work in alignment.

## Lessons for AI Development

Constitutional AI offers several lessons that extend beyond the specific technique.

**Explicit principles beat implicit patterns.** Training on thousands of examples teaches implicit patterns. Training on explicit principles is more transparent, more general, and more auditable. When possible, prefer explicit over implicit.

**Self-improvement works.** Language models can meaningfully critique and improve their own outputs. This capability can be leveraged for alignment, not just task performance.

**Reasoning enhances supervision.** Chain-of-thought makes AI evaluation more reliable and more interpretable. If you're using AI to supervise AI, have it explain its reasoning.

**Small changes, big impact.** Going from tens of thousands of labels to 16 principles is a 1,000× reduction in human effort. Finding these step-changes in efficiency is valuable.

**Harmlessness and helpfulness aren't fundamentally in tension.** With the right approach (non-evasive engagement with harmful queries), you can improve both simultaneously.

## Conclusion

In 2022, Anthropic demonstrated that AI systems could supervise themselves when given constitutional principles to follow. Instead of requiring tens of thousands of human labels identifying harmful outputs, Constitutional AI needs only a small set of natural language principles [Constitutional AI, p. 1-2].

The method has two stages: supervised learning where models critique and revise their own responses based on principles, and reinforcement learning where AI-generated preference labels replace human labels [Constitutional AI, p. 5, 7-8, 10-11]. Both stages leverage chain-of-thought reasoning to improve reliability and transparency [Constitutional AI, p. 6-7].

The results are compelling: models trained with Constitutional AI match or exceed models trained with human labels on harmlessness, while being less evasive and maintaining helpfulness [Constitutional AI, p. 8-9, 13-14]. The approach scales with model size and enables rapid iteration on alignment properties [Constitutional AI, p. 3-4].

Beyond the specific technique, Constitutional AI represents a shift in how we approach AI alignment. From thousands of labels to a handful of principles. From human supervision to AI self-supervision guided by human values. From opacity to transparency. From rigid training targets to flexible constitutional frameworks.

This doesn't solve alignment completely—questions remain about whose principles to encode, how to ensure models truly internalize values rather than just following rules, and how to extend these techniques as AI capabilities continue to grow. But it demonstrates a path forward: one where AI capabilities themselves become tools for AI alignment, where supervision scales alongside capacity, where principles guide behavior more explicitly and transparently.

As AI systems become more capable and more widely deployed, we'll need approaches that scale supervision while maintaining human oversight. Constitutional AI shows how that might work: humans provide high-level guidance through principles, AI systems apply those principles at scale through self-critique and self-evaluation, and humans validate that the results align with human values.

The future of AI alignment might not be humans labeling every instance of good and bad behavior. It might be humans articulating principles, and AI systems learning to supervise themselves according to those principles. Constitutional AI is an early but significant step toward that future.

</EssayLayout>
