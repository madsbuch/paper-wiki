import EssayLayout from "../../components/EssayLayout";

export const meta = {
  title: "The Hidden Cost of AI Assistance: When Productivity Undermines Competence",
  description: "Groundbreaking research reveals that AI assistance can significantly impair skill formation—but the way you use AI matters more than whether you use it at all.",
  readingTime: "18 min read",
  audioPath: "/audio/the-hidden-cost-of-ai-assistance.mp3",
  relatedPapers: [
    { title: "How AI Impacts Skill Formation", slug: "ai-impacts-skill-formation" },
    { title: "Training language models to follow instructions with human feedback", slug: "instructgpt" },
    { title: "Constitutional AI: Harmlessness from AI Feedback", slug: "constitutional-ai" }
  ],
  relatedConcepts: [
    { name: "Cognitive Offloading", slug: "cognitive-offloading" },
    { name: "AI Alignment", slug: "ai-alignment" },
    { name: "Transfer Learning", slug: "transfer-learning" },
    { name: "RLHF", slug: "rlhf" }
  ],
  citations: [
    {
      paper: "How AI Impacts Skill Formation",
      authors: "Shen, J. H. & Tamkin, A.",
      year: "2026",
      pages: "1-20"
    },
    {
      paper: "The impact of generative ai on critical thinking",
      authors: "Lee, H., Sarkar, A., et al.",
      year: "2025",
      pages: "1-22"
    },
    {
      paper: "Measuring progress on scalable oversight for large language models",
      authors: "Bowman, S. R., et al.",
      year: "2022",
      pages: "1-15"
    }
  ]
};

<EssayLayout {...meta}>

## The Apprentice's Dilemma

Picture a junior software developer on their first day at a new job. They're handed a task: implement a feature using a Python library they've never seen before. The library is called Trio, designed for asynchronous programming. The deadline is tight. Their manager has already moved on to other meetings.

The developer has two choices. They could read the documentation, write some code, encounter errors, debug those errors, and gradually build understanding through struggle. Or they could open an AI assistant, paste in the task description, and have working code in minutes.

Which path leads to productivity? Which leads to competence?

For years, we assumed these were the same question—or at least that they pointed in the same direction. Get more done, learn more in the process. But a rigorous new study from Anthropic reveals something uncomfortable: when it comes to learning new skills, AI assistance can be a Faustian bargain. The productivity comes, but the competence doesn't follow [Shen & Tamkin, 2026].

## The Experiment

Judy Hanwen Shen and Alex Tamkin designed an elegant experiment to test how AI affects skill formation. They recruited 52 professional and freelance programmers—people who code regularly, use Python frequently, and have tried AI coding assistants before. None had used the Trio library, making it a clean test of learning something new.

Participants were randomly assigned to two groups. Both groups had 35 minutes to complete two coding tasks using Trio. Both had access to the library's documentation. But only one group had access to an AI assistant (GPT-4o) that could answer questions and generate code.

After completing the tasks, everyone took a quiz measuring their understanding of Trio—conceptual knowledge, code reading ability, and debugging skills. No one could use AI for the quiz. The question was simple: did using AI to complete the tasks help or hurt learning the library?

The results were striking [Shen & Tamkin, 2026, p. 9].

## A 17% Learning Gap

Participants who used AI to complete the tasks scored 17% lower on the quiz than those who worked without AI assistance. In academic terms, this was a "two grade point" difference—the gap between a B and a C+.

The effect was statistically robust (Cohen's d = 0.738, p = 0.010) and held up even after controlling for prior programming experience. It wasn't that less skilled programmers happened to use more AI. The AI itself appeared to cause the learning deficit.

But here's what makes the study truly interesting: the AI group didn't complete the tasks significantly faster. Some participants in the AI condition spent up to 11 minutes just composing queries to the assistant. Others asked 15 or more questions across the 35-minute session. The time they saved by not debugging was often consumed by interacting with the AI [Shen & Tamkin, 2026, p. 14].

So they learned less and didn't even save time. What went wrong?

## The Error Advantage

To understand what happened, the researchers watched screen recordings of every participant completing the tasks. What they found was a striking difference in how the two groups experienced the learning process.

The control group—working without AI—encountered significantly more errors. The median participant hit three errors during the tasks, compared to just one error for the AI group. This might sound like a victory for AI assistance. Fewer errors means smoother sailing, right?

But those errors turned out to be teachers [Shen & Tamkin, 2026, p. 15-17].

When a control group participant got a `RuntimeWarning` because they forgot to await a coroutine, they had to stop, read the error message, understand what a coroutine is, figure out where the await keyword goes, and try again. When they got a `TypeError` because they passed a coroutine object instead of an async function, they had to grasp the difference between calling an async function and referencing it.

These weren't random errors. They were errors specific to Trio's concurrency model—the very concepts the quiz would later test. Each error was a forced lesson in how the library actually works.

The AI group rarely encountered these errors. The assistant generated correct code, the code ran, the task completed. But without the friction of failure, the concepts never lodged in memory. "The skill development of the control group can be attributed to the process of encountering and subsequently resolving errors independently," the researchers concluded [Shen & Tamkin, 2026, p. 3].

## The Debugging Gap

The largest difference between groups appeared on debugging questions—exactly the skills needed to supervise AI-generated code [Shen & Tamkin, 2026, p. 11].

This creates a troubling paradox. AI can generate code faster than humans can write it. But someone needs to verify that code is correct, especially in high-stakes applications. If using AI to write code prevents people from developing debugging skills, we're creating a generation of developers who can't effectively supervise the AI systems they depend on.

As AI models become more capable, the problem of overseeing them becomes harder. It requires humans who understand the domain deeply enough to catch subtle errors. But if those humans developed their skills with heavy AI assistance, they may lack exactly this capability [Shen & Tamkin, 2026, p. 2].

The researchers put it starkly: "When complex software tasks require human-AI collaboration, humans still need to understand the basic concepts of code development even if their software skills are complementary to the strengths of AI" [Shen & Tamkin, 2026, p. 2].

## Six Ways to Use AI (Three That Work)

Here's where the study offers hope. Not everyone in the AI group learned poorly. When the researchers analyzed the screen recordings in detail, they identified six distinct patterns of AI interaction—and three of them preserved learning outcomes even with full AI access [Shen & Tamkin, 2026, p. 12-14].

**The Low-Learning Patterns:**

*AI Delegation* (average quiz score: 39%): These participants asked AI to generate all their code, pasted it directly without reading, and completed tasks fastest. They learned almost nothing.

*Progressive AI Reliance* (35%): Started by asking a few questions, then progressively delegated everything to AI. They learned about the first task but missed the second entirely.

*Iterative AI Debugging* (24%): Asked AI to check and fix their code repeatedly (5-15 queries). Slow and unproductive—the worst of both worlds.

**The High-Learning Patterns:**

*Conceptual Inquiry* (65%): Only asked conceptual questions ("What does start_soon do?"), then wrote code themselves. Encountered errors but resolved them independently. Nearly as fast as AI delegation with far better learning.

*Hybrid Code-Explanation* (68%): Asked AI to generate code AND explain what it does. Read and understood both the code and explanations. Slower but highly effective.

*Generation-Then-Comprehension* (86%): Had AI generate code first, then asked follow-up questions to verify understanding. Looked similar to AI delegation at first glance but added the crucial step of checking comprehension. Highest learning outcomes of any pattern.

The difference wasn't whether participants used AI. It was whether they stayed cognitively engaged while using it.

## The Cognitive Engagement Principle

What separates the high-learning patterns from the low-learning ones? In every case, it's the presence of active thinking.

When participants asked for explanations alongside code, they had to process those explanations. When they asked conceptual questions, they had to map the answers onto their own mental models. When they verified understanding after generation, they forced themselves to engage with the material rather than just accept it.

The low-learning patterns all involved delegation without engagement. The participant's brain was essentially a conduit—receiving AI output and depositing it into the code editor without meaningful processing in between.

This aligns with broader research on cognitive offloading—the tendency to outsource mental effort to external tools. "Frequent use of AI has been associated with worse critical thinking abilities and increased cognitive offloading," notes a recent survey of knowledge workers [Lee et al., 2025]. The AI handles the thinking, and the human's thinking muscles atrophy from disuse.

But cognitive offloading isn't inevitable. The high-learning patterns show that AI can enhance rather than replace human cognition—if used deliberately.

## The Apprentice and the Power Tool

Think of it like an apprentice carpenter learning from a master craftsman. The master has access to a sophisticated CNC machine that can cut any piece of wood perfectly. How should the apprentice learn?

One approach: have the CNC do everything. The furniture gets built quickly and flawlessly. But the apprentice never develops an intuition for wood grain, never learns to visualize joints, never builds the judgment needed to know when the CNC's output is wrong.

Another approach: use the CNC strategically. Do simpler cuts by hand to build fundamental skills. Use the CNC for complex curves that would take forever manually. Ask the master to explain why certain techniques work. Make mistakes, ruin some wood, learn to see the material.

The second apprentice will be slower initially. But years later, they'll be the one who can design new furniture, troubleshoot problems, and train the next generation. The first apprentice will remain dependent on the CNC—helpless when it breaks, unable to judge when its output is subtly wrong.

AI coding assistants are the CNC machines of software engineering. Powerful tools that can produce impressive output. But tools that must be used with awareness of their effect on the user's skill development.

## Implications for the AI Age

These findings have serious implications beyond individual learning.

**For workers**: Be strategic about when you use AI assistance. If you're working on something you already know well, AI can accelerate routine tasks without harm. But if you're learning something new—a new library, a new language, a new domain—consider the hidden cost. Use AI to enhance understanding, not replace it.

**For educators**: We may need to rethink how we train the next generation of developers. Banning AI isn't realistic or even desirable. But teaching students to use AI effectively—emphasizing the high-engagement patterns—might be essential. "AI assistance should be carefully adopted into workflows to preserve skill formation – particularly in safety-critical domains," the researchers warn [Shen & Tamkin, 2026, p. 1].

**For organizations**: Productivity metrics that reward speed may inadvertently punish learning. A junior developer who takes longer to complete tasks because they're building real understanding is more valuable long-term than one who completes tickets quickly via AI delegation but never develops expertise.

**For AI safety**: This research connects directly to the challenge of scalable oversight. As AI systems become more capable, humans need skills to supervise them—to catch errors, verify correctness, understand failure modes. If our training practices undermine these skills, we create a dangerous feedback loop: AI gets better while human oversight gets worse [Bowman et al., 2022].

## The Productivity-Competence Tradeoff

There's a deeper lesson here about the nature of productivity itself.

For decades, we've optimized for visible output: lines of code written, tasks completed, projects shipped. These metrics are easy to measure and satisfying to increase. AI assistants excel at boosting them.

But competence—the ability to understand deeply, reason carefully, and solve novel problems—is harder to measure and slower to build. It requires exactly the kind of effortful processing that AI makes easy to avoid.

The study reveals that these two goals can diverge. A developer using AI may be highly productive by conventional metrics while failing to develop the expertise that makes them genuinely valuable. The organization sees the output and celebrates. No one notices what didn't develop in the developer's mind.

This isn't an argument against AI assistance. It's an argument for using it consciously, with awareness of its effects on human cognition. The goal should be augmented intelligence, not replaced intelligence.

## Learning to Learn with AI

The researchers offer some hope. Major AI providers are beginning to recognize this challenge. ChatGPT has introduced a "Study Mode" that encourages conceptual engagement. Claude Code offers an "explanatory mode" that emphasizes understanding over raw code generation.

These features acknowledge that not all AI interactions are equal. Sometimes you want the AI to just complete the task. But sometimes—especially when you're building new skills—you want it to teach you while completing the task.

The three high-learning patterns from the study suggest what effective AI-assisted learning looks like:

1. **Ask why, not just what.** Request explanations alongside any generated output.

2. **Solve problems yourself, verify with AI.** Try to work through challenges independently before asking AI to generate solutions.

3. **Test your understanding.** After AI helps you complete something, ask follow-up questions to ensure you actually understand what happened.

These practices require more time than simple delegation. But they preserve the cognitive engagement that builds lasting competence.

## The Choices We Face

The apprentice developer we imagined at the beginning faces a real choice—one that millions of knowledge workers face every day. Use AI to get the task done quickly, or struggle through to build understanding?

The research suggests this is often a false dichotomy. With the right approach, you can use AI AND build understanding. The key is staying cognitively engaged—asking questions, seeking explanations, verifying comprehension.

But the default is dangerous. Without conscious effort, it's easy to slip into AI delegation—getting tasks done while learning nothing. The productivity looks good on daily standups. The competence gap only becomes visible much later, when you're asked to debug something you've never actually understood.

The AI won't protect you from this. It will happily generate code, answer questions, and complete your tasks. It doesn't know or care whether you're building skills in the process. That responsibility falls to you.

So the next time you reach for the AI assistant, ask yourself: Am I using this tool to enhance my understanding, or to bypass it?

The answer will shape not just what you accomplish today, but what you're capable of tomorrow.

---

*This essay synthesizes findings from "How AI Impacts Skill Formation" (Shen & Tamkin, 2026), the first rigorous experimental study of how AI coding assistance affects skill development. All empirical claims are supported by direct citations to the source paper.*

</EssayLayout>

export default ({ children }) => children;
