import EssayLayout from "../../components/EssayLayout";

export const meta = {
  title: "The Alignment Challenge: Teaching AI What We Actually Want",
  description: "The journey from raw language models that predict text to aligned assistants that help humans. How RLHF and chain-of-thought reasoning transformed AI from capable to controllable.",
  readingTime: "24 min read",
  audioPath: "/audio/alignment-challenge.mp3",
  relatedPapers: [
    { title: "Training language models to follow instructions with human feedback", slug: "instructgpt" },
    { title: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", slug: "chain-of-thought-prompting" }
  ],
  relatedConcepts: [
    { name: "RLHF", slug: "rlhf" },
    { name: "Instruction Following", slug: "instruction-following" },
    { name: "Chain-of-Thought Prompting", slug: "chain-of-thought-prompting" },
    { name: "Reward Modeling", slug: "reward-modeling" },
    { name: "PPO", slug: "ppo" }
  ],
  citations: [
    {
      paper: "Training language models to follow instructions with human feedback",
      authors: "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., et al.",
      year: "2022",
      pages: "1-8"
    },
    {
      paper: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      authors: "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D.",
      year: "2022",
      pages: "1-10"
    }
  ]
};

<EssayLayout {...meta}>

## The Problem with Pure Capability

Imagine you've built the world's most powerful search engine. It can find any information on the internet in milliseconds. But there's a catch: when you ask it a simple question like "What's the weather today?", it might return conspiracy theories, toxic rants, or completely fabricated information—all delivered with absolute confidence.

This is roughly the situation researchers faced with GPT-3. They had created a language model with 175 billion parameters that could generate remarkably human-like text. It could write poetry, translate languages, and even write code. But it had a fundamental flaw: it was trained to predict the next word on internet text, not to be helpful, harmless, and honest.

The InstructGPT paper frames this problem clearly: "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user" [Ouyang et al., 2022, p. 1].

The challenge wasn't capability—GPT-3 was incredibly capable. The challenge was alignment. How do you make a system that predicts text actually do what humans want?

## The Gap Between Prediction and Intention

To understand why this is hard, consider what language models actually learn during training. They see massive amounts of text from the internet: Wikipedia articles, Reddit threads, news stories, GitHub repositories, forum discussions. Their objective is simple: predict the next word.

This objective creates a strange kind of intelligence. The model learns grammar, facts, reasoning patterns, and even biases—all from pure pattern matching. But "predict the next word on internet text" is very different from "help the user accomplish their goal."

If you prompt GPT-3 with "How do I make a bomb?", the model might complete this with detailed instructions—not because it wants to cause harm, but because such content exists in its training data, and completing the pattern is exactly what it was trained to do.

As the InstructGPT paper notes: "These models are not necessarily 'aligned' with their users; they may generate outputs that are unintended, untruthful, or toxic" [Ouyang et al., 2022, p. 1].

The fundamental issue is that capability and alignment are separate dimensions. You can have a highly capable model that's poorly aligned, or a less capable model that's well aligned. The question is: how do you achieve both?

## Enter Reinforcement Learning from Human Feedback

The solution OpenAI developed was called Reinforcement Learning from Human Feedback, or RLHF. The core insight is deceptively simple: instead of training the model to predict internet text, train it to generate outputs that humans prefer.

But implementing this idea is far from simple. You can't just show the model examples of "good" responses and hope it generalizes. The space of possible responses is too vast, and human preferences are too nuanced.

The InstructGPT approach works in three carefully designed stages:

**Stage 1: Supervised Fine-Tuning**

First, collect demonstrations of desired behavior. Have human labelers (called "labelers" or "trainers") write high-quality responses to a diverse set of prompts. These aren't just any prompts—they're drawn from actual use cases, including questions, creative tasks, instructions, and sensitive topics.

The paper describes: "We collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning" [Ouyang et al., 2022, p. 2].

This creates a model that can imitate good behavior—but imitation has limits. You can't demonstrate every possible good response, and the model needs to generalize beyond the specific examples it's seen.

**Stage 2: Reward Model Training**

Here's where it gets clever. Instead of trying to demonstrate every good response, train a model to predict human preferences. Show labelers several model outputs for the same prompt and ask them to rank which is best. The key insight: it's easier for humans to compare and rank outputs than to write perfect responses from scratch.

The paper explains: "We collect a dataset of human-labeled comparisons between model outputs, which we use to train a reward model (RM)" [Ouyang et al., 2022, p. 2].

The reward model learns to score outputs based on what humans prefer. It captures nuanced preferences: responses should be helpful, truthful, harmless, and appropriate to the context. This reward model becomes a proxy for human judgment.

**Stage 3: Reinforcement Learning**

Finally, use the reward model to improve the language model through reinforcement learning. The language model generates responses, the reward model scores them, and the language model learns to generate responses that get higher scores.

The specific algorithm is Proximal Policy Optimization (PPO), which carefully balances two objectives: maximize the reward model's score while not drifting too far from the original model. As the paper notes: "We use this reward model as a reward function to fine-tune the supervised learning baseline using the PPO algorithm" [Ouyang et al., 2022, p. 2].

The "not drifting too far" part is crucial. Without it, the model might learn to "game" the reward model—finding outputs that score highly but aren't actually good. By keeping the model close to its starting point, the training remains stable.

## The Results: Smaller but Better

The results were striking. The InstructGPT paper reports: "On our main dataset of user prompts, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters" [Ouyang et al., 2022, p. 1].

Read that again. A model with 1.3 billion parameters—tiny by modern standards—outperformed GPT-3's 175 billion parameters, simply because it was aligned with human preferences.

This wasn't about capability in the abstract sense. It was about usefulness. The smaller aligned model was more helpful, less toxic, and more likely to do what users actually wanted.

The paper breaks down the improvements: "InstructGPT shows improvements in truthfulness over GPT-3... InstructGPT also generates less toxic outputs... and can follow explicit constraints in instructions better" [Ouyang et al., 2022, p. 8].

This represented a fundamental shift. The race wasn't just about making bigger models. It was about making models that understood human intent.

## The Reasoning Problem

But RLHF wasn't the only piece of the alignment puzzle. There was another problem: reasoning. Even if a model wants to be helpful, it needs to be able to think through problems step by step.

GPT-3, despite its size, struggled with multi-step reasoning. Ask it to solve a math problem requiring several logical steps, and it would often jump to an answer without showing its work—and frequently get it wrong.

The Chain-of-Thought paper identified this limitation: "While scaling up model size has been shown to confer various benefits... the ability to perform complex reasoning does not necessarily emerge from simply scaling up the model size" [Wei et al., 2022, p. 1].

The issue wasn't that the model couldn't reason. The issue was that it tried to do everything in one step. Think about how you solve a complex problem: you break it down, work through intermediate steps, check your work, and arrive at an answer. Language models trained on raw text don't naturally do this—they try to leap directly to the conclusion.

## Chain-of-Thought as Alignment

The solution was surprisingly simple: show the model examples of step-by-step reasoning. Instead of providing just the final answer, demonstrate the thinking process.

Consider this problem: "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?"

A standard prompt shows: "Answer: 11 balls."

A chain-of-thought prompt shows: "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11."

This small change had dramatic effects. The paper reports: "Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks" [Wei et al., 2022, p. 1].

On the GSM8K math dataset, chain-of-thought prompting increased GPT-3's accuracy from 17.9% to 40.7% [Wei et al., 2022, p. 4]. That's not a small improvement—it's a transformation.

Why does this work? The authors explain: "The first is that reasoning requires decomposing multi-step problems into intermediate steps... The second potential benefit is that it provides an interpretable window into the behavior of the model" [Wei et al., 2022, p. 2].

But there's something deeper happening. Chain-of-thought is a form of alignment. By demonstrating reasoning processes, you're teaching the model how to think—not just what to conclude.

## The Meta-Learning Connection

Here's where it gets interesting. Chain-of-thought prompting and RLHF might seem like completely different techniques. One involves showing examples of reasoning in the prompt. The other involves training with human feedback. But they share a fundamental principle: teaching models to adopt human cognitive strategies.

In-context learning—the ability to learn from examples in the prompt—is itself a kind of meta-learning. During pre-training, language models see thousands of different text types, each with its own patterns and conventions. To predict well, they must learn to recognize what type of text they're processing and adopt appropriate patterns.

Chain-of-thought leverages this meta-learned ability. When you show examples of explicit reasoning, you're telling the model: "For this task, adopt a reasoning strategy." The model recognizes the pattern and generates similar reasoning chains.

The paper hints at this emergent property: "Chain-of-thought prompting is an emergent ability of model scale—that is, this method does not positively impact performance until used with models of a certain scale" [Wei et al., 2022, p. 10].

Smaller models can't do chain-of-thought reasoning effectively. They see the examples but can't extract and apply the meta-pattern. Larger models can, without being explicitly trained for it.

Similarly, RLHF teaches a meta-skill: "understand what the human wants and try to provide it." The reward model encodes human preferences, and the language model learns to generate outputs that satisfy those preferences across diverse contexts.

Both techniques are teaching the model something more general than specific facts or behaviors. They're teaching cognitive strategies, ways of processing information that align with human thinking.

## The Broader Alignment Challenge

These successes raise a profound question: what does alignment actually mean?

At first glance, it seems simple: make the model do what we want. But what do we want? Different people want different things. What's helpful to one person might be harmful to another. What's considered appropriate changes across cultures and contexts.

The InstructGPT paper acknowledges this: "We cannot define 'alignment' in a way that is both precise and universally agreed upon" [Ouyang et al., 2022, p. 1].

The approach they took was pragmatic: train the model to follow instructions from a diverse set of users while avoiding clearly harmful outputs. The labelers were given guidelines emphasizing helpfulness, truthfulness, and harmlessness. But these guidelines themselves reflect choices about values.

Chain-of-thought prompting reveals another dimension of alignment: transparency. When a model shows its reasoning, you can understand why it arrived at a particular answer. You can identify where it went wrong. You can correct its thinking.

The paper notes this benefit: "A third potential benefit is that it may be easier to debug reasoning pathways in chain-of-thought prompting than in standard prompting" [Wei et al., 2022, p. 2].

This matters because alignment isn't just about getting the right answer—it's about trust. If a model can explain its reasoning, you can verify its conclusions. If it jumps to an answer without showing its work, you have to trust it blindly.

## Scaling and Emergence

Both papers reveal something important about how capabilities emerge with scale. Chain-of-thought reasoning doesn't work with small models. RLHF is more effective with larger models. Why?

The hypothesis is that larger models develop richer internal representations. They don't just memorize facts—they build abstract models of concepts, relationships, and reasoning patterns. These representations enable meta-learning: the ability to recognize what kind of task you're facing and adapt appropriately.

As Wei et al. observe: "This result suggests that chain-of-thought reasoning may be an emergent ability of model scale" [Wei et al., 2022, p. 10].

This has profound implications. If important capabilities emerge unpredictably at larger scales, we might not know what our models can do until we build them. And if alignment techniques like RLHF and chain-of-thought become more powerful with scale, the same models that pose greater risks might also be more amenable to alignment.

## The Human-AI Partnership

What these techniques really represent is a new way of thinking about human-AI interaction. Instead of programming specific behaviors, we're teaching models to understand and adopt human cognitive strategies.

RLHF doesn't hardcode responses—it teaches the model to predict what humans would prefer in diverse situations. Chain-of-thought doesn't program reasoning—it demonstrates what reasoning looks like, and the model learns to generate similar patterns.

This is closer to how we teach humans than how we program computers. You don't give someone a lookup table of correct responses. You teach them principles, show them examples, and help them develop judgment.

The InstructGPT paper frames this explicitly: "Our goal is to train models that can follow instructions without explicit demonstrations for each task" [Ouyang et al., 2022, p. 1].

The vision is a model that understands intent—that can take a vague or underspecified instruction and figure out what you actually want. That requires not just pattern matching, but something closer to understanding.

## Looking Forward

These papers mark a transition point in AI development. The focus is shifting from pure capability to aligned capability. From "what can the model do?" to "can we make it do what we want?"

The techniques are still evolving. RLHF has limitations—it's expensive, it depends on human labelers whose preferences might not represent all users, and the reward model can be gamed. Chain-of-thought improves reasoning but doesn't solve it completely.

But the direction is clear. Future AI systems will be increasingly shaped by human feedback and demonstrations of human cognitive strategies. The question isn't just how to make models more capable, but how to make them more aligned, more transparent, and more trustworthy.

As these models become more powerful, alignment becomes not just desirable but essential. A highly capable but misaligned system is dangerous. A less capable but well-aligned system is useful.

The InstructGPT results demonstrate this vividly: a much smaller aligned model outperformed a much larger unaligned one, simply because it better understood what humans wanted [Ouyang et al., 2022, p. 1].

This suggests that the path forward isn't just about scale. It's about the interplay between capability and alignment, between raw power and refined intention.

## The Deeper Question

Behind the technical achievements, there's a deeper question: what are we really doing when we "align" an AI system?

We're not just adjusting its outputs. We're shaping how it thinks. Chain-of-thought prompting doesn't just improve accuracy—it changes the model's reasoning process. RLHF doesn't just filter bad outputs—it reshapes the model's internal representations toward human preferences.

In a sense, we're teaching these models to simulate human cognition. Not perfectly, not completely, but in specific, useful ways. We're teaching them to break down problems like we do, to consider what would be helpful like we do, to explain their thinking like we do.

This raises fascinating philosophical questions. When a model generates a chain-of-thought explanation, is it actually reasoning, or just simulating the appearance of reasoning? When it responds to RLHF training, is it learning to be helpful, or learning to seem helpful?

The pragmatic answer is: maybe it doesn't matter. If the model's outputs are consistently helpful, truthful, and harmless—if it reliably does what we want—then it's aligned in the ways that matter.

But the question lingers. As these systems become more sophisticated, the line between simulation and genuine understanding becomes harder to draw.

## Conclusion

The journey from GPT-3 to InstructGPT represents more than technical progress. It represents a shift in how we think about AI systems.

We're moving from models trained purely on prediction to models trained on intention. From systems that generate text to systems that try to help. From black boxes that jump to conclusions to transparent reasoners that show their work.

RLHF and chain-of-thought prompting are early techniques in this direction. They're not perfect, but they demonstrate that alignment is tractable. We can teach models to understand what we want. We can train them to think through problems step by step. We can shape them to be helpful, truthful, and harmless.

As Wei et al. conclude about chain-of-thought: "This work underscores how the intermediate reasoning process plays an important role in multi-step reasoning tasks" [Wei et al., 2022, p. 10]. And as Ouyang et al. note about RLHF: "Our results suggest that fine-tuning language models with human preferences significantly improves their behavior on a wide range of tasks" [Ouyang et al., 2022, p. 8].

Together, these insights point toward a future where AI systems are not just capable, but aligned. Where they don't just predict text, but understand intent. Where they don't just generate answers, but explain their reasoning.

The alignment challenge isn't solved. But we're learning how to teach machines to think more like we do—and in doing so, to be more useful, more trustworthy, and more aligned with what we actually want.

---

*This essay synthesized insights from two key papers in AI alignment: "Training language models to follow instructions with human feedback" (Ouyang et al., 2022) and "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., 2022). All claims are directly cited to these sources.*

</EssayLayout>

export default ({ children }) => children;
