import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Transformer Architecture",
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.",
      year: "2017",
      pages: "1-15"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

The **Transformer** is a revolutionary neural network architecture that broke away from the dominant paradigm of using recurrent or convolutional neural networks for sequence transduction tasks. Instead, it relies entirely on attention mechanisms to draw global dependencies between input and output [Vaswani et al., 2017, p. 1].

## The Core Innovation

Prior to the Transformer, sequence modeling relied heavily on:
- **Recurrent Neural Networks (RNNs)**: Processing sequences step-by-step sequentially
- **Long Short-Term Memory (LSTM)**: Managing long-range dependencies
- **Convolutional Neural Networks**: Parallel processing with limited receptive fields

The Transformer dispenses with recurrence and convolutions entirely, using only attention mechanisms [Vaswani et al., 2017, p. 1].

## Architecture Components

The Transformer follows an encoder-decoder structure with these key components [Vaswani et al., 2017, pp. 3-4]:

### Encoder

The encoder consists of a stack of N = 6 identical layers. Each layer has two sub-layers:

1. **Multi-head self-attention mechanism**: Allows each position to attend to all positions in the previous layer
2. **Position-wise feed-forward network**: Two linear transformations with ReLU activation

Both sub-layers employ **residual connections** followed by **layer normalization**: `LayerNorm(x + Sublayer(x))` [Vaswani et al., 2017, p. 3].

### Decoder

The decoder also has a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer that performs multi-head attention over the encoder's output [Vaswani et al., 2017, p. 3].

The decoder's self-attention is modified with **masking** to prevent positions from attending to subsequent positions, preserving the auto-regressive property [Vaswani et al., 2017, p. 3].

## Why It Works

The Transformer achieves superior results for several reasons:

1. **Parallelization**: Unlike RNNs which must process sequentially, the Transformer can process all positions simultaneously, dramatically reducing training time [Vaswani et al., 2017, p. 2].

2. **Constant Path Length**: The maximum path length between any two positions is O(1), compared to O(n) for recurrent models. This makes learning long-range dependencies significantly easier [Vaswani et al., 2017, p. 6].

3. **Computational Efficiency**: Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality, which is typically the case [Vaswani et al., 2017, p. 6-7].

## Performance

On the WMT 2014 English-to-German translation task, the Transformer achieved a BLEU score of 28.4, improving over the existing best results by over 2.0 BLEU. The model trained for only 3.5 days on 8 GPUs [Vaswani et al., 2017, p. 1].

On English-to-French translation, it established a new single-model state-of-the-art BLEU score of 41.8, achieving this in a fraction of the training cost of previous best models [Vaswani et al., 2017, p. 1].

---

## The Library Analogy: A Story

Imagine you're a researcher in a vast library trying to write a comprehensive report.

**The Old Way (RNNs)**: You must read every book in sequence, one at a time, from beginning to end. You carry a notebook where you write summaries, but the notebook has limited pages. By the time you reach book 100, your notes from book 1 are faint and hard to recall. Reading takes forever because you can't skip ahead, and you might forget important connections between early and late books.

**The Transformer Way**: You have the magical ability to instantly scan all books simultaneously. Instead of reading sequentially, you ask questions like "Which books discuss neural networks?" and immediately all relevant pages glow, showing you exactly where to look. You can see connections between concepts across all books at once, regardless of where they appear. You don't forget early information because you're not relying on a limited-capacity notebook—you can always go back and look at any book directly.

This is the power of attention: instead of compressing everything into a sequential memory, you maintain access to all information and selectively focus on what's relevant.

</WikiLayout>

export default ({ children }) => children;
