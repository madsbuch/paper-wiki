import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Kernel Fusion",
  description: "GPU optimization technique that combines multiple operations into a single kernel to eliminate intermediate memory writes and reads, dramatically improving performance for memory-bound operations",
  category: "Optimization & Efficiency",
  tags: ["gpu-optimization", "performance", "memory", "cuda"],
  citations: [
    {
      paper: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      authors: "Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C.",
      year: "2022",
      pages: "4"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

**Kernel Fusion** is a GPU optimization technique that combines multiple operations into a single GPU kernel, eliminating the need to write intermediate results to global memory (HBM) and read them back. This dramatically reduces memory traffic and improves performance for memory-bound operations [Dao et al., 2022, p.4].

## The Problem: Kernel Boundaries and Memory Traffic

### Separate Kernels

In standard GPU programming, each operation launches a separate kernel:

**Example: Computing attention**
```python
# Kernel 1: Matrix multiplication
S = torch.matmul(Q, K.transpose(-2, -1))  # Write S to HBM

# Kernel 2: Softmax
P = torch.softmax(S, dim=-1)  # Read S from HBM, write P to HBM

# Kernel 3: Matrix multiplication
O = torch.matmul(P, V)  # Read P from HBM, write O to HBM
```

**Memory traffic:**
- Kernel 1: Read Q, K (2Nd), write S (N²)
- Kernel 2: Read S (N²), write P (N²)
- Kernel 3: Read P (N²), V (Nd), write O (Nd)
- **Total:** 3Nd + 4N² memory accesses [Dao et al., 2022, p.3]

**Problem:** For N=1024, d=64, the intermediate matrices S and P require multiple GB of memory traffic!

### Why Separate Kernels?

**Question:** Why not fuse by default?

**Reasons:**
1. **Simplicity:** Each operation is independent, easier to implement
2. **Modularity:** Reusable kernels for different operations
3. **Framework design:** Deep learning frameworks (PyTorch, TensorFlow) historically operated at this granularity
4. **Automatic differentiation:** Easier to track gradients with discrete operations

**Cost:** Significant performance loss from memory traffic

## The Solution: Fused Kernels

### Combining Operations

**Kernel fusion** combines multiple operations into one kernel that:
1. Reads inputs once from global memory
2. Performs all intermediate computations in fast memory (SRAM/registers)
3. Writes only final output to global memory

**Fused attention kernel:**
```cuda
__global__ void fused_attention_kernel(Q, K, V, O) {
    // Load Q, K, V tiles to shared memory (SRAM)
    __shared__ float tile_Q[...], tile_K[...], tile_V[...];

    // Compute S = QK^T in shared memory
    float S_local[...];
    matmul_shared(tile_Q, tile_K, S_local);

    // Compute P = softmax(S) in shared memory/registers
    float P_local[...];
    softmax_local(S_local, P_local);

    // Compute O = PV in shared memory
    float O_local[...];
    matmul_local(P_local, tile_V, O_local);

    // Write O to global memory
    store_global(O_local, O);
}
```

**Memory traffic:**
- Read Q, K, V (3Nd)
- Write O (Nd)
- **Total:** 4Nd memory accesses [Dao et al., 2022, p.4]
- **Speedup:** (3Nd + 4N²) / 4Nd ≈ N²/Nd = N/d ≈ 16× for N=1024, d=64!

## FlashAttention's Kernel Fusion

### Fused Operations

FlashAttention fuses the entire attention computation [Dao et al., 2022, p.4]:

**Fused operations within each tile:**
1. Matrix multiply: S_ij = Q_i K_j^T
2. Softmax: P_ij = softmax(S_ij)
3. Matrix multiply: O_i += P_ij V_j
4. Normalization: Update running softmax statistics (m, ℓ)

**Key benefit:** The N×N matrices S and P never touch global memory—they exist only in SRAM/registers [Dao et al., 2022, p.4].

### Implementation Strategy

**Challenges:**
1. **Softmax normalization:** Requires seeing all values (solved by online softmax)
2. **Memory capacity:** Must fit tiles in SRAM (solved by tiling)
3. **Backward pass:** Must recompute attention matrix (solved by selective recomputation)

**Result:** Exact attention with minimal memory traffic [Dao et al., 2022, p.5].

## Benefits of Kernel Fusion

### 1. Reduced Memory Traffic

**Primary benefit:** Eliminate reads/writes of intermediate results.

**Impact:** For memory-bound operations (most deep learning), this directly translates to speedup.

**Example:** FlashAttention achieves 3-4× speedup primarily from kernel fusion [Dao et al., 2022, p.8].

### 2. Improved Memory Bandwidth Utilization

**Effect:** More effective use of available memory bandwidth.

**Reason:** Fewer, larger memory transactions are more efficient than many small ones.

**Measurement:** Kernel fusion can increase memory bandwidth utilization from 40% to 80%+.

### 3. Reduced Kernel Launch Overhead

**Overhead:** Each kernel launch has fixed cost (~1-10μs).

**Benefit:** Fusing 3 kernels → save 2 kernel launches.

**Impact:** Significant for small operations, less important for large ones.

### 4. Enabling Other Optimizations

**Synergy with tiling:** Fusion allows processing tiles completely in SRAM.

**Synergy with recomputation:** Can recompute fused operations efficiently.

**Synergy with mixed precision:** Can use different precisions for different operations within kernel.

## When to Apply Kernel Fusion

### High-Impact Scenarios

**1. Memory-bound pipelines**
- Operations where memory access &gt;&gt; computation
- Multiple operations with large intermediate results
- Example: Attention (matmul → softmax → matmul)

**2. Small operations**
- Operations where kernel launch overhead is significant
- Many tiny kernels in sequence
- Example: Element-wise operations, activations

**3. Data-dependent patterns**
- Operations that must process data in specific order
- Cannot be easily parallelized across kernels
- Example: Softmax, layer normalization

### Low-Impact Scenarios

**1. Compute-bound operations**
- Operations where computation &gt;&gt; memory access
- Fusion doesn't reduce the bottleneck
- Example: Large matrix multiplications on modern hardware

**2. Irregular patterns**
- Operations with unpredictable control flow
- Hard to fuse efficiently
- Example: Sparse operations with dynamic sparsity

**3. Limited SRAM**
- Fused operations too large to fit in SRAM
- Must split anyway, reducing fusion benefits

## Trade-offs and Challenges

### Code Complexity

**Separate kernels:**
- Simple, modular code
- Easy to understand and maintain
- Framework-friendly (PyTorch, TensorFlow)

**Fused kernels:**
- Complex, monolithic code
- Harder to debug and modify
- Requires custom CUDA/Triton kernels

**Decision:** Fuse when performance gain justifies complexity.

### Compilation and Tuning

**Challenge:** Fused kernels often require problem-specific tuning.

**Factors to tune:**
- Block sizes
- Thread counts
- Shared memory allocation
- Register usage

**Effort:** Significant engineering time for optimal performance.

### Portability

**Separate kernels:**
- Hardware-agnostic
- Run on CPU, GPU, TPU with minimal changes

**Fused kernels:**
- Hardware-specific optimizations
- May need different implementations for different GPUs

## Implementation Approaches

### Manual Fusion (CUDA)

**Approach:** Write custom CUDA kernels with fused operations.

**Pros:**
- Maximum control and performance
- Can optimize for specific hardware

**Cons:**
- High development cost
- Requires CUDA expertise
- Maintenance burden

**Example:** FlashAttention's CUDA implementation [Dao et al., 2022, p.7].

### Compiler-Based Fusion (XLA, TorchScript)

**Approach:** Let compiler automatically fuse operations.

**Pros:**
- Automatic, no manual effort
- Works with existing code

**Cons:**
- Limited fusion opportunities
- Less control over optimization
- May not achieve optimal performance

**Example:** TensorFlow's XLA compiler, PyTorch's TorchScript.

### DSL-Based Fusion (Triton, Halide)

**Approach:** Use domain-specific language for kernel description.

**Pros:**
- Easier than raw CUDA
- Good performance
- More portable

**Cons:**
- Still requires kernel programming
- Limited to supported operations

**Example:** FlashAttention also has Triton implementation [Dao et al., 2022, p.7].

## Creative Analogy: The Assembly Line

Imagine manufacturing a product with three steps: cut, paint, dry.

### Unfused Approach (Separate Kernels)

**Setup:** Three separate stations (cut station, paint station, dry station).

**Process:**
1. Cut all pieces at cut station → store in warehouse
2. Retrieve all pieces from warehouse → move to paint station
3. Paint all pieces at paint station → store in warehouse
4. Retrieve all pieces from warehouse → move to dry station
5. Dry all pieces at dry station → store finished products

**Problems:**
- Constant trips to/from warehouse (memory traffic)
- Pieces sit idle between stations
- Workers wait for pieces to arrive

**Time:** 1 hour cutting + 30 min warehouse + 1 hour painting + 30 min warehouse + 1 hour drying = 4 hours

### Fused Approach (Kernel Fusion)

**Setup:** Integrated assembly line where all stations are adjacent.

**Process:**
1. Cut piece → immediately pass to painter (no warehouse)
2. Paint piece → immediately pass to dryer (no warehouse)
3. Dry piece → store finished product
4. Repeat continuously

**Benefits:**
- No warehouse trips (no intermediate memory accesses)
- Pieces flow smoothly through pipeline
- Workers always productive

**Time:** 1 hour (throughput limited by slowest station, no warehouse overhead)

**Key insight:** By fusing operations into one continuous process, we eliminate the expensive warehouse (global memory) trips.

## Practical Examples

### Element-wise Operations

**Unfused:**
```python
x = input + bias          # Kernel 1: write x to HBM
y = activation(x)          # Kernel 2: read x, write y to HBM
z = dropout(y)             # Kernel 3: read y, write z to HBM
```

**Fused:**
```python
z = fused_add_act_dropout(input, bias)  # One kernel, no intermediate writes
```

**Speedup:** 3-5× (dominated by memory access elimination)

### Layer Normalization

**Unfused:**
```python
mean = x.mean(dim=-1)              # Kernel 1
var = x.var(dim=-1)                # Kernel 2 (or combined with mean)
normalized = (x - mean) / sqrt(var + eps)  # Kernel 3
scaled = normalized * gamma + beta  # Kernel 4
```

**Fused:**
```python
scaled = fused_layer_norm(x, gamma, beta)  # One kernel
```

**Speedup:** 2-3× (reduced memory accesses)

### Attention (FlashAttention)

**Unfused:**
```python
S = Q @ K.T       # Kernel 1: write N²
P = softmax(S)    # Kernel 2: read/write N²
O = P @ V         # Kernel 3: read N²
```

**Fused (with tiling):**
```python
O = fused_attention(Q, K, V)  # One kernel, no N² writes
```

**Speedup:** 3-4× for typical sequence lengths [Dao et al., 2022, p.8]

## Measuring Fusion Effectiveness

### Memory Traffic Reduction

**Metric:** Compare HBM reads/writes before and after fusion.

**Tool:** NVIDIA Nsight Compute profiler.

**Target:** Reduce HBM traffic proportional to eliminated intermediate results.

### Bandwidth Utilization

**Metric:** Achieved memory bandwidth / theoretical bandwidth.

**Before fusion:** Often 20-40% (many small transfers)
**After fusion:** Should be 60-80%+ (fewer, larger transfers)

### Speedup

**Metric:** Wall-clock time before / after fusion.

**Expected:** For memory-bound operations, speedup ≈ memory traffic reduction.

**Example:** FlashAttention: 3-4× speedup from 3-4× memory traffic reduction [Dao et al., 2022, p.8].

## Related Concepts

- [IO-Aware Algorithms](/wiki/io-aware-algorithms)
- [GPU Memory Hierarchy](/wiki/gpu-memory-hierarchy)
- [Tiling Techniques](/wiki/tiling-techniques)
- [Attention Mechanism](/wiki/attention-mechanism)

## References

- Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems (NeurIPS 2022).
- Chen, T., et al. (2018). TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. In OSDI 2018.
- Ragan-Kelley, J., et al. (2013). Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines. In PLDI 2013.

</WikiLayout>
