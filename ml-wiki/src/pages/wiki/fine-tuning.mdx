import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Fine-Tuning",
  description: "Adapting a pretrained model to a specific task by continuing training on task-specific data with minimal additional parameters.",
  category: "Training Method",
  difficulty: "Beginner",
  citations: [
    {
      paper: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      authors: "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K.",
      year: "2018",
      pages: "1-2"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

Fine-tuning is the process of taking a pretrained model and adapting it to a specific downstream task by training it on task-specific data [Devlin et al., 2018, p. 1].

## The Two-Stage Paradigm

Modern NLP follows a two-stage approach:
1. **Pretraining**: Train on large amounts of unlabeled data to learn general language representations
2. **Fine-tuning**: Adapt to specific tasks with labeled data

BERT pioneered fine-tuning with just one additional output layer, making it extremely simple to adapt the model [Devlin et al., 2018, p. 1].

## Why It Works

Fine-tuning leverages transfer learning—knowledge learned during pretraining transfers to the downstream task. The model already understands language, so it only needs to learn task-specific patterns [Devlin et al., 2018, p. 2].

## A Simple Analogy

Think of pretraining as general education and fine-tuning as job training. You learn fundamentals in school (pretraining), then quickly adapt those skills to your specific job (fine-tuning). You don't start from scratch each time you change jobs.

## The BERT Approach

BERT showed that you can achieve state-of-the-art performance on a wide range of tasks by simply adding a classification layer and fine-tuning all parameters [Devlin et al., 2018, p. 1].

---

**Related Concepts:** [Transfer Learning](/wiki/transfer-learning) · [Bidirectional Pretraining](/wiki/bidirectional-pretraining) · [Masked Language Model](/wiki/masked-language-model)

</WikiLayout>

export default ({ children }) => children;
