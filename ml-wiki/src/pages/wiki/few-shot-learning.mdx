import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Few-Shot Learning",
  citations: [
    {
      paper: "Language Models are Few-Shot Learners (GPT-3)",
      authors: "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., et al.",
      year: "2020",
      pages: "5-7"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

**Few-shot learning** refers to the setting where a language model is given a few demonstrations of a task at inference time as conditioning, but **no weight updates are allowed** [Brown et al., 2020, p. 6]. It represents a middle ground between zero-shot and traditional fine-tuning approaches.

## The Mechanism

For a typical dataset where an example has a context and a desired completion (e.g., an English sentence and its French translation), few-shot learning works by [Brown et al., 2020, p. 6]:

1. Providing K examples of context and completion
2. Then providing one final example of context
3. The model generates the completion based on the pattern it recognizes

The number K is typically set in the range of **10 to 100**, limited by the model's context window (n_ctx = 2048 tokens for GPT-3) [Brown et al., 2020, p. 6].

## Example

```
Translate English to French:
sea otter => loutre de mer
peppermint => menthe poivrÃ©e
plush giraffe => girafe peluche
cheese => ?
```

The model must infer from the examples that it should translate "cheese" to French ("fromage").

## Advantages

Few-shot learning provides several key benefits [Brown et al., 2020, p. 6]:

1. **Major reduction in task-specific data requirements**: Only need 10-100 examples vs thousands for fine-tuning
2. **Reduced overfitting risk**: Less potential to learn overly narrow distributions from large but narrow fine-tuning datasets
3. **No weight updates**: Model remains general-purpose and can switch between tasks instantly
4. **Faster adaptation**: Can perform new tasks immediately without retraining

## Disadvantages

The main challenges of few-shot learning [Brown et al., 2020, p. 6]:

1. **Lower performance than fine-tuning**: Results have historically been much worse than state-of-the-art fine-tuned models (though GPT-3 narrows this gap significantly)
2. **Still requires some data**: A small amount of task-specific data is still needed
3. **Context window limitations**: Number of examples is limited by model's context size

## Scaling with Model Size

One of the most important findings from the GPT-3 paper is that **few-shot learning performance scales dramatically with model size** [Brown et al., 2020, p. 5]:

### Performance on 42 Benchmarks (Aggregate):

| Model Size | Few-Shot Accuracy |
|------------|------------------|
| 0.1B params | 25% |
| 1.3B params | 36% |
| 6.7B params | 43% |
| 13B params | 48% |
| 175B params | **58%** |

The gap between zero-shot, one-shot, and few-shot performance grows with model capacity, suggesting that **larger models are more proficient meta-learners** [Brown et al., 2020, p. 6].

## Performance Examples

GPT-3's few-shot performance on various tasks [Brown et al., 2020, p. 5]:

- **CoQA** (conversational QA): 85.0 F1 (few-shot) vs 81.5 F1 (zero-shot)
- **TriviaQA**: 71.2% accuracy (few-shot, state-of-the-art for closed-book)
- **SuperGLUE**: Sometimes competitive with fine-tuned models
- **Arithmetic**: Can perform 2-digit addition with 100% accuracy, 3-digit addition at 80.2%

## Connection to Meta-Learning

Few-shot learning in language models is related to few-shot learning in other ML contexts [Brown et al., 2020, p. 6]. Both involve:

1. **Broad distribution learning**: Learning from a wide variety of tasks (implicit in pre-training data for LMs)
2. **Rapid adaptation**: Quickly adapting to a new task with minimal examples

The model essentially learns **how to learn** during pre-training, then applies this meta-learning ability at inference time.

## Learning Curves

The GPT-3 paper emphasizes that these "learning" curves involve **no gradient updates or fine-tuning**, just increasing numbers of demonstrations given as conditioning [Brown et al., 2020, p. 5]. Performance improves with:

- More examples in context (up to context window limit)
- Larger model size
- Addition of natural language task descriptions

---

## The Pattern Recognition Story

Imagine you're a detective who has solved thousands of different types of crimes over your career.

**Few-Shot Learning**: You arrive at a crime scene. Someone shows you 3-5 photos of similar crimes from last week and says "We have a serial pattern here." You immediately recognize:
- "Ah, this is a breaking-and-entering pattern"
- The entry method
- The items targeted
- The likely next target

You don't need to study these crimes for weeks. Your vast experience means you can recognize the pattern from just a handful of examples and act accordingly.

**Fine-Tuning (for comparison)**: Would be like spending weeks studying ONLY breaking-and-entering cases until you become hyper-specialized in them. You'd be extremely good at that one crime type, but you'd lose your general detective flexibility.

**Zero-Shot (for comparison)**: Would be like arriving at the scene with just a verbal description: "It's a breaking-and-entering." No photos, no examples - just the category name.

Few-shot gives you enough examples to recognize the pattern while keeping you flexible. Your years of diverse training (pre-training on varied data) mean you can quickly adapt to this specific pattern without specialized retraining.

The power comes from the **combination** of:
1. Broad experience (pre-training)
2. A few specific examples (few-shot conditioning)
3. No need for retraining (no weight updates)

</WikiLayout>

export default ({ children }) => children;
