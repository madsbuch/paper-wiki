import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Transformer Architecture",
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.",
      year: "2017",
      pages: "2-5"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

The **Transformer** is a model architecture that eschews recurrence entirely and relies solely on an attention mechanism to draw global dependencies between input and output [Vaswani et al., 2017, p. 2]. It represents a fundamental shift from sequential processing (RNNs) to parallel processing via attention.

The Transformer follows an encoder-decoder architecture using stacked self-attention and point-wise, fully connected layers [Vaswani et al., 2017, p. 3].

## Key Innovation

Unlike recurrent models which process sequences step-by-step, the Transformer allows for **significantly more parallelization** and can be trained much faster. For machine translation, it achieved a new state of the art while training for as little as twelve hours on eight P100 GPUs [Vaswani et al., 2017, p. 2].

## Architecture Components

### Encoder Stack

The encoder is composed of a **stack of N = 6 identical layers** [Vaswani et al., 2017, p. 3]. Each layer has two sub-layers:

1. **Multi-head self-attention mechanism**: Allows each position to attend to all positions in the previous layer
2. **Position-wise fully connected feed-forward network**: Applied to each position separately and identically

Around each of the two sub-layers, the architecture employs:
- **Residual connections** [Vaswani et al., 2017, p. 3]
- **Layer normalization**: The output of each sub-layer is LayerNorm(x + Sublayer(x))

All sub-layers and embedding layers produce outputs of dimension **d_model = 512** [Vaswani et al., 2017, p. 3].

### Decoder Stack

The decoder is also composed of a **stack of N = 6 identical layers** [Vaswani et al., 2017, p. 3]. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer:

1. **Masked multi-head self-attention**: Modified to prevent positions from attending to subsequent positions
2. **Multi-head attention over encoder output**: Keys and values come from the encoder stack
3. **Position-wise fully connected feed-forward network**

The masking, combined with the fact that output embeddings are offset by one position, ensures that predictions for position i can depend only on known outputs at positions less than i [Vaswani et al., 2017, p. 3].

## Hyperparameters

### Base Model Configuration

The base Transformer model uses the following hyperparameters [Vaswani et al., 2017, p. 5, 9]:

- **Number of layers (N)**: 6 (both encoder and decoder)
- **Model dimension (d_model)**: 512
- **Feed-forward dimension (d_ff)**: 2048
- **Number of attention heads (h)**: 8
- **Dimension per head (d_k = d_v)**: 64 (= d_model / h)
- **Dropout rate (P_drop)**: 0.1
- **Label smoothing (ε_ls)**: 0.1
- **Total parameters**: ~65 million

### Big Model Configuration

For their best results, the authors used a "big" variant [Vaswani et al., 2017, p. 9]:

- **Model dimension (d_model)**: 1024
- **Feed-forward dimension (d_ff)**: 4096
- **Number of attention heads (h)**: 16
- **Dropout rate (P_drop)**: 0.3
- **Total parameters**: ~213 million

## Position-wise Feed-Forward Networks

Each layer contains a fully connected feed-forward network that is applied to each position separately and identically [Vaswani et al., 2017, p. 5]. This consists of two linear transformations with a ReLU activation in between.

## Positional Encoding

Since the Transformer contains no recurrence and no convolution, the model must inject information about the relative or absolute position of tokens in the sequence [Vaswani et al., 2017, p. 6]. The authors use **sinusoidal positional encodings** added to the input embeddings at the bottom of the encoder and decoder stacks.

## Training Details

The base model was trained with the following specifications [Vaswani et al., 2017, p. 8]:

- **Optimizer**: Adam
- **Training steps**: 100,000 steps
- **Hardware**: 8 NVIDIA P100 GPUs
- **Training time**: 12 hours for base model, 3.5 days for big model
- **Residual dropout**: Applied to output of each sub-layer before addition and normalization
- **Embedding dropout**: Applied to sums of embeddings and positional encodings

## Performance

The Transformer achieved state-of-the-art results on machine translation tasks [Vaswani et al., 2017, p. 8]:

- **WMT 2014 English-to-German**: 28.4 BLEU (big model)
- **WMT 2014 English-to-French**: 41.8 BLEU (big model)
- **English Constituency Parsing**: 92.7 F1 score

These results were achieved at a fraction of the training cost compared to previous models.

## Advantages Over RNNs

The Transformer architecture offers several key advantages over recurrent models [Vaswani et al., 2017, p. 6]:

1. **Constant path length**: O(1) operations to relate any two positions (vs. O(n) for RNNs)
2. **Parallelization**: All positions processed simultaneously
3. **Training speed**: Significantly faster training due to parallelization
4. **Long-range dependencies**: Easier to learn due to constant path length
5. **Interpretability**: Attention mechanisms are more interpretable than recurrent hidden states

---

## The Assembly Line Story

Imagine two factories making complex products:

**The RNN Factory (Sequential Processing)**: Workers stand in a line. The first worker receives raw materials and does their job, passing the result to the next worker. Worker 2 can't start until Worker 1 finishes. Worker 10 can't even see what Worker 1 did—they only get what Worker 9 hands them. If Worker 1 discovers something important, that information must pass through 8 other workers before reaching Worker 10, getting diluted each time.

**The Transformer Factory (Parallel Processing)**: All workers stand around a large circular table with all the materials visible in the center. When the whistle blows, every worker simultaneously looks at everything on the table (this is self-attention). Each worker decides what's relevant to their specific job.

Worker 1 might notice parts A and B fit together. Worker 10 can directly see this too—they don't need to wait for the message to travel through 8 other workers. Within a single moment, all workers understand the entire context and can work simultaneously.

The Transformer factory produces the same product in 12 hours instead of 3.5 days, and the quality is better because no worker ever has to rely on diluted second-hand information.

## Related Concepts

- [Self-Attention](/wiki/self-attention)
- [Multi-Head Attention](/wiki/multi-head-attention)

</WikiLayout>

export default ({ children }) => children;
