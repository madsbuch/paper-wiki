import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Autoregressive Language Model",
  citations: [
    {
      paper: "Language Models are Few-Shot Learners (GPT-3)",
      authors: "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., et al.",
      year: "2020",
      pages: "5, 8"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

An **autoregressive language model** is a type of language model that generates text by predicting each token based on all previous tokens in the sequence [Brown et al., 2020, p. 5]. GPT-3 is a 175 billion parameter autoregressive language model.

## How It Works

In an autoregressive model:

1. The model receives a sequence of tokens as input
2. For each position, it predicts the next token based on all previous tokens
3. During generation, each predicted token becomes part of the context for predicting the next token
4. This process continues sequentially, one token at a time

The key characteristic is that the model can only attend to **previous tokens**, not future ones - making it autoregressive.

## Architecture

GPT-3 uses the same model and architecture as GPT-2, with modifications including [Brown et al., 2020, p. 8]:

- Modified initialization
- Pre-normalization
- Reversible tokenization
- Alternating dense and locally banded sparse attention patterns (similar to Sparse Transformer)

The model scales across 8 different sizes, from 125 million to 175 billion parameters [Brown et al., 2020, p. 8].

## Context Window

All GPT-3 models use a **context window of n_ctx = 2048 tokens** [Brown et al., 2020, p. 8]. This means:

- The model can attend to up to 2048 previous tokens
- For few-shot learning, K examples must fit within this window
- Typically allows 10-100 examples for few-shot learning, depending on example length

## Training

GPT-3 was trained on [Brown et al., 2020, p. 8]:

- A filtered version of Common Crawl (570GB after filtering)
- High-quality reference corpora including WebText, Books1, Books2, and Wikipedia
- 300 billion tokens total
- Training involved scaling up model size, dataset size, and training length

## Advantages for In-Context Learning

The autoregressive structure is particularly well-suited for in-context learning because:

1. **Natural task framing**: Examples can be provided sequentially in the context
2. **Pattern recognition**: The model learns to recognize task patterns from the sequence of examples
3. **No architectural changes needed**: In-context learning emerges naturally from the autoregressive objective

---

## The Storyteller Analogy

Imagine a storyteller who must continue a story one word at a time.

**Autoregressive Process**:
- They hear: "Once upon a time, there was a brave..."
- They can only use those words to predict the next word: "knight"
- Now they hear: "Once upon a time, there was a brave knight..."
- They predict: "who"
- The context grows: "Once upon a time, there was a brave knight who..."
- They predict: "lived"

Each prediction is based on **all previous words** but cannot look ahead to future words. The storyteller builds the narrative step by step, where each word depends on the entire history but not on what comes next.

**In-Context Learning**: When you show the storyteller several example stories in a specific genre (say, fairy tales), they pick up on the patterns and can generate new stories in that style - all without being explicitly retrained. They learned the meta-skill of recognizing story patterns during their years of reading (pre-training).

This is how GPT-3 works: autoregressive token prediction that enables natural in-context learning.

## Related Concepts

- [In-Context Learning](/wiki/in-context-learning)
- [Transformer Architecture](/wiki/transformer-architecture)

</WikiLayout>

export default ({ children }) => children;
