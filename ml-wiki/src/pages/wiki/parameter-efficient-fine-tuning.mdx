export const meta = {
  title: "Parameter-Efficient Fine-Tuning (PEFT)",
  description: "Techniques for adapting large pre-trained models to downstream tasks while training only a small fraction of parameters",
  category: "Training Techniques",
  tags: ["fine-tuning", "efficiency", "transfer-learning", "adaptation"],
  citations: [
    {
      title: "LoRA: Low-Rank Adaptation of Large Language Models",
      authors: "Hu et al.",
      year: 2021,
      slug: "lora",
      pages: "3, 6, 9"
    }
  ]
};

# Parameter-Efficient Fine-Tuning (PEFT)

**Parameter-Efficient Fine-Tuning (PEFT)** refers to a family of techniques for adapting large pre-trained models to downstream tasks while training only a small fraction of the model's parameters. These methods address the computational and storage challenges of traditional fine-tuning as models scale to hundreds of billions of parameters.

## Motivation

### The Full Fine-Tuning Problem

Traditional transfer learning follows this paradigm:
1. Pre-train a model on large-scale general data
2. Fine-tune all parameters on task-specific data

This approach becomes prohibitively expensive for very large models [Hu et al., 2021, p.1]:

**For GPT-3 175B:**
- Each fine-tuned instance requires 175B parameters (~350GB storage)
- Deploying 100 task-specific models requires ~35TB
- Training requires massive GPU memory and compute
- Switching between tasks requires loading entirely new models

### The PEFT Solution

PEFT methods adapt models by:
- **Freezing** the pre-trained model weights
- Training only a **small set of additional or selected parameters**
- Achieving comparable or better performance than full fine-tuning
- Dramatically reducing memory, storage, and compute requirements

## Major PEFT Approaches

### 1. Adapter Layers

**Concept:** Insert small neural network modules (adapters) between layers of the pre-trained model.

**How it works:**
- Add bottleneck feed-forward layers after attention and MLP modules
- Only train the adapter parameters while freezing the original model
- Typically 2 fully-connected layers with a nonlinearity between them

**Variants:**
- **AdapterH** (Houlsby et al., 2019): Two adapters per Transformer block
- **AdapterL** (Lin et al., 2020): One adapter per block, after MLP module
- **AdapterP** (Pfeiffer et al., 2021): Similar to AdapterL with additional LayerNorm

**Advantages:**
- Modular design
- Easy to add/remove for different tasks
- Well-studied with many variants

**Disadvantages:**
- Introduces inference latency (20-30% in online scenarios)
- Sequential computation cannot be fully parallelized
- Requires synchronization in distributed settings

[Hu et al., 2021, p.3, 6]

### 2. Prefix Tuning / Prompt Tuning

**Concept:** Add trainable "prefix" or "prompt" tokens to the input or intermediate activations.

**Variants:**

**Prefix-Embedding Tuning:**
- Inserts special trainable tokens at the input level
- Only the embeddings of these tokens are trainable
- Positions can be "prefix" (prepend) or "infix" (append)

**Prefix-Layer Tuning:**
- Extends prefix-embedding by making activations trainable at every layer
- Instead of letting representations evolve naturally, replace them with trainable vectors

**Advantages:**
- No architectural changes to the model
- Relatively simple to implement
- No additional parameters in the base model

**Disadvantages:**
- Reduces available sequence length for actual task tokens
- Performance can be unstable and "changes non-monotonically" with number of parameters
- Difficult to optimize, especially on small datasets
- Poor performance in low-data regimes (e.g., 37.6% on MNLI-100 vs 63.8% for LoRA)

[Hu et al., 2021, p.3, 6, 23]

### 3. Low-Rank Adaptation (LoRA)

**Concept:** Inject trainable low-rank decomposition matrices in parallel to existing weights.

**How it works:**
- Represent weight updates as ΔW = BA where B and A are low-rank
- Train only B and A while freezing W₀
- Can merge during inference: W = W₀ + BA

**Advantages:**
- No inference latency (unlike adapters)
- Massive parameter reduction (10,000× for GPT-3)
- Better performance than adapters and prefix methods
- Easy task switching

**Details:** See [Low-Rank Adaptation](/wiki/low-rank-adaptation)

[Hu et al., 2021, p.1-5]

### 4. BitFit / Bias-Only Tuning

**Concept:** Train only the bias vectors while freezing all other parameters.

**Characteristics:**
- Extremely simple approach
- Minimal trainable parameters (~0.1M for RoBERTa base)
- Performance is decent but typically worse than other PEFT methods

[Hu et al., 2021, p.6]

### 5. Selective Layer Tuning

**Concept:** Train only specific layers or modules while freezing the rest.

**Example:**
- **FTTop2**: Train only the top 2 layers of the model
- Can also train specific modules (e.g., only attention, only MLP)

**Characteristics:**
- Simple to implement
- Performance varies greatly depending on which layers are chosen
- Often underperforms compared to more sophisticated methods

[Hu et al., 2021, p.6]

## Comparison of PEFT Methods

### Performance (GPT-3 175B on WikiSQL)

| Method | Trainable Params | Accuracy |
|--------|------------------|----------|
| Full Fine-Tuning | 175B | 73.8% |
| BitFit | 14.2M | 71.3% |
| Prefix-Embedding | 3.2M | 63.1% |
| Prefix-Layer | 20.2M | 70.1% |
| AdapterH | 7.1M - 40.1M | 71.9% - 73.2% |
| **LoRA** | **4.7M** | **73.4%** |

[Hu et al., 2021, p.8]

### Trade-offs

**Adapters:**
- ✅ Well-established, many variants
- ✅ Modular design
- ❌ Inference latency
- ❌ Synchronization overhead in distributed settings

**Prefix Tuning:**
- ✅ No architectural changes
- ✅ Simple concept
- ❌ Reduces sequence length
- ❌ Unstable training
- ❌ Poor low-data performance

**LoRA:**
- ✅ No inference latency
- ✅ Highest parameter efficiency
- ✅ Best performance across methods
- ✅ Easy task switching
- ❌ Cannot easily batch different tasks in single forward pass

[Hu et al., 2021, p.3-5]

## Why PEFT Works

### Intrinsic Dimensionality Hypothesis

Pre-trained language models have low "intrinsic dimension" - they can learn efficiently even when constrained to a random low-dimensional subspace (Aghajanyan et al., 2020).

PEFT methods exploit this by hypothesizing that:
- **The changes needed for adaptation** also lie in a low-dimensional space
- Most model capacity is needed for general knowledge (pre-training)
- Task-specific adaptation requires relatively few degrees of freedom

[Hu et al., 2021, p.2]

### Over-Parametrization

Large pre-trained models are heavily over-parametrized for any single downstream task. PEFT methods effectively:
- Use the pre-trained weights as a powerful feature extractor
- Add minimal task-specific capacity on top
- Avoid overfitting that can occur with full fine-tuning on small datasets

## Practical Considerations

### When to Use PEFT

**Ideal scenarios:**
- Limited compute/memory resources
- Many downstream tasks to support
- Frequent task switching required
- Small task-specific datasets
- Production deployment with latency constraints

**When full fine-tuning might be better:**
- Single task deployment
- Extremely different task from pre-training (e.g., different language)
- Abundant compute resources
- Very large task-specific datasets

### Choosing a PEFT Method

Consider:
1. **Inference Latency:** Use LoRA or Prefix methods if latency is critical
2. **Parameter Budget:** LoRA offers best performance per parameter
3. **Ease of Implementation:** BitFit is simplest, Prefix methods next, then LoRA and Adapters
4. **Stability:** LoRA and Adapters more stable than Prefix methods
5. **Low-Data Performance:** LoRA significantly outperforms others

[Hu et al., 2021, p.23]

## Creative Analogy: The Specialist Team

Imagine your pre-trained model is like a massive general hospital with thousands of expert doctors, nurses, and equipment (parameters). You want to adapt it for specialized clinics - cardiology, pediatrics, orthopedics, etc.

**Full Fine-Tuning** = Building entirely new hospitals for each specialty, each with thousands of staff. Expensive and wasteful since most medical knowledge overlaps.

**Adapter Layers** = Hiring a small specialist team for each clinic that works alongside the general hospital staff. Works well, but the specialist team has to be consulted sequentially, adding time to each patient visit.

**Prefix Tuning** = Creating special intake questionnaires that prime the general hospital for each specialty. Simple, but takes up space in the waiting room and doesn't always work reliably.

**LoRA** = Training a handful of general hospital staff to adjust their techniques slightly for each specialty. The adjustments are so seamless that when a patient arrives, it's as if the hospital was always specialized for that condition - no extra waiting time, minimal extra training needed.

The key insight: The general hospital already knows most of what's needed. You just need small, strategic adjustments, not entirely new facilities.

## Implementation Examples

### Storage Requirements

**Example: 100 Task Adaptation for GPT-3 175B**

| Method | Storage Required |
|--------|------------------|
| Full Fine-Tuning | 35 TB (100 × 350GB) |
| Adapters (r=8) | 4.3 TB (350GB + 100 × 40GB) |
| **LoRA (r=4)** | **354 GB** (350GB + 100 × 35MB) |

[Hu et al., 2021, p.5]

### Training Memory

**GPT-3 175B with Adam Optimizer:**

| Method | VRAM Usage |
|--------|------------|
| Full Fine-Tuning | 1.2 TB |
| **LoRA** | **350 GB** (3× reduction) |

[Hu et al., 2021, p.5]

## Current Trends and Adoption

PEFT methods, particularly LoRA, have seen rapid adoption:
- Integrated into major libraries (HuggingFace PEFT, PyTorch, etc.)
- Standard practice in open-source LLM community
- Enables fine-tuning on consumer hardware
- Facilitates sharing of adapted models (small LoRA weights vs. full models)

## Future Directions

Open research questions:
1. **Optimal Selection:** More principled methods for choosing which parameters/layers to adapt
2. **Theoretical Understanding:** Better formal understanding of why low-rank/sparse updates suffice
3. **Combination Methods:** How to optimally combine different PEFT approaches
4. **Task Relationships:** How to leverage relationships between tasks in multi-task PEFT
5. **Automatic Configuration:** Methods to automatically determine optimal rank/adapter size per task

[Hu et al., 2021, p.13]

## Related Concepts

- [Low-Rank Adaptation](/wiki/low-rank-adaptation)
- [Transfer Learning](/wiki/transfer-learning)
- [Adapter Layers](/wiki/adapter-layers)
- [Few-Shot Learning](/wiki/few-shot-learning)

## References

- Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.
- Aghajanyan, A., Zettlemoyer, L., & Gupta, S. (2020). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv preprint arXiv:2012.13255.
