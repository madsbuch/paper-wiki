import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Sequence-to-Sequence Models",
  category: "Architecture",
  description: "A neural network architecture that transforms one sequence into another, consisting of an encoder that processes the input and a decoder that generates the output.",
  relatedConcepts: ["encoder-decoder", "attention-mechanism", "rnn", "machine-translation"],
  citations: [
    {
      paper: "Neural Machine Translation by Jointly Learning to Align and Translate",
      authors: "Bahdanau, D., Cho, K., & Bengio, Y.",
      year: "2014",
      pages: "1-2"
    }
  ]
};

<WikiLayout {...meta}>

## What are Sequence-to-Sequence Models?

Sequence-to-sequence (seq2seq) models are neural networks designed to transform one sequence into another sequence of potentially different length. They're the foundation of modern machine translation, text summarization, dialogue systems, and many other sequence transformation tasks.

Think of seq2seq like a translator who first reads and understands an entire sentence in one language (encoding), then expresses that understanding in another language (decoding).

## The Core Architecture

A seq2seq model consists of two main components:

### 1. Encoder

The encoder processes the input sequence and compresses it into a fixed-length representation:

```
Input: x1, x2, x3, ..., xT
  ↓
Encoder RNN
  ↓
Context vector c (fixed length)
```

"The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation" [Bahdanau et al., 2014, p. 1].

### 2. Decoder

The decoder generates the output sequence from the context vector:

```
Context vector c
  ↓
Decoder RNN
  ↓
Output: y1, y2, y3, ..., yT'
```

The decoder is typically an autoregressive model, generating one token at a time based on the context and previously generated tokens.

## How Seq2Seq Works

### Encoding Phase

```
h1 = f(h0, x1)
h2 = f(h1, x2)
h3 = f(h2, x3)
...
hT = f(hT-1, xT)

c = hT  (context = final hidden state)
```

The encoder processes the input sequence token by token, updating its hidden state. The final hidden state becomes the context vector.

### Decoding Phase

```
s1 = g(s0, c, y0)  → predict y1
s2 = g(s1, c, y1)  → predict y2
s3 = g(s2, c, y2)  → predict y3
...
```

The decoder generates output tokens one at a time, using:
- The context vector c
- Its own hidden state
- The previously generated token

## Example: Machine Translation

Translating "Je suis étudiant" → "I am a student"

**Encoding:**
```
Input: [Je] [suis] [étudiant]
  ↓
Encoder RNN processes left-to-right
  ↓
Context vector c (captures meaning)
```

**Decoding:**
```
c → Decoder → [I]
c + [I] → Decoder → [am]
c + [I] + [am] → Decoder → [a]
c + [I] + [am] + [a] → Decoder → [student]
c + [I] + [am] + [a] + [student] → Decoder → [EOS]
```

## The Fixed-Length Bottleneck Problem

The original seq2seq architecture had a critical limitation:

"We conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture" [Bahdanau et al., 2014, p. 1].

**The problem:**
- Long input sequences must compress into a single fixed-size vector
- Information loss increases with sequence length
- Decoder must reconstruct everything from one vector
- Performance degrades on long sequences

This bottleneck led to the invention of the attention mechanism, which allows the decoder to access all encoder states instead of just a single context vector.

## Seq2Seq with Attention

Modern seq2seq models use attention to overcome the bottleneck:

```
Encoder:
x1, x2, ..., xT → h1, h2, ..., hT (all states available)

Decoder with Attention:
For each output position i:
  - Compute attention over all encoder states
  - Create dynamic context ci (weighted combination)
  - Generate yi using ci and previous outputs
```

This is a major improvement over the fixed context vector approach.

## Applications

### Machine Translation
- English → French
- Any language pair
- Handles variable-length inputs and outputs

### Text Summarization
- Long document → Short summary
- Encoder reads document, decoder writes summary

### Dialogue Systems
- User utterance → Bot response
- Contextual conversation generation

### Question Answering
- Question + Context → Answer
- Encoder processes both, decoder generates answer

### Code Generation
- Natural language description → Code
- Specification to implementation

## Training Seq2Seq Models

### Teacher Forcing

During training, use ground truth previous tokens instead of model predictions:

```
Training:
Decoder input: [SOS] [I] [am] [a]
Decoder output: [I] [am] [a] [student]

(Use real tokens, not predictions)
```

This speeds up training and provides stable gradients.

### Inference (No Teacher Forcing)

At test time, use model's own predictions:

```
Inference:
Generate [I]
Generate [am] (using [I])
Generate [a] (using [I] [am])
Generate [student] (using [I] [am] [a])
```

### Loss Function

Cross-entropy loss on predicted tokens:

```
Loss = -Σ log P(yt | y<t, x)
```

Minimize negative log-likelihood of correct output sequence.

## Beam Search Decoding

Instead of greedily selecting the most likely token at each step, beam search maintains multiple hypotheses:

```
Beam size = 3:
Step 1: Keep top 3 completions: [I], [The], [A]
Step 2: Expand each, keep top 3 overall
Step 3: Continue until [EOS]
```

This often finds better translations than greedy decoding.

## The Analogy: Two-Person Communication

**Without attention (original seq2seq):**
- Person A reads a long story
- Person A whispers one sentence to Person B (the bottleneck!)
- Person B must retell the entire story from that one sentence
- Information loss is inevitable

**With attention (modern seq2seq):**
- Person A reads the story and remembers all of it
- Person B can ask Person A about any part while retelling
- Person B attends to relevant parts for each sentence
- Much more accurate retelling

## Advantages

1. **End-to-end learning**: Single model, trained jointly
2. **Variable lengths**: Input and output can differ in length
3. **Flexible**: Same architecture for many tasks
4. **No explicit alignment**: Learns relationships automatically

## Limitations

1. **Fixed context bottleneck**: Without attention, long sequences suffer
2. **Sequential decoding**: Slow, can't parallelize generation
3. **Exposure bias**: Training vs inference mismatch
4. **Error propagation**: Early mistakes affect later outputs

## Modern Improvements

### Attention Mechanisms
Added dynamic context via attention over all encoder states.

### Transformers
Replaced RNNs with self-attention, enabling parallelization and better long-range dependencies.

### Copy Mechanisms
Allow decoder to copy tokens from input (useful for names, numbers).

### Coverage Mechanisms
Prevent repetition and ensure all input is covered.

## Key Takeaways

- **Seq2seq = Encoder + Decoder**: Two-stage architecture for sequence transformation
- **Fixed-length bottleneck**: Original limitation that motivated attention
- **Foundation for NMT**: Revolutionized machine translation
- **General framework**: Applicable to many sequence tasks
- **Evolved with attention**: Modern versions use attention mechanisms
- **Preceded Transformers**: Conceptual ancestor of current state-of-the-art

Sequence-to-sequence models established the encoder-decoder paradigm that remains fundamental to modern NLP, even as the specific implementations (RNNs → Transformers) have evolved.

</WikiLayout>

export default ({ children }) => children;
