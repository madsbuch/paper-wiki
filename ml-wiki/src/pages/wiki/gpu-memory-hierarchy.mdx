import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "GPU Memory Hierarchy",
  description: "The multi-level structure of memory in GPUs, from fast on-chip SRAM to slower high-bandwidth memory, with dramatic implications for algorithm performance",
  category: "Hardware & Systems",
  tags: ["gpu", "memory", "hardware", "performance", "optimization"],
  citations: [
    {
      paper: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      authors: "Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C.",
      year: "2022",
      pages: "2"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

The **GPU Memory Hierarchy** is a multi-level system of memory storage in Graphics Processing Units (GPUs), where different levels trade off capacity for speed. Understanding this hierarchy is critical for writing high-performance GPU code, as data movement between levels can dominate overall runtime [Dao et al., 2022, p.2].

## The Memory Hierarchy Levels

Modern GPUs like the NVIDIA A100 have a clear memory hierarchy [Dao et al., 2022, p.2]:

### Level 1: Registers
- **Capacity:** Smallest (~256KB per SM)
- **Bandwidth:** Highest (~20+ TB/s effective)
- **Latency:** Lowest (1 cycle)
- **Scope:** Per-thread
- **Use:** Immediate computation values

### Level 2: SRAM (On-Chip Memory / Shared Memory)
- **Capacity:** Small (~20MB total, ~164KB per SM on A100)
- **Bandwidth:** Very high (~19 TB/s on A100)
- **Latency:** Very low (~20-30 cycles)
- **Scope:** Per-block (shared across threads in a thread block)
- **Use:** Intermediate results, shared data

### Level 3: L2 Cache
- **Capacity:** Medium (~40MB on A100)
- **Bandwidth:** High (~7 TB/s)
- **Latency:** Moderate (~200 cycles)
- **Scope:** GPU-wide
- **Use:** Recently accessed data (automatic caching)

### Level 4: HBM (High Bandwidth Memory / Global Memory)
- **Capacity:** Largest (40-80GB on A100)
- **Bandwidth:** Lower (~1.5-2.0 TB/s on A100)
- **Latency:** Higher (~300-600 cycles)
- **Scope:** GPU-wide
- **Use:** Main storage, all data starts/ends here

## The Speed-Capacity Trade-off

**Key Insight:** Fast memory is small, slow memory is large [Dao et al., 2022, p.2].

| Memory Level | Capacity | Bandwidth | Relative Speed |
|--------------|----------|-----------|----------------|
| Registers | ~256KB | ~20 TB/s | 10-15× |
| SRAM | ~20MB | ~19 TB/s | 10× |
| L2 Cache | ~40MB | ~7 TB/s | 4× |
| HBM | 40-80GB | ~1.5-2 TB/s | 1× (baseline) |

**Implication:** Algorithms must be designed to keep data in fast memory (SRAM) as much as possible to achieve good performance.

## Memory Access Patterns

### Sequential Access
Reading/writing consecutive memory addresses is faster due to:
- **Coalescing:** Multiple threads' accesses combined into fewer transactions
- **Prefetching:** Hardware can predict and load ahead
- **Cache-friendly:** Better spatial locality

### Random Access
Reading/writing scattered addresses is slower due to:
- **No coalescing:** Each access may be a separate transaction
- **Poor prefetching:** Hardware can't predict patterns
- **Cache-unfriendly:** Poor spatial locality

**Performance difference:** Sequential access can be 10-100× faster than random access!

## The Memory Bottleneck

### Compute vs. Memory Bound

**Compute-bound operation:** Performance limited by how fast arithmetic operations can execute.

**Memory-bound operation:** Performance limited by how fast data can move through memory hierarchy [Dao et al., 2022, p.2].

**Modern reality:** Most deep learning operations are **memory-bound**, not compute-bound [Dao et al., 2022, p.2].

### Why Memory is the Bottleneck

Modern GPUs have enormous computational capacity:
- **A100 GPU:** ~312 TFLOPS (FP16)
- **Computation time for N² operations:** ~3μs for N=1024

But limited memory bandwidth:
- **A100 HBM bandwidth:** ~1.5-2.0 TB/s
- **Time to read N² values:** ~500μs for N=1024

**Result:** Reading data takes 100-200× longer than computing on it!

## Data Movement Costs

### Quantifying the Cost

For an N×N attention matrix (N=1024, FP16 precision) [Dao et al., 2022, p.2-3]:

**Storage:** N² × 2 bytes = 1024² × 2 = 2MB

**HBM read time:** 2MB / 1.5 TB/s ≈ 1.3μs

**SRAM read time:** 2MB / 19 TB/s ≈ 0.1μs

**Computation time (matmul):** 1024² operations / 312 TFLOPS ≈ 0.003μs

**Observation:** HBM read takes 400× longer than the actual computation!

### The 10× Rule

**Rule of thumb:** SRAM is roughly 10× faster than HBM [Dao et al., 2022, p.2].

**Design implication:** If an algorithm reduces HBM accesses by 10×, it can provide near 10× speedup (if memory-bound).

## Managing the Memory Hierarchy

### Programmer-Controlled: SRAM (Shared Memory)

In CUDA/GPU programming, SRAM (shared memory) is **explicitly managed** by the programmer:

```cuda
__shared__ float shared_data[BLOCK_SIZE];  // Explicitly allocated SRAM
```

**Programmer responsibilities:**
1. Explicitly allocate shared memory
2. Explicitly load data from HBM to SRAM
3. Synchronize threads accessing shared memory
4. Explicitly write results back to HBM

**Benefit:** Fine-grained control for maximum performance

**Challenge:** Requires careful programming to avoid errors

### Hardware-Controlled: L2 Cache

The L2 cache is **automatically managed** by hardware:
- No explicit allocation needed
- Hardware decides what to cache
- Transparent to programmer

**Benefit:** Easy to use, no extra code

**Limitation:** Less control, may not cache what you need

## Optimizing for the Memory Hierarchy

### Principle 1: Minimize HBM Accesses

**Strategy:** Keep data in SRAM as long as possible [Dao et al., 2022, p.3-4].

**Techniques:**
- Load data once, use multiple times (data reuse)
- Process in blocks (tiling) that fit in SRAM
- Fuse operations to avoid intermediate HBM writes
- Recompute instead of storing when beneficial

**Example:** FlashAttention reduces HBM accesses from Θ(N²) to Θ(N²d²M⁻¹) by tiling [Dao et al., 2022, p.3].

### Principle 2: Maximize Data Reuse

**Strategy:** Amortize the cost of loading data by doing as much work on it as possible.

**Techniques:**
- Block/tile algorithms: load block once, use in many operations
- Kernel fusion: combine multiple operations on same data
- Loop reordering: access same data while it's still in fast memory

### Principle 3: Coalesce Memory Accesses

**Strategy:** Structure memory access patterns so multiple threads read consecutive addresses.

**Benefit:** Hardware combines multiple accesses into one transaction.

**Techniques:**
- Organize data in memory to match access patterns
- Ensure threads in a warp access consecutive addresses
- Avoid strided or random access patterns

## Memory Capacity Constraints

### SRAM Size Limitations

**Problem:** SRAM is small (~20MB), many algorithms need more [Dao et al., 2022, p.2].

**Solution 1: Tiling**
- Split computation into tiles that fit in SRAM
- Process one tile at a time
- Iterate through all tiles

**Solution 2: Recomputation**
- Don't store all intermediate values
- Recompute them when needed
- Trade computation for memory

**Example:** FlashAttention doesn't store the N×N attention matrix (could be &gt;1GB for N=16K). Instead, it recomputes blocks of the matrix as needed during the backward pass [Dao et al., 2022, p.5].

### HBM Size Limitations

**Problem:** HBM is large but finite (40-80GB on A100).

**Solutions:**
- Gradient checkpointing: store only some activations, recompute others
- Model parallelism: split model across multiple GPUs
- Mixed precision: use FP16/BF16 instead of FP32
- Efficient attention: reduce O(N²) memory usage

## Practical Considerations

### Measuring Memory Bandwidth

Tools to measure actual memory bandwidth:
- **NVIDIA Profiler (Nsight Compute):** Detailed memory metrics
- **Achieved bandwidth:** Actual bandwidth utilized by kernel
- **Theoretical bandwidth:** Maximum possible bandwidth

**Metric:** Memory bandwidth utilization = (Achieved / Theoretical) × 100%

**Goal:** High-performance kernels should achieve &gt;80% of theoretical bandwidth

### Memory-Bound vs. Compute-Bound

**How to identify:**

**Memory-bound:**
- Low arithmetic intensity (ops per byte &lt; 10)
- Low GPU utilization (&lt;50%)
- Memory bandwidth near maximum
- Doubling memory bandwidth would nearly double speed

**Compute-bound:**
- High arithmetic intensity (ops per byte &gt; 100)
- High GPU utilization (&gt;80%)
- Memory bandwidth not saturated
- Doubling memory bandwidth wouldn't help

**Most attention operations are memory-bound** [Dao et al., 2022, p.2].

## Creative Analogy: The Factory Storage System

Imagine a manufacturing facility:

### Level 1: Worker's Hands (Registers)
- **Capacity:** Can hold 1-2 parts
- **Access time:** Instant (already in hand)
- **Use:** Currently being assembled

### Level 2: Workbench (SRAM)
- **Capacity:** Can hold 10-20 parts and tools
- **Access time:** 1 second (reach to bench)
- **Use:** Parts for current assembly task

### Level 3: Tool Cart (L2 Cache)
- **Capacity:** Can hold 50-100 items
- **Access time:** 10 seconds (walk to cart)
- **Use:** Recently used tools and parts (automatic)

### Level 4: Warehouse (HBM)
- **Capacity:** Thousands of items
- **Access time:** 5 minutes (walk to warehouse, find item, return)
- **Use:** All inventory

**Inefficient process:**
1. Walk to warehouse, get part A (5 min)
2. Walk back, attach to product (10 sec)
3. Walk to warehouse, get part B (5 min)
4. Walk back, attach to product (10 sec)
5. Total: 10 minutes of walking, 20 seconds of work!

**Efficient process:**
1. Walk to warehouse once, get all parts for next hour (5 min)
2. Place on workbench (10 sec)
3. Assemble from workbench for 1 hour (work continuously)
4. Return finished products to warehouse (5 min)
5. Total: 10 minutes of walking, 60 minutes of work!

**Key insight:** Minimize trips to the warehouse (HBM), keep everything you need on the workbench (SRAM).

## Impact on Algorithm Design

### Traditional Algorithm Analysis

**Focus:** Count number of operations (adds, multiplies)

**Assumption:** All operations cost the same

**Metric:** Big-O computational complexity (e.g., O(N²))

### Modern Algorithm Analysis (IO-Aware)

**Focus:** Count number of memory accesses between slow (HBM) and fast (SRAM) memory [Dao et al., 2022, p.3]

**Assumption:** Memory access dominates cost

**Metric:** IO complexity (e.g., Θ(N²) HBM accesses)

**Example:**
- Standard attention: O(N²d) operations, Θ(Nd + N²) HBM accesses
- FlashAttention: Same O(N²d) operations, Θ(N²d²M⁻¹) HBM accesses
- **Speedup:** Comes from reducing HBM accesses, not reducing operations!

## Real-World Performance Impact

### FlashAttention Example

On A100 GPU with sequence length N=1024 [Dao et al., 2022, p.8]:

**Standard attention:**
- HBM accesses: ~2MB (attention matrix)
- HBM bandwidth: ~1.5 TB/s
- Memory access time: ~1.3ms
- Compute time: ~0.01ms
- **Total time:** ~1.3ms (99% memory, 1% compute)

**FlashAttention:**
- HBM accesses: ~0.5MB (inputs/outputs only, no attention matrix)
- Memory access time: ~0.3ms
- Compute time: ~0.01ms (same)
- **Total time:** ~0.3ms (97% memory, 3% compute)

**Speedup:** 1.3ms / 0.3ms ≈ **4× faster**, purely from reducing memory accesses!

### Scaling to Long Sequences

For N=16K (long context) [Dao et al., 2022, p.12]:

**Standard attention:**
- Attention matrix: 16K × 16K × 2 bytes = 512MB
- Doesn't fit in HBM efficiently (with other activations)
- **Result:** Out of memory or extremely slow

**FlashAttention:**
- Never materializes full attention matrix
- HBM usage: ~32MB (just inputs/outputs)
- **Result:** Feasible, 2-4× faster than approximate methods

## Emerging Trends

### Larger SRAM

Newer GPUs are increasing on-chip memory:
- **A100:** ~20MB SRAM
- **H100:** ~50MB SRAM
- **Trend:** More SRAM enables larger tiles, fewer HBM accesses

### Memory-Compute Co-Design

Future architectures may bring compute closer to memory:
- Processing-in-memory (PIM)
- Near-data processing
- Reduces data movement costs fundamentally

## Related Concepts

- [IO-Aware Algorithms](/wiki/io-aware-algorithms)
- [Tiling Techniques](/wiki/tiling-techniques)
- [Kernel Fusion](/wiki/kernel-fusion)
- [Attention Mechanism](/wiki/attention-mechanism)

## References

- Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems (NeurIPS 2022).
- NVIDIA Corporation. (2020). NVIDIA A100 Tensor Core GPU Architecture. Technical Whitepaper.
- Jia, Z., Maggioni, M., Staiger, B., & Scarpazza, D. P. (2018). Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking. arXiv preprint arXiv:1804.06826.

</WikiLayout>
