import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Reward Modeling",
  description: "Training a model to predict human preferences between different outputs, used as a reward signal in reinforcement learning.",
  category: "Training Method",
  difficulty: "Advanced",
  citations: [
    {
      paper: "Training language models to follow instructions with human feedback",
      authors: "Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al.",
      year: "2022",
      pages: "6, 8-9"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

Reward modeling is the technique of training a model to predict which outputs humans prefer, then using those predictions as a reward signal for reinforcement learning [Ouyang et al., 2022, p. 8].

## The Two-Step Process

### Step 1: Collect Comparisons
Show humans multiple model outputs for the same input and have them rank them by preference:
- Output A vs Output B vs Output C vs Output D
- Human ranks: B &gt; D &gt; C &gt; A

### Step 2: Train the Reward Model
Train a model to predict these human preferences. Given an output, the reward model predicts a score representing how much a human would like it [Ouyang et al., 2022, p. 8].

## Why Not Direct Human Feedback?

Getting human ratings for every training sample is expensive and slow. The reward model learns to approximate human preferences, providing fast feedback for millions of training examples.

## The InstructGPT Reward Model

InstructGPT's reward model:
- Started from the supervised fine-tuned model
- Was trained on 33K comparison examples
- Predicted a scalar reward for each output
- Achieved high agreement with human preferences

[Ouyang et al., 2022, p. 8]

## Key Insight

The reward model doesn't need to be perfect—it just needs to guide the policy in the right direction. Even an imperfect reward model significantly improves model behavior [Ouyang et al., 2022, p. 9].

## Challenges

Reward modeling faces several challenges:
- Over-optimization: Models can exploit reward model weaknesses
- Distribution shift: Performance on novel inputs
- Capturing human values: What should the model learn?

InstructGPT addressed over-optimization using KL divergence penalties [Ouyang et al., 2022, p. 9].

## The Power of Preference Data

Preference comparisons are easier for humans than writing demonstrations, making reward modeling a scalable approach to alignment [Ouyang et al., 2022, p. 6].

---

**Related Concepts:** [RLHF](/wiki/rlhf) · [PPO](/wiki/ppo) · [AI Alignment](/wiki/ai-alignment) · [Instruction Following](/wiki/instruction-following)

</WikiLayout>

export default ({ children }) => children;
