import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Instruction Following",
  description: "The ability of a language model to understand and execute natural language instructions provided by users.",
  category: "Capability",
  difficulty: "Intermediate",
  citations: [
    {
      paper: "Training language models to follow instructions with human feedback",
      authors: "Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al.",
      year: "2022",
      pages: "1-4"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

Instruction following is the ability of a language model to understand and accurately execute instructions given in natural language, behaving helpfully and safely [Ouyang et al., 2022, p. 1].

## The Alignment Problem

Large language models trained on internet text learn to predict the next token, but this doesn't make them good at following user instructions. A model might:
- Generate plausible but incorrect information
- Produce toxic or harmful content
- Fail to follow the user's actual intent

Instruction following addresses this misalignment [Ouyang et al., 2022, p. 2].

## The InstructGPT Solution

InstructGPT showed that models can be taught to follow instructions through:
1. Collecting demonstrations of desired behavior
2. Training a reward model on human preferences
3. Fine-tuning with reinforcement learning

The result: A 1.3B InstructGPT was preferred to 175B GPT-3 [Ouyang et al., 2022, p. 1].

## What Makes Good Instruction Following?

According to InstructGPT's criteria, good instruction following means being:
- **Helpful**: Actually solving the user's task
- **Honest**: Not making up facts or misleading
- **Harmless**: Not causing harm to users or others

[Ouyang et al., 2022, p. 2]

## A Simple Example

**Poor instruction following:**
User: "Summarize this article in 2 sentences."
Model: *Writes 10 sentences*

**Good instruction following:**
User: "Summarize this article in 2 sentences."
Model: *Writes exactly 2 sentences summarizing key points*

## The Generalization Surprise

InstructGPT models could follow instructions in domains with little training data (like code, foreign languages), suggesting they learned a general concept of "following instructions" [Ouyang et al., 2022, p. 4].

---

**Related Concepts:** [RLHF](/wiki/rlhf) · [AI Alignment](/wiki/ai-alignment) · [Reward Modeling](/wiki/reward-modeling)

</WikiLayout>

export default ({ children }) => children;
