import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Tokenization",
  category: "Fundamentals",
  description: "The process of converting text into discrete units (tokens) that language models can process. Essential preprocessing step for all NLP systems.",
  relatedConcepts: ["byte-pair-encoding", "subword-tokenization", "vocabulary", "embeddings"],
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani et al.",
      year: "2017",
      pages: "5"
    },
    {
      paper: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      authors: "Devlin et al.",
      year: "2019",
      pages: "4"
    }
  ]
};

<WikiLayout {...meta}>

## What is Tokenization?

Tokenization is the process of breaking down text into smaller units called tokens. These tokens are the basic building blocks that language models operate on. Without tokenization, models would need to work directly with raw text, which is inefficient and difficult to learn from.

Think of tokenization like breaking speech into words. When you hear someone speak, your brain doesn't process one continuous stream—it segments the audio into discrete words that have meaning. Tokenization does the same thing for text, creating discrete units that a model can process.

## Why Tokenization Matters

Language models don't understand text directly. They work with numbers. Tokenization bridges this gap by:

1. **Converting text to numbers**: Each token maps to a unique integer ID
2. **Defining vocabulary**: The set of all possible tokens the model can process
3. **Handling unknown words**: Strategies for dealing with words not seen during training
4. **Balancing granularity**: Too coarse (word-level) misses patterns, too fine (character-level) loses meaning

## Tokenization Strategies

### Character-Level Tokenization

The simplest approach: each character is a token.

**Example:**
```
Text: "hello"
Tokens: ["h", "e", "l", "l", "o"]
```

**Advantages:**
- Tiny vocabulary (26 letters + punctuation + special chars)
- No unknown tokens
- Can represent any text

**Disadvantages:**
- Very long sequences
- Loses word-level semantics
- Harder to capture long-range dependencies

### Word-Level Tokenization

Split text by spaces and punctuation.

**Example:**
```
Text: "Hello, world!"
Tokens: ["Hello", ",", "world", "!"]
```

**Advantages:**
- Natural semantic units
- Shorter sequences than character-level
- Intuitive and interpretable

**Disadvantages:**
- Huge vocabulary (hundreds of thousands of words)
- Out-of-vocabulary problem: what about "supercalifragilisticexpialidocious"?
- No way to represent rare or new words
- Different forms of same word are separate (run, running, ran)

### Subword Tokenization

A middle ground: break words into meaningful subunits.

**Example:**
```
Text: "unhappiness"
Tokens: ["un", "happi", "ness"]
```

This approach balances vocabulary size and representation power. Common words stay intact, rare words split into recognizable pieces.

## Byte-Pair Encoding (BPE)

BPE is the most popular subword tokenization method, used in GPT, BERT, and most modern language models.

### How BPE Works

1. **Start with characters**: Initialize vocabulary with all individual characters
2. **Find most frequent pair**: Look at your training corpus and find the most common adjacent token pair
3. **Merge the pair**: Add the merged pair as a new token to vocabulary
4. **Repeat**: Continue merging until you reach desired vocabulary size

**Example training process:**

```
Corpus: "low", "low", "low", "lower", "newest"

Initial: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't']

Step 1: Most frequent pair is 'l' + 'o' → add 'lo'
Step 2: Most frequent pair is 'lo' + 'w' → add 'low'
Step 3: Most frequent pair is 'e' + 's' → add 'es'
Step 4: Most frequent pair is 'es' + 't' → add 'est'

Final vocabulary: ['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'lo', 'low', 'es', 'est']
```

### Applying BPE

Once trained, apply BPE greedily: always use the longest matching token from vocabulary.

```
Text: "lowest"
Process:
  - Check if "lowest" is in vocab → no
  - Check if "lowes" is in vocab → no
  - Check if "lowe" is in vocab → no
  - Check if "low" is in vocab → yes! Take it
  - Remaining: "est"
  - Check if "est" is in vocab → yes! Take it

Result: ["low", "est"]
```

The Transformer paper uses BPE: "We used byte-pair encoding with a shared source-target vocabulary of about 37000 tokens" [Vaswani et al., 2017, p. 5].

## WordPiece Tokenization

Similar to BPE but with a different merge criterion. Used in BERT.

Instead of merging the most frequent pair, WordPiece merges the pair that maximizes the likelihood of the training data.

BERT uses WordPiece: "We use WordPiece embeddings with a 30,000 token vocabulary" [Devlin et al., 2019, p. 4].

**Key difference from BPE:**
- BPE: Merge most frequent pair
- WordPiece: Merge pair that best explains training data (likelihood-based)

In practice, results are similar, but WordPiece can be slightly better at preserving meaningful subwords.

## Special Tokens

All tokenization schemes include special tokens for control:

- **`<PAD>`**: Padding token for batching sequences of different lengths
- **`<UNK>`**: Unknown token for words outside vocabulary
- **`<BOS>`/`<SOS>`**: Beginning/start of sequence
- **`<EOS>`**: End of sequence
- **`<SEP>`**: Separator between segments (used in BERT)
- **`<CLS>`**: Classification token (used in BERT)
- **`<MASK>`**: Mask token for masked language modeling

Example BERT input:
```
Text: "Hello world" and "How are you"
Tokenized: [<CLS>, "Hello", "world", <SEP>, "How", "are", "you", <SEP>]
```

## Vocabulary Size Trade-offs

Choosing vocabulary size is a key design decision:

**Small vocabulary (1,000-5,000 tokens):**
- Pros: Fewer parameters in embedding matrix, faster training
- Cons: Longer sequences, words split into many pieces

**Medium vocabulary (30,000-50,000 tokens):**
- Most common choice
- Good balance between sequence length and model size
- BERT uses 30K [Devlin et al., 2019, p. 4]
- GPT-2 uses ~50K

**Large vocabulary (100,000+ tokens):**
- Pros: Shorter sequences, common words stay intact
- Cons: Huge embedding matrix, many rare tokens

The Transformer paper chose 37K tokens for translation [Vaswani et al., 2017, p. 5], representing a practical balance.

## From Tokens to Embeddings

After tokenization, each token ID is mapped to a dense vector (embedding):

1. **Tokenize**: "Hello world" → [15496, 995]
2. **Embed**: Look up vectors in embedding matrix
   - Token 15496 → [0.23, -0.45, 0.67, ..., 0.12]  (d_model dimensions)
   - Token 995 → [-0.34, 0.89, -0.23, ..., 0.45]

These embeddings are learned during training and capture semantic meaning. Similar tokens have similar embeddings.

## Practical Considerations

### Case Sensitivity

Different approaches to capitalization:

- **Case-sensitive**: "Apple" and "apple" are different tokens
  - Larger vocabulary
  - Can distinguish proper nouns
- **Lowercase everything**: Normalize to lowercase before tokenization
  - Smaller vocabulary
  - Loses capitalization information

BERT uses case-sensitive (actually offers both cased and uncased models).

### Handling Whitespace

Subword tokenization needs to track word boundaries:

**Approach 1: Special marker**
```
Text: "playing"
Tokens: ["play", "##ing"]  # ## indicates continuation
```

**Approach 2: Leading space**
```
Text: "playing games"
Tokens: ["play", "ing", " games"]  # space indicates new word
```

GPT models use leading space approach. BERT uses ## continuation marker.

### Unicode and Multiple Languages

Modern tokenizers handle Unicode properly:
- Support for non-Latin scripts (Chinese, Arabic, etc.)
- Emoji and special characters
- Consistent handling across languages

## The Analogy: Digital Audio

Tokenization is like converting analog audio to digital:

- **Analog audio** = continuous waveform (like raw text)
- **Sampling** = taking discrete measurements (like tokenization)
- **Sample rate** = granularity (like token size: character vs subword vs word)
- **Quantization** = mapping to discrete values (like token IDs)

Just as audio sampling must balance quality (fine-grained) with file size (coarse-grained), tokenization must balance semantic granularity with computational efficiency.

Too coarse (word-level): misses morphological patterns, huge vocabulary
Too fine (character-level): loses semantic meaning, very long sequences
Just right (subword-level): captures patterns, manageable vocabulary, reasonable sequence length

## Implementation Tips

When building a tokenization system:

1. **Choose vocabulary size based on**:
   - Available compute (larger vocab = more parameters)
   - Language characteristics (morphologically rich languages benefit from subwords)
   - Task requirements (generation tasks prefer shorter sequences)

2. **Train tokenizer on diverse data**:
   - Should represent the domain you'll deploy on
   - Include rare but important terms
   - Balance across languages if multilingual

3. **Handle normalization carefully**:
   - Decide on case handling early
   - Consistent preprocessing (spaces, punctuation)
   - Unicode normalization (NFD, NFC, etc.)

4. **Test edge cases**:
   - Very long words
   - Multiple languages mixed
   - Special characters and emoji
   - Numbers and dates

## Key Takeaways

- **Tokenization bridges text and models**: Converts strings to numbers
- **Subword tokenization is dominant**: BPE and WordPiece offer best trade-offs
- **Vocabulary size matters**: Affects model size, sequence length, and performance
- **Special tokens provide control**: Padding, masking, separators, etc.
- **Learned during training**: Tokenizer and embeddings are trained together

Every language model starts with tokenization. Getting it right is crucial for everything that follows.

</WikiLayout>

export default ({ children }) => children;
