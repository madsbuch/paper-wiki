import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Tiling Techniques",
  description: "Algorithm optimization strategy that divides large computations into smaller blocks (tiles) that fit in fast memory, enabling efficient processing of problems larger than available cache",
  category: "Optimization & Efficiency",
  tags: ["algorithm-optimization", "memory", "blocking", "cache-optimization"],
  citations: [
    {
      paper: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      authors: "Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C.",
      year: "2022",
      pages: "4-5"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

**Tiling** (also called **blocking**) is an optimization technique that divides a large computation into smaller sub-problems called tiles or blocks, where each tile fits entirely in fast memory (e.g., GPU SRAM or CPU cache). By processing one tile at a time and maximizing data reuse within each tile, tiling dramatically reduces expensive accesses to slow memory [Dao et al., 2022, p.4].

## The Core Idea

### Problem: Data Doesn't Fit in Fast Memory

**Scenario:** You need to process a large dataset or matrix, but:
- Fast memory (SRAM/cache) is small (~20MB on A100 GPU)
- Your data is large (&gt;100MB, 1GB, or more)
- Processing requires random access to data (can't just stream through)

**Example:** Computing attention for sequence length N=4096:
- Attention matrix: 4096 × 4096 × 2 bytes = 32MB
- GPU SRAM: ~20MB
- **Problem:** Doesn't fit!

### Solution: Divide and Conquer

Instead of processing the entire problem at once, **break it into tiles** [Dao et al., 2022, p.4]:

1. **Divide:** Split the data into smaller blocks (tiles)
2. **Fit:** Each tile fits in fast memory
3. **Process:** Load one tile into fast memory, compute, store results
4. **Iterate:** Repeat for all tiles
5. **Combine:** Assemble final result from tile results

**Key benefit:** Each tile stays in fast memory throughout its processing, minimizing slow memory accesses.

## Tiling in FlashAttention

### The Standard Attention Problem

Standard attention computes [Dao et al., 2022, p.3]:
```
S = QK^T     # N×N matrix
P = softmax(S)
O = PV
```

For N=4096, sequence length:
- Q, K, V: 4096 × 64 = 0.5MB each (manageable)
- S, P: 4096 × 4096 = 32MB each (doesn't fit in SRAM)

### The Tiling Strategy

FlashAttention tiles Q, K, V into blocks [Dao et al., 2022, p.4-5]:

**Block sizes:**
- B_c = ⌈M/(4d)⌉ (columns, K/V blocks)
- B_r = min(⌈M/(4d)⌉, d) (rows, Q blocks)
- M = SRAM size

**Example:** With M=20MB, d=64:
- B_c = ⌈20MB/(4×64×2bytes)⌉ = 40,960
- B_r = min(40,960, 64) = 64

**Tiling pattern:**
- Split Q into T_r = ⌈N/B_r⌉ row blocks
- Split K, V into T_c = ⌈N/B_c⌉ column blocks
- Process Q_i with all K_j, V_j blocks

### The Algorithm Flow

For each block K_j, V_j (outer loop) [Dao et al., 2022, p.5]:
1. Load K_j, V_j from HBM to SRAM (size B_c × d)
2. For each block Q_i (inner loop):
   - Load Q_i from HBM to SRAM (size B_r × d)
   - **Compute S_ij = Q_i K_j^T in SRAM** (size B_r × B_c)
   - **Compute softmax block and accumulate output in SRAM**
   - Update output O_i in HBM
3. Move to next K_j, V_j block

**Key insight:** The full N×N attention matrix S never exists in memory—only one B_r × B_c tile at a time [Dao et al., 2022, p.4].

## Tile Size Selection

### The Trade-off

**Larger tiles:**
- ✓ Fewer tiles total → fewer memory reads of Q, K, V
- ✓ More computation per tile → better amortization
- ✗ May not fit in SRAM
- ✗ More computation before results are written

**Smaller tiles:**
- ✓ Always fit in SRAM
- ✓ Faster individual tile processing
- ✗ More tiles → more overhead
- ✗ More passes over data

### Optimal Tile Size

**Goal:** Maximize tile size while fitting in SRAM [Dao et al., 2022, p.4].

**Constraint:** For FlashAttention, all of these must fit in SRAM simultaneously:
- Q block: B_r × d
- K block: B_c × d
- V block: B_c × d
- S block: B_r × B_c
- Intermediate values (m, ℓ for softmax): B_r

**Total SRAM usage:** ~2B_r d + 2B_c d + B_r B_c + B_r

**Choice:** Set B_c = B_r to balance memory usage, then maximize to fill SRAM [Dao et al., 2022, p.4].

## Incremental Computation: Online Softmax

### The Softmax Challenge

**Problem:** Softmax requires seeing all values before normalizing:
```
softmax(x)_i = exp(x_i) / Σ_j exp(x_j)
```

With tiling, we only see one block of values at a time!

### The Solution: Online Softmax

FlashAttention uses **online softmax** to compute softmax incrementally across blocks [Dao et al., 2022, p.4]:

**Key idea:** Maintain running statistics that can be updated as new blocks arrive:
- **m:** Maximum value seen so far (for numerical stability)
- **ℓ:** Sum of exponentials (softmax denominator)

**Update rule:** When seeing new block with max m̃, sum ℓ̃:
```
m_new = max(m_old, m̃)
ℓ_new = exp(m_old - m_new) × ℓ_old + exp(m̃ - m_new) × ℓ̃
```

**Output update:**
```
O_new = [exp(m_old - m_new) × ℓ_old × O_old + exp(m̃ - m_new) × P̃ V] / ℓ_new
```

where P̃ = exp(S̃ - m̃).

**Result:** Can compute softmax(S) block-by-block without materializing full S [Dao et al., 2022, p.4].

## General Tiling Principles

### 1. Identify Data Reuse

**Question:** What data is accessed multiple times?

**Strategy:** Keep that data in fast memory (the tile) while it's being reused.

**Example in matrix multiplication C = AB:**
- Each element A[i,k] is reused N times (for all columns of B)
- Each element B[k,j] is reused N times (for all rows of A)
- **Tiling opportunity:** Load tile of A and tile of B, compute all interactions

### 2. Choose Tile Dimensions

**Factors to consider:**
- **Memory capacity:** Tiles must fit in SRAM
- **Data reuse:** Larger tiles → more reuse per load
- **Parallelism:** Tiles should enable concurrent processing
- **Memory alignment:** Respect hardware memory access patterns

**Common patterns:**
- **Square tiles:** Balanced dimensions (e.g., 64×64)
- **Rectangular tiles:** When one dimension has more reuse
- **1D tiles:** When processing is row-wise or column-wise

### 3. Minimize Tile Overhead

**Overhead sources:**
- Loading tiles from slow memory
- Storing tile results back to slow memory
- Setup/teardown for each tile

**Mitigation strategies:**
- Maximize tile size (fill SRAM)
- Process multiple tiles before writing results
- Prefetch next tile while processing current one
- Fuse operations to avoid intermediate stores

## Creative Analogy: Reading a Large Book

Imagine you need to summarize a 1,000-page book, but you can only focus on 10 pages at once (your "working memory" or fast memory).

### Naive Approach (No Tiling)

**Strategy:** Try to keep the entire book in your mind at once.

**Problem:** Impossible! You keep forgetting earlier sections, constantly flipping back, losing your place.

**Result:** Takes forever, extremely inefficient.

### Tiling Approach

**Strategy:** Break the book into 10-page chapters (tiles).

**Process:**
1. Read chapter 1 (10 pages) carefully, take notes
2. Summarize chapter 1 based on notes
3. Set chapter 1 aside
4. Read chapter 2 (10 pages) carefully, take notes
5. Summarize chapter 2, relate to chapter 1 summary
6. Continue for all chapters
7. Combine chapter summaries into final book summary

**Benefits:**
- Each chapter fits in your working memory
- You fully understand each chapter before moving on
- Notes allow you to combine information later
- Much faster and more accurate than constant page-flipping

**Key insight:** By working on manageable chunks (tiles) that fit in fast memory (working memory), you process the large problem (book) efficiently.

## Tiling Patterns

### Outer Product Tiling

**Use case:** Matrix multiplication C = AB

**Pattern:**
- Outer loop: tiles of C (output)
- Inner loop: tiles of A and B that contribute to each C tile

**Data flow:**
- Load tile of A (rows)
- Load tile of B (columns)
- Compute partial result for C tile
- Accumulate to C

### Inner Product Tiling

**Use case:** Dot products, attention

**Pattern:**
- Outer loop: output elements
- Inner loop: tiles of input vectors

**Data flow:**
- For each output element:
  - Load tiles of input vectors
  - Compute partial dot product
  - Accumulate result

### Block-Recursive Tiling

**Use case:** Hierarchical structures (e.g., hierarchical attention)

**Pattern:**
- Recursively divide problem into quarters/halves
- Process at multiple granularities
- Combine results up the hierarchy

## Tiling for Different Operations

### Matrix Multiplication

**Standard tiling:** Divide A, B, C into tiles, compute C_tile = Σ A_tile × B_tile

**IO complexity:** Θ(n³/√M) with optimal tiling vs. Θ(n³) without

### Attention (FlashAttention)

**Tiling strategy:** Outer loop over K/V tiles, inner loop over Q tiles [Dao et al., 2022, p.5]

**IO complexity:** Θ(N²d²M⁻¹) with tiling vs. Θ(N²) without [Dao et al., 2022, p.3]

### Convolution

**Tiling strategy:** Tile input and kernel, compute output tile from overlapping input tiles

**Benefit:** Reuse input values for multiple output locations

### Sorting

**Tiling strategy:** Divide array into tiles, sort each tile, merge sorted tiles

**IO complexity:** Θ(N/B log_{M/B}(N/B)) with optimal tiling

## Combining Tiling with Other Techniques

### Tiling + Kernel Fusion

**Technique:** Fuse multiple operations within each tile [Dao et al., 2022, p.4].

**Example in FlashAttention:**
- Within each tile: matmul + softmax + matmul
- All intermediate values stay in SRAM
- Only final output written to HBM

**Benefit:** Reduces memory accesses even further

### Tiling + Recomputation

**Technique:** Don't store intermediate tiles, recompute them when needed [Dao et al., 2022, p.5].

**Trade-off:**
- Saves memory: no need to store all intermediate tiles
- Costs computation: must recompute tiles
- **When beneficial:** If computation is cheap but storage is expensive

**Example in FlashAttention backward pass:**
- Don't store full attention matrix S (would require Θ(N²) memory)
- Recompute tiles of S as needed using saved Q, K (requires Θ(Nd) memory)
- Net savings: Θ(N²) → Θ(Nd) memory usage

### Tiling + Prefetching

**Technique:** Load next tile into SRAM while processing current tile.

**Benefit:** Overlaps memory transfer with computation, hiding latency.

**Requirement:** Sufficient SRAM to hold both current and next tile.

## Performance Analysis

### Memory Access Reduction

**Standard attention (no tiling):**
- Materialize S: write N² values to HBM
- Read S for softmax: read N² values from HBM
- Materialize P: write N² values to HBM
- Read P for output: read N² values from HBM
- **Total:** 4N² HBM accesses

**FlashAttention (with tiling):**
- Load Q, K, V: read 3Nd values from HBM
- Load Q, K, V tiles multiple times: read T_c × T_r tiles
- Write O: write Nd values to HBM
- **Total:** Θ(N²d²M⁻¹) HBM accesses [Dao et al., 2022, p.3]

**Speedup:** For d=64, M=20MB, N=1024: ~10× fewer HBM accesses

### Computational Overhead

**Question:** Does tiling add extra computation?

**Answer:** No! Same number of arithmetic operations, just reordered.

**Key insight:** Tiling changes **memory access patterns**, not the **computation** [Dao et al., 2022, p.4].

**Benefit:** All speedup comes from reduced memory access, with no computational trade-off.

## Implementation Considerations

### CUDA Implementation

**Shared memory allocation:**
```cuda
__shared__ float tile_Q[BLOCK_SIZE][HEAD_DIM];
__shared__ float tile_K[BLOCK_SIZE][HEAD_DIM];
__shared__ float tile_S[BLOCK_SIZE][BLOCK_SIZE];
```

**Loading tiles:**
```cuda
// Cooperatively load tile from global memory (HBM) to shared memory (SRAM)
tile_Q[threadIdx.x][threadIdx.y] = Q[block_idx * BLOCK_SIZE + threadIdx.x][threadIdx.y];
__syncthreads();  // Ensure entire tile is loaded before processing
```

**Processing tiles:**
```cuda
// All operations on tile happen in shared memory (fast)
for (int k = 0; k < BLOCK_SIZE; k++) {
    tile_S[threadIdx.x][threadIdx.y] += tile_Q[threadIdx.x][k] * tile_K[threadIdx.y][k];
}
__syncthreads();
```

### Choosing Block Sizes

**Hardware constraints:**
- SRAM size (e.g., 20MB on A100)
- Maximum threads per block (e.g., 1024 on modern GPUs)
- Memory alignment requirements (e.g., 128-byte alignment)

**Software constraints:**
- Problem dimensions (e.g., head dimension d in attention)
- Parallelism requirements (multiple blocks running concurrently)

**Tuning approach:**
1. Start with maximum block size that fits in SRAM
2. Profile performance
3. Adjust based on occupancy and bandwidth utilization
4. Iterate

## Related Concepts

- [IO-Aware Algorithms](/wiki/io-aware-algorithms)
- [GPU Memory Hierarchy](/wiki/gpu-memory-hierarchy)
- [Kernel Fusion](/wiki/kernel-fusion)
- [Attention Mechanism](/wiki/attention-mechanism)
- [Operator Splitting Methods](/wiki/operator-splitting)

## References

- Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems (NeurIPS 2022).
- Lam, M. D., Rothberg, E. E., & Wolf, M. E. (1991). The cache performance and optimizations of blocked algorithms. ACM SIGPLAN Notices, 26(4), 63-74.
- Frigo, M., Leiserson, C. E., Prokop, H., & Ramachandran, S. (1999). Cache-oblivious algorithms. In Proceedings of the 40th Annual Symposium on Foundations of Computer Science (FOCS).

</WikiLayout>
