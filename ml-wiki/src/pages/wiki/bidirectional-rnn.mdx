import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Bidirectional RNN",
  category: "Architecture Components",
  description: "A recurrent neural network that processes sequences in both forward and backward directions, allowing each position to have context from both past and future.",
  relatedConcepts: ["encoder-decoder", "attention-mechanism", "seq2seq", "soft-alignment"],
  citations: [
    {
      paper: "Neural Machine Translation by Jointly Learning to Align and Translate",
      authors: "Bahdanau, D., Cho, K., & Bengio, Y.",
      year: "2014",
      pages: "3"
    }
  ]
};

<WikiLayout {...meta}>

## What is a Bidirectional RNN?

A Bidirectional RNN (BiRNN) processes a sequence in two directions simultaneously: forward (left-to-right) and backward (right-to-left). The outputs from both directions are then combined, giving each position access to context from both the past and the future.

Think of it like reading a mystery novel twice: once from beginning to end to see how events unfold, and once from end to beginning to see how everything connects. After both readings, you understand each scene in the context of what came before AND what comes after.

## The Motivation

Standard RNNs only see the past:

"In conventional RNNs, the hidden state hj at time j depends only on the previous hidden state hj−1 and the input xj" [Bahdanau et al., 2014, p. 3].

But in many tasks, especially language understanding, future context is equally important. Consider:

- "The bank was **steep**" (river bank)
- "The bank was **closed**" (financial bank)

To understand "bank" in position 2, you need to see the adjective in position 3. A forward-only RNN wouldn't have that information yet.

## How Bidirectional RNNs Work

A BiRNN runs two separate RNNs over the same sequence:

### Forward RNN (Left-to-Right)

Processes the sequence from start to end:

```
→h1 = f(→h0, x1)
→h2 = f(→h1, x2)
→h3 = f(→h2, x3)
...
→hT = f(→hT-1, xT)
```

Each forward hidden state →hj contains information about positions 1 through j (the past).

### Backward RNN (Right-to-Left)

Processes the sequence from end to start:

```
←hT = f(←hT+1, xT)
←hT-1 = f(←hT, xT-1)
←hT-2 = f(←hT-1, xT-2)
...
←h1 = f(←h2, x1)
```

Each backward hidden state ←hj contains information about positions j through T (the future).

### Combining Directions

The final annotation for position j combines both:

```
hj = [→hj ; ←hj]
```

This is typically concatenation, creating a vector that contains both past and future context.

"We propose to use a bidirectional RNN (BiRNN), which has been successfully used recently in speech recognition. A BiRNN consists of forward and backward RNNs. The forward RNN →f reads the input sequence as it is ordered (from x1 to xT) and calculates a sequence of forward hidden states. The backward RNN ←f reads the sequence in the reverse order (from xT to x1), resulting in a sequence of backward hidden states" [Bahdanau et al., 2014, p. 3].

## The Complete Picture

For a sequence "The cat sat":

**Forward pass:**
```
→h1 ("The") = f(→h0, "The")           → knows: "The"
→h2 ("cat") = f(→h1, "cat")           → knows: "The", "cat"
→h3 ("sat") = f(→h2, "sat")           → knows: "The", "cat", "sat"
```

**Backward pass:**
```
←h3 ("sat") = f(←h4, "sat")           → knows: "sat"
←h2 ("cat") = f(←h3, "cat")           → knows: "cat", "sat"
←h1 ("The") = f(←h2, "The")           → knows: "The", "cat", "sat"
```

**Combined annotations:**
```
h1 = [→h1 ; ←h1]  → knows: "The" + ("cat", "sat")
h2 = [→h2 ; ←h2]  → knows: ("The", "cat") + ("sat")
h3 = [→h3 ; ←h3]  → knows: ("The", "cat", "sat") + nothing ahead
```

Now h2 for "cat" has context from both "The" (before) and "sat" (after).

## Why BiRNNs Are Powerful

### 1. Full Context at Every Position

Each hidden state has information from the entire sequence:

"We would like the annotation of each word to summarize not only the preceding words, but also the following words. Hence we propose to use a bidirectional RNN" [Bahdanau et al., 2014, p. 3].

### 2. Better Representations

With both directions, the model can resolve ambiguities:
- Word sense disambiguation (bank = river or finance?)
- Coreference resolution (pronouns)
- Syntactic parsing (understanding sentence structure)

### 3. Rich Encodings for Attention

When used as an encoder with attention, BiRNN provides rich annotations:

"By doing so, the annotation hj contains the summaries of both the preceding words and the following words" [Bahdanau et al., 2014, p. 3].

The attention mechanism can then focus on positions that have complete context, making better alignment decisions.

## Architecture Details

### Hidden State Dimensions

If each direction has hidden dimension d:
- Forward RNN: →hj ∈ ℝᵈ
- Backward RNN: ←hj ∈ ℝᵈ
- Combined: hj ∈ ℝ²ᵈ (concatenation doubles the size)

This is why BiRNN encodings are typically twice as large as unidirectional RNN encodings.

### Independent RNNs

The forward and backward RNNs are independent:
- Separate parameters (weights and biases)
- Trained jointly but process independently
- No information flow between them during the forward pass

They only interact when their outputs are concatenated.

### RNN Cell Types

Any RNN variant can be used for both directions:
- Vanilla RNN cells
- LSTM cells (most common)
- GRU cells (used in Bahdanau et al., 2014)

The choice affects capacity and training dynamics, but the bidirectional structure remains the same.

## BiRNN in the Encoder

In sequence-to-sequence models, BiRNN is typically used in the encoder:

```
Input: x1, x2, ..., xT

Encoder (BiRNN):
  For each position j:
    →hj = →f(→hj-1, xj)    (forward)
    ←hj = ←f(←hj+1, xj)    (backward)
    hj = [→hj ; ←hj]        (combine)

Decoder (unidirectional):
  Uses encoder annotations {h1, h2, ..., hT}
  Generates output left-to-right
```

The decoder is still unidirectional because it generates sequentially (can't look into the future during generation).

## A Concrete Example

Encoding "Je suis étudiant" (I am a student):

**Forward RNN:**
```
→h1 (Je):       [0.2, 0.5, -0.3, ...]  ← knows: "Je"
→h2 (suis):     [0.4, -0.1, 0.7, ...]  ← knows: "Je suis"
→h3 (étudiant): [0.1, 0.3, -0.5, ...]  ← knows: "Je suis étudiant"
```

**Backward RNN:**
```
←h1 (Je):       [0.8, -0.2, 0.4, ...]  ← knows: "Je suis étudiant"
←h2 (suis):     [-0.3, 0.6, 0.1, ...]  ← knows: "suis étudiant"
←h3 (étudiant): [0.5, 0.2, -0.1, ...]  ← knows: "étudiant"
```

**Combined BiRNN:**
```
h1 = [0.2, 0.5, -0.3, ..., 0.8, -0.2, 0.4, ...]  ← both directions
h2 = [0.4, -0.1, 0.7, ..., -0.3, 0.6, 0.1, ...]
h3 = [0.1, 0.3, -0.5, ..., 0.5, 0.2, -0.1, ...]
```

Now when the attention mechanism looks at h2 ("suis"), it has information about:
- What came before: "Je"
- What comes after: "étudiant"

This helps align "suis" correctly with "am" in English.

## Training BiRNNs

### Forward and Backward Pass

Training requires two forward passes (one per direction) and backpropagation through both:

**Forward pass:**
1. Compute forward RNN: →h1, →h2, ..., →hT
2. Compute backward RNN: ←hT, ←hT-1, ..., ←h1
3. Concatenate: hj = [→hj ; ←hj] for all j
4. Use in decoder/task
5. Compute loss

**Backward pass (gradient computation):**
1. Gradients flow to both →hj and ←hj
2. Backpropagate through forward RNN (right to left in time)
3. Backpropagate through backward RNN (left to right in time)
4. Update both sets of parameters

### Memory Requirements

BiRNNs require storing all hidden states:
- Can't process streaming (need entire sequence)
- Memory: O(T × d) for both directions
- About twice the memory of unidirectional RNN

### Computational Cost

- Two RNN passes instead of one
- Approximately 2× computation of unidirectional RNN
- Can't parallelize over sequence length (still sequential)

## BiRNN vs Alternatives

### Unidirectional RNN
- Only past context
- Can process streaming
- Half the parameters
- Worse representations for understanding tasks

### BiRNN
- Full context (past + future)
- Requires entire sequence
- Double the parameters
- Better representations
- Gold standard for encoders

### Transformer (Self-Attention)
- Full context in one pass
- Parallel computation
- No sequential bottleneck
- Even better representations
- Eventually replaced BiRNN in most applications

## When to Use BiRNN

**Good for:**
- Encoding text for classification (sentiment, NER, etc.)
- Sequence-to-sequence encoders (translation, summarization)
- Feature extraction for downstream tasks
- Any task where full sequence is available upfront

**Not suitable for:**
- Real-time processing (need to see future)
- Autoregressive generation (decoder)
- Streaming applications
- When you must generate left-to-right

**Modern alternative:**
- Transformers largely replaced BiRNNs
- Self-attention gives similar benefits with parallelization
- But BiRNN is still conceptually important and used in some applications

## The Analogy: Writing a Book Review

Imagine writing a review after reading a book:

**Unidirectional (forward only):**
- You write notes while reading, page by page
- Your understanding of chapter 3 only includes chapters 1-3
- You don't know how the story ends yet
- Your interpretation might change later

**Bidirectional (read both ways):**
- You read the entire book first
- Then you analyze each chapter knowing the full story
- Your understanding of chapter 3 includes:
  - Setup from chapters 1-2 (forward context)
  - Consequences in chapters 4-10 (backward context)
- Your analysis is informed by the complete narrative

BiRNN is like reading the whole book before writing your review, so every comment has full context.

## Key Takeaways

- **BiRNN processes sequences in both directions**: Forward (past) and backward (future)
- **Each position gets full context**: Knows what came before and what comes after
- **Concatenation combines directions**: Typically [→hj ; ←hj], doubling dimension
- **Better representations**: Resolves ambiguities and captures dependencies
- **Essential for encoders**: Standard choice before Transformers
- **Requires full sequence**: Can't process streaming or incomplete sequences
- **Foundation for modern NLP**: Influenced Transformer design and attention mechanisms

Bidirectional RNNs were a key innovation that showed the power of using full sequence context. While Transformers have largely superseded them in modern applications, the core insight—that both past and future context matter—remains fundamental to how we design language models.

</WikiLayout>

export default ({ children }) => children;
