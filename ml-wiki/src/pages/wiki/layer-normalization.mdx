import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Layer Normalization",
  category: "Architecture Components",
  description: "A normalization technique that standardizes activations across features within each layer, stabilizing training and enabling deeper networks.",
  relatedConcepts: ["residual-connections", "transformer-architecture", "batch-normalization", "training-stability"],
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani et al.",
      year: "2017",
      pages: "3-4"
    },
    {
      paper: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      authors: "Devlin et al.",
      year: "2019",
      pages: "4"
    }
  ]
};

<WikiLayout {...meta}>

## What is Layer Normalization?

Layer normalization is a technique that normalizes the activations in a neural network layer by computing the mean and variance across all features for each individual example. It helps stabilize training, especially in deep networks like Transformers.

Think of it like adjusting the brightness and contrast of an image. Each layer in a neural network receives inputs that might have wildly different scales. Layer normalization adjusts these inputs to have consistent statistics (mean of 0, standard deviation of 1), making them easier for the next layer to process.

## The Mathematical Formula

For each example in a batch, layer norm computes:

```
μ = (1/H) Σ x_i           (mean across features)
σ² = (1/H) Σ (x_i - μ)²   (variance across features)

y_i = γ * (x_i - μ) / √(σ² + ε) + β
```

Where:
- `x_i` is the i-th feature/dimension of the input
- `H` is the total number of features (hidden dimension)
- `μ` is the mean across all features
- `σ²` is the variance across all features
- `ε` is a small constant (e.g., 1e-5) for numerical stability
- `γ` and `β` are learned parameters (scale and shift)

The key insight: **normalize across features for each example independently**, not across examples.

## Why Layer Normalization Works

### Training Stability

Deep networks suffer from internal covariate shift: as lower layers change during training, the distribution of inputs to higher layers keeps changing. This makes training unstable and slow.

Layer normalization addresses this by ensuring each layer receives inputs with consistent statistics, regardless of what's happening in earlier layers.

### Gradient Flow

Normalization helps gradients flow more smoothly through the network:

- **Without normalization**: Large activations in one layer can cause exploding gradients
- **With normalization**: Activations stay in a reasonable range, gradients remain stable

This is crucial for very deep networks like Transformers, which can have 12, 24, or even 96 layers.

### Enables Larger Learning Rates

With stable activations and gradients, you can use larger learning rates, speeding up training significantly.

## Layer Norm vs Batch Norm

Layer normalization is related to but different from batch normalization:

### Batch Normalization
- Normalizes across the batch dimension
- Computes mean/variance over all examples in the batch for each feature
- Requires batch statistics
- Works well for CNNs with large batches
- Problematic for small batches or sequence models

### Layer Normalization
- Normalizes across the feature dimension
- Computes mean/variance over all features for each example
- Independent of batch size
- Works well for RNNs and Transformers
- Consistent between training and inference

**Visual comparison:**

```
Batch Norm (features × batch):
         Example 1  Example 2  Example 3  Example 4
Feature 1:  [normalize across this dimension]
Feature 2:  [normalize across this dimension]
Feature 3:  [normalize across this dimension]

Layer Norm (features × batch):
                Example 1  Example 2  Example 3  Example 4
Feature 1:         |          |          |          |
Feature 2:     normalize  normalize  normalize  normalize
Feature 3:      across     across     across     across
                 this       this       this       this
```

## Usage in Transformers

The Transformer architecture uses layer normalization extensively:

**In the original Transformer:**
"We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized" [Vaswani et al., 2017, p. 4].

The structure is:
```
x' = x + Dropout(Sublayer(x))
output = LayerNorm(x')
```

This is called **post-norm**: normalize after the residual connection.

**In BERT and modern Transformers:**
Many modern models use **pre-norm**:
```
output = x + Dropout(Sublayer(LayerNorm(x)))
```

BERT uses layer normalization throughout: "We use gelu activation rather than the standard relu, following OpenAI GPT. We note that we could equivalently use relu; the difference is small" [Devlin et al., 2019, p. 4].

Pre-norm tends to be more stable for very deep networks, though post-norm sometimes achieves slightly better performance.

## Where Layer Norm Appears

In a Transformer encoder layer:

1. **After self-attention**:
   ```
   x' = x + SelfAttention(x)
   x_norm = LayerNorm(x')
   ```

2. **After feed-forward network**:
   ```
   x'' = x_norm + FeedForward(x_norm)
   output = LayerNorm(x'')
   ```

This pattern repeats in every Transformer layer. With 12 layers, that's 24 layer norm operations per forward pass.

## The Learned Parameters

Layer norm has two learned parameter vectors:

### Scale (γ, gamma)
- Multiplies normalized activations
- Default: all 1s (no scaling)
- Learned during training
- Allows model to expand normalized range if needed

### Shift (β, beta)
- Added to normalized activations
- Default: all 0s (no shift)
- Learned during training
- Allows model to shift the mean if needed

These parameters give the model flexibility to "undo" normalization if that's beneficial for the task.

**Example:**
```
After normalization: [-1.2, 0.3, 0.9]  (mean=0, std=1)
Learned γ = [2.0, 1.5, 1.0]
Learned β = [0.5, -0.2, 0.8]

Output = γ * normalized + β
       = [2.0*(-1.2)+0.5, 1.5*0.3+(-0.2), 1.0*0.9+0.8]
       = [-1.9, 0.25, 1.7]
```

The model learns optimal γ and β for each layer and dimension.

## Implementation Details

### Epsilon for Numerical Stability

The formula includes `ε` (epsilon) in the denominator:
```
y = (x - μ) / √(σ² + ε)
```

Without `ε`, if variance is zero (all inputs identical), you'd divide by zero. Epsilon prevents this:
- Typical value: 1e-5 or 1e-6
- Small enough not to affect normal cases
- Large enough to prevent numerical issues

### Computing Statistics

For a hidden size of 768 (like BERT):

```
Input shape: (batch_size, sequence_length, 768)

For each position in each example:
  μ = mean over 768 dimensions
  σ² = variance over 768 dimensions

Normalize each of the 768 values using those statistics
```

This is computed independently for every position in every sequence in the batch.

### Computational Cost

Layer norm is relatively cheap:
- Two passes over features: one for mean, one for variance
- Element-wise operations: subtract, divide, scale, shift
- Much faster than attention mechanisms

For a 768-dimensional layer:
- ~768 additions for mean
- ~768 multiplications for variance
- ~768 normalizations
- Total: ~3K simple ops (negligible compared to attention's matrix multiplications)

## The Analogy: Standardizing Test Scores

Imagine students taking tests in different subjects:

**Without normalization:**
- Math test: scores 0-100
- History test: scores 0-50
- English test: scores 0-200

Comparing raw scores is meaningless because scales differ.

**With normalization (z-score):**
- Convert each score to "standard deviations from mean"
- Math score 75 → z-score +1.2 (above average)
- History score 30 → z-score +0.8 (above average)
- English score 120 → z-score -0.5 (below average)

Now scores are comparable. Layer normalization does the same for neural network activations: it puts everything on a common scale, making them easier to work with.

## Why Not Other Normalization Schemes?

**Instance Normalization:**
- Used in style transfer for images
- Similar to layer norm but per-channel
- Not common in NLP

**Group Normalization:**
- Divide features into groups, normalize within groups
- Balance between layer norm and instance norm
- Occasionally used in vision Transformers

**RMSNorm (Root Mean Square Normalization):**
- Simplified version: skips computing mean, only uses RMS
- Slightly faster than layer norm
- Used in some recent large models

Layer normalization won for Transformers because:
1. Works well with varying sequence lengths
2. Consistent between training and inference (no batch statistics)
3. Simple and effective
4. Well-suited for sequential processing

## Common Pitfalls

### 1. Forgetting Epsilon
**Problem**: Division by zero when variance is zero
**Solution**: Always add small epsilon (1e-5)

### 2. Wrong Normalization Dimension
**Problem**: Normalizing across wrong axis (e.g., across batch instead of features)
**Solution**: For layer norm, always normalize across the feature/hidden dimension

### 3. Position of Layer Norm
**Problem**: Placing layer norm incorrectly relative to residual connections
**Solution**:
- Post-norm: `LayerNorm(x + Sublayer(x))`
- Pre-norm: `x + Sublayer(LayerNorm(x))`
Choose one and be consistent

### 4. Not Initializing γ and β
**Problem**: Random initialization of scale and shift parameters
**Solution**: Initialize γ=1 and β=0 (so norm starts as standard normalization)

## When to Use Layer Normalization

**Good for:**
- Transformer models (encoders and decoders)
- Recurrent networks (RNNs, LSTMs)
- Any architecture processing sequences
- Models with small batch sizes
- Inference with single examples

**Not necessary for:**
- Very shallow networks (&lt; 3 layers)
- Networks that already train stably
- When batch norm works well (CNNs with large batches)

## Key Takeaways

- **Layer norm stabilizes training**: Keeps activations in reasonable range
- **Normalizes across features, not examples**: Independent of batch size
- **Essential for Transformers**: Appears twice in every encoder/decoder layer
- **Learned parameters γ and β**: Give model flexibility to adjust normalized values
- **Enables deeper networks**: Crucial for 12+ layer architectures like BERT
- **Pre-norm vs post-norm**: Different placement relative to residual connections

Layer normalization is one of the unsung heroes of modern NLP. It's not glamorous like attention mechanisms, but without it, training deep Transformers would be far more difficult—or impossible.

</WikiLayout>

export default ({ children }) => children;
