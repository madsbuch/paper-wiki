import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Positional Encoding",
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.",
      year: "2017",
      pages: "5-6"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

Since the Transformer contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, it must inject some information about the relative or absolute position of tokens in the sequence. This is accomplished through **positional encodings** added to the input embeddings at the bottoms of the encoder and decoder stacks [Vaswani et al., 2017, p. 6].

## Why Positional Encoding Is Necessary

Unlike RNNs, which inherently process sequences in order (position 1, then 2, then 3...), the Transformer's self-attention mechanism processes all positions simultaneously in parallel. This is great for speed but creates a problem: the model has no inherent notion of sequence order!

Without positional encoding, the sentence "The cat chased the mouse" would be indistinguishable from "The mouse chased the cat" or "cat The mouse chased the"—the model would see the same words but have no idea about their order.

## The Sinusoidal Approach

The Transformer uses sine and cosine functions of different frequencies [Vaswani et al., 2017, p. 6]:

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

Where:
- **pos**: Position of the token in the sequence
- **i**: Dimension index
- **d_model**: Dimension of the model (512 in the base Transformer)

This means:
- Even dimensions (2i) use sine functions
- Odd dimensions (2i+1) use cosine functions
- Different dimensions use different frequencies

## Properties of Sinusoidal Encoding

### 1. Unique Representation

Each position gets a unique positional encoding vector. Position 0 has one pattern, position 1 has another, position 100 has yet another—all distinct [Vaswani et al., 2017, p. 6].

### 2. Relative Position Learning

The wavelengths form a geometric progression from 2π to 10000 · 2π. The authors hypothesized this would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos} [Vaswani et al., 2017, p. 6].

This means the model can learn patterns like "attend to the word 3 positions back" without hardcoding specific positions.

### 3. Extrapolation Capability

Sinusoidal encodings may allow the model to extrapolate to sequence lengths longer than those encountered during training—a critical advantage for generalization [Vaswani et al., 2017, p. 6].

## Learned vs. Sinusoidal Encodings

The paper also experimented with learned positional embeddings instead of fixed sinusoidal functions. Both versions produced nearly identical results [Vaswani et al., 2017, p. 6, Table 3 row E].

The sinusoidal version was chosen because:
- It may allow extrapolation to longer sequences
- It requires no learned parameters (more parameter-efficient)
- It works just as well in practice

## Implementation Details

- Positional encodings have the same dimension as the embeddings (d_model = 512)
- They are **added** to the token embeddings (not concatenated)
- The same positional encodings are used at the bottom of both encoder and decoder stacks [Vaswani et al., 2017, p. 6]

---

## The Theater Seating Analogy: A Story

Imagine you're directing a play, and you have 100 actors on stage for a massive ensemble scene. Each actor wears a costume that identifies their character (this is like the token embedding—it tells you "who" they are).

But there's a problem: you also need to know "where" each actor stands, because position matters! The person standing front-center should be treated differently from someone in the back corner, even if they're wearing the same costume.

**The Positional Encoding Solution**: You give each position on stage a unique colored spotlight pattern. These spotlights don't change the actors' costumes (character identity) but add information about location:

- **Front-left corner**: Illuminated by slowly pulsing red and quickly oscillating blue
- **Center stage**: Medium-pulsing green and medium-oscillating yellow
- **Back-right**: Quickly pulsing red and slowly oscillating blue

Each position has its own unique "light signature"—a combination of different pulsing frequencies that can never be confused with any other position.

**Why Sine/Cosine Waves?**

Think of the spotlights as rotating color wheels spinning at different speeds:
- Some dimensions (spotlight colors) spin very slowly (long wavelengths): these help distinguish nearby vs. far-away positions
- Others spin very quickly (short wavelengths): these help distinguish adjacent positions
- The combination creates a unique "fingerprint" for each location

**The Magic Property**: Because these are smooth mathematical functions, positions near each other have similar patterns. Position 5's lights look similar to position 6's lights—just shifted slightly. This helps the model learn that "nearby positions are related," which is crucial for understanding language.

When an actor at position 10 needs to interact with the actor at position 7, the model can learn to recognize the 3-position offset from the pattern of how their light signatures differ, regardless of their absolute positions. This is the power of positional encoding: making position information accessible in a learnable, extrapolatable way.

</WikiLayout>

export default ({ children }) => children;
