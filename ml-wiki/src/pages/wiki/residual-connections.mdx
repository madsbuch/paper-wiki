import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Residual Connections",
  category: "Architecture Components",
  description: "Skip connections that allow gradients to flow directly through deep networks by adding layer inputs to their outputs. Critical for training very deep architectures.",
  relatedConcepts: ["layer-normalization", "transformer-architecture", "identity-mapping", "bottleneck-architecture"],
  citations: [
    {
      paper: "Deep Residual Learning for Image Recognition",
      authors: "He, K., Zhang, X., Ren, S., & Sun, J.",
      year: "2015",
      pages: "1-6"
    },
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani et al.",
      year: "2017",
      pages: "3-4"
    },
    {
      paper: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      authors: "Devlin et al.",
      year: "2019",
      pages: "3-4"
    }
  ]
};

<WikiLayout {...meta}>

## What are Residual Connections?

A residual connection (also called skip connection or shortcut connection) is a direct path that adds a layer's input to its output. Instead of computing `output = Layer(input)`, you compute `output = input + Layer(input)`.

This simple addition has profound consequences: it enables training networks with dozens or hundreds of layers, which would be impossible otherwise.

## The Core Idea

```
Traditional layer:
input → [Layer] → output

Residual connection:
input → [Layer] → (+) → output
  ↓__________________|
  (skip connection)
```

The mathematical formula is simple:
```
output = x + F(x)
```

Where:
- `x` is the input to the layer
- `F(x)` is the transformation applied by the layer (e.g., attention, feed-forward)
- `output` is the sum of both

The layer learns to compute a **residual** (the difference needed), rather than the full output.

## Why Residual Connections Work

### The Vanishing Gradient Problem

In deep networks, gradients must flow backward through many layers during training. Each layer multiplies the gradient by its local gradient. With many layers:

```
gradient = ∂L/∂layer₁ × ∂layer₁/∂layer₂ × ... × ∂layerₙ₋₁/∂layerₙ
```

If any of these terms is less than 1 (which is common), the product shrinks exponentially. After 20-30 layers, gradients become vanishingly small—the network can't learn because updates to early layers are negligible.

### How Residuals Help

The residual connection creates a direct gradient path:

```
Without residual:
∂output/∂input = ∂F(x)/∂x

With residual:
∂output/∂input = 1 + ∂F(x)/∂x
```

The "+1" term ensures gradients can always flow backward, even if `∂F(x)/∂x` is small. This prevents vanishing gradients.

**Intuition:** Think of it like a highway with both local roads and an express lane. Gradients can take the express lane (skip connection) to flow directly through the network, or take local roads (through the layer) to make detailed adjustments.

### Learning Identity Mappings

Another benefit: if the optimal output equals the input (identity mapping), the layer can simply learn `F(x) = 0`.

**Without residual:**
- Layer must learn `output = input` precisely
- Difficult because network is initialized randomly

**With residual:**
- Layer learns `F(x) = 0` (easy: push weights toward zero)
- Output automatically equals input via skip connection

This makes optimization easier. The network can choose to "do nothing" if that's optimal.

## Usage in Transformers

The Transformer architecture uses residual connections around every sub-layer.

### Encoder Structure

Each encoder layer has two sub-layers, each with a residual connection:

```
1. Self-Attention with Residual:
   x' = x + SelfAttention(x)
   x_norm = LayerNorm(x')

2. Feed-Forward with Residual:
   x'' = x_norm + FeedForward(x_norm)
   output = LayerNorm(x'')
```

From the Transformer paper: "We employ a residual connection around each of the two sub-layers, followed by layer normalization" [Vaswani et al., 2017, p. 3].

### Decoder Structure

The decoder has three sub-layers, each with residuals:

```
1. Masked Self-Attention with Residual:
   x' = x + MaskedSelfAttention(x)
   x_norm1 = LayerNorm(x')

2. Encoder-Decoder Attention with Residual:
   x'' = x_norm1 + EncoderDecoderAttention(x_norm1, encoder_output)
   x_norm2 = LayerNorm(x'')

3. Feed-Forward with Residual:
   x''' = x_norm2 + FeedForward(x_norm2)
   output = LayerNorm(x''')
```

With 6 encoder layers and 6 decoder layers, a Transformer has **36 residual connections**.

## Residuals + Layer Normalization

Transformers combine residual connections with layer normalization. Two common patterns:

### Post-Norm (Original Transformer)

```
output = LayerNorm(x + Sublayer(x))
```

Normalize after adding the residual. This is what the original Transformer paper used: "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized" [Vaswani et al., 2017, p. 4].

### Pre-Norm (Modern Transformers)

```
output = x + Sublayer(LayerNorm(x))
```

Normalize before the sublayer, then add residual. BERT and most modern models use this because it's more stable for very deep networks.

**Why pre-norm is more stable:**
- Normalized activations go through the sublayer (prevents explosion)
- Original input adds directly (preserves gradient flow)
- Easier to train models with 24+ layers

## The Analogy: Document Editing

Imagine you're editing a document:

**Traditional approach (no residual):**
- Rewrite each paragraph completely from scratch
- Risk losing important information
- Hard to make small refinements

**Residual approach:**
- Start with the original paragraph
- Make edits (additions, deletions, changes)
- Final version = original + edits

The residual connection is like "track changes" mode: you always have the original, and you're learning what **changes** to make, not creating the entire output from scratch.

If the original is already good, you can make minimal changes (F(x) ≈ 0). If significant rewriting is needed, you can learn large changes (F(x) large). The network has flexibility.

## Implementation Details

### Dimension Requirements

For `output = x + F(x)` to work, dimensions must match:
- `x`: shape (batch, seq_len, d_model)
- `F(x)`: shape (batch, seq_len, d_model)

If dimensions don't match, you need a projection:
```
output = x + Projection(F(x))
```

In Transformers, careful design ensures dimensions always match, so simple addition works.

### Dropout

The Transformer applies dropout to `F(x)` before adding:
```
output = LayerNorm(x + Dropout(F(x)))
```

This randomly zeros some elements of the transformation, providing regularization while still allowing gradient flow through the residual.

### Initialization

With residuals, initialization becomes less critical because gradients can flow regardless. But good initialization still helps:

- Initialize layer weights such that `F(x)` is initially small
- This makes early training behave like identity mappings
- Network gradually learns to deviate as needed

## Gradient Flow Analysis

Let's see how gradients flow with residuals:

**Forward pass:**
```
Layer 1: x₁ = x₀ + F₁(x₀)
Layer 2: x₂ = x₁ + F₂(x₁)
Layer 3: x₃ = x₂ + F₃(x₂)
...
Layer N: xₙ = xₙ₋₁ + Fₙ(xₙ₋₁)
```

**Backward pass (chain rule):**
```
∂xₙ/∂x₀ = ∂xₙ/∂xₙ₋₁ × ∂xₙ₋₁/∂xₙ₋₂ × ... × ∂x₁/∂x₀

Since xᵢ = xᵢ₋₁ + Fᵢ(xᵢ₋₁):
∂xᵢ/∂xᵢ₋₁ = 1 + ∂Fᵢ/∂xᵢ₋₁

Therefore:
∂xₙ/∂x₀ = (1 + ∂Fₙ/∂xₙ₋₁) × (1 + ∂Fₙ₋₁/∂xₙ₋₂) × ... × (1 + ∂F₁/∂x₀)
```

The "+1" terms ensure the product never vanishes, even if the ∂F terms are small. This is why residuals enable training deep networks.

## ResNet: The Origin of Residual Learning

Residual connections were introduced in the ResNet (Residual Networks) paper by He et al. in 2015, specifically to solve the **degradation problem** in very deep neural networks.

### The Degradation Problem

Before ResNet, researchers discovered something puzzling: deeper networks performed **worse** than shallower ones, even on training data.

"When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error" [He et al., 2015, p. 1].

This was counterintuitive. If a 56-layer network has higher training error than a 20-layer network, something is fundamentally wrong with optimization—not generalization.

**Why degradation happens:**

Theoretically, a deeper model should do at least as well as a shallower one. The deeper model could learn identity mappings for the extra layers, effectively becoming the shallower model. But experiments showed solvers couldn't find these solutions.

"There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution" [He et al., 2015, p. 1-2].

### The Residual Learning Solution

ResNet's key insight: instead of learning H(x) directly, learn the residual F(x) = H(x) - x.

"We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions" [He et al., 2015, p. 2].

**The hypothesis:** It's easier to optimize the residual mapping than the original mapping. If an identity mapping were optimal, it's easier to push the residual to zero than to fit an identity mapping with nonlinear layers.

**The implementation:**

```
y = F(x, W) + x
```

Where:
- x is the input
- F(x, W) is the residual function (typically 2-3 conv layers) with parameters W
- The addition is element-wise
- A ReLU activation is applied after the addition

"The formulation of F(x) + x can be realized by feedforward neural networks with 'shortcut connections'. Shortcut connections are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers" [He et al., 2015, p. 2].

### ResNet Results

The impact was dramatic:

**ImageNet Classification:**
- 152-layer ResNet (8× deeper than VGG-16)
- 3.57% top-5 error on ImageNet test set
- Won 1st place at ILSVRC 2015
- Lower complexity than VGG-16 despite being much deeper

**CIFAR-10:**
- Successfully trained networks with 100 and even 1,202 layers
- 110-layer ResNet achieved 6.43% error

"The 34-layer ResNet reduces the top-1 error by 3.5%, resulting from the successfully reduced training error. This comparison verifies the effectiveness of residual learning on extremely deep systems" [He et al., 2015, p. 6].

### ResNet Architecture Patterns

ResNet uses residual connections around pairs of convolutional layers:

**Basic Block (ResNet-18, ResNet-34):**
```
x → [3×3 conv, 64] → [ReLU] → [3×3 conv, 64] → (+) → [ReLU] → output
↓_______________________________________________|
```

Two 3×3 convolutions with a skip connection around both.

**Bottleneck Block (ResNet-50, ResNet-101, ResNet-152):**
```
x → [1×1 conv, 64] → [ReLU] → [3×3 conv, 64] → [ReLU] → [1×1 conv, 256] → (+) → [ReLU] → output
↓__________________________________________________________________|
```

Three layers: 1×1 reduces dimensions, 3×3 processes at reduced dimension (the "bottleneck"), 1×1 restores dimensions.

"For each residual function F, we use a stack of 3 layers instead of 2. The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions" [He et al., 2015, p. 6].

**Why bottleneck design:**
- Reduces parameters and computation
- Makes very deep networks (50+ layers) practical
- Identity shortcuts become even more important (no extra parameters)

### Projection Shortcuts

When dimensions change (e.g., spatial size halves, channels double), the skip connection needs adjustment:

**Option A: Zero-padding**
Pad with zeros to match dimensions. No extra parameters.

**Option B: Projection**
Use 1×1 convolution to project to the correct dimension:
```
y = F(x, W) + W_s·x
```

ResNet experiments showed projection shortcuts slightly improve accuracy, but identity shortcuts with zero-padding work nearly as well and are more efficient.

"The parameter-free identity shortcuts are particularly important for the bottleneck architectures. If the identity shortcut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled" [He et al., 2015, p. 6].

### What Residual Functions Learn

ResNet's analysis showed that residual functions tend to have small responses:

"We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning" [He et al., 2015, p. 3].

This validates the hypothesis: when identity mapping is close to optimal, it's easier to learn small residuals (F(x) ≈ 0) than to learn the full function.

### ResNet vs Plain Networks

He et al. compared "plain" networks (no shortcuts) with ResNets:

**Plain Networks (no residuals):**
- 34-layer plain net has HIGHER training error than 18-layer plain net
- Deeper is worse, even on training data
- Demonstrates the degradation problem

**ResNets (with residuals):**
- 34-layer ResNet has LOWER training error than 18-layer ResNet
- Deeper is better
- Degradation problem solved

"The situation is reversed with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting" [He et al., 2015, p. 5].

### Impact on Computer Vision

ResNet revolutionized computer vision:

1. **Depth became practical**: 50, 101, 152+ layers trainable
2. **State-of-the-art everywhere**: ImageNet classification, detection, localization, segmentation
3. **Universal adoption**: Residual connections became standard across CV
4. **Foundation for Transformers**: The pattern migrated to NLP

"Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation" [He et al., 2015, p. 1].

## From ResNet to Transformers

The residual connection concept from ResNet directly influenced the Transformer architecture:

**ResNet (2015):**
- Residual connections around convolutional blocks
- Enables 100+ layer vision networks
- Batch normalization after convolutions

**Transformer (2017):**
- Residual connections around attention and feedforward blocks
- Enables deep sequence models (6-24+ layers)
- Layer normalization instead of batch normalization

The core principle remained: add the input to the transformation's output, enabling deep architectures through superior gradient flow.

## Highway Networks vs Residuals

Highway networks are a related concept with gated skip connections:
```
output = g × F(x) + (1 - g) × x
```

Where `g` is a learned gate (0 to 1) controlling how much of the transformation vs skip to use.

**Comparison:**
- **Highway**: Learned gating, more parameters, more flexible
- **Residual**: Simple addition, no extra parameters, works just as well

Residuals won in practice: simpler, no extra parameters, just as effective.

## Dense Connections (DenseNet)

An extension of residuals: concatenate all previous layer outputs:
```
Layer 1: x₁ = F₁(x₀)
Layer 2: x₂ = F₂([x₀, x₁])
Layer 3: x₃ = F₃([x₀, x₁, x₂])
```

**Benefits:**
- Even stronger gradient flow
- Feature reuse across layers

**Downsides:**
- Memory intensive (concatenation grows)
- More complex

Residuals strike a better balance for most applications.

## When Residuals Matter Most

**Critical for:**
- Very deep networks (20+ layers)
- Networks with multiple processing stages (encoder + decoder)
- Architectures with complex transformations (attention, convolutions)

**Less critical for:**
- Shallow networks (&lt; 5 layers)
- Networks that already train stably
- Simple feedforward networks with few layers

**In Transformers:** Absolutely essential. Without residuals, training a 12-layer BERT would be nearly impossible.

## Common Pitfalls

### 1. Dimension Mismatch
**Problem:**
```python
output = x + F(x)  # Error if shapes don't match
```

**Solution:**
- Design network so dimensions match throughout
- Or add projection layer: `output = x + Project(F(x))`

### 2. Forgetting to Add Input
**Problem:** Implementing only `F(x)` without adding `x`

**Solution:** Always explicitly add the skip connection

### 3. Wrong Order with Normalization
**Problem:** Unclear whether to normalize before or after adding

**Solution:** Pick one pattern and be consistent:
- Post-norm: `LayerNorm(x + F(x))`
- Pre-norm: `x + F(LayerNorm(x))`

### 4. Not Applying Dropout Correctly
**Problem:** Applying dropout to `x` instead of `F(x)`

**Solution:** Only dropout the transformation: `x + Dropout(F(x))`

## Key Insights from Research

The Transformer paper describes the structure: "We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x))" [Vaswani et al., 2017, p. 3].

BERT uses the same pattern across all its layers: "The number of layers (i.e., Transformer blocks) is L, the hidden size is H, and the number of self-attention heads is A. We primarily report results on two model sizes: BERT_BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT_LARGE (L=24, H=1024, A=16, Total Parameters=340M)" [Devlin et al., 2019, p. 4].

These deep models (12-24 layers) would be impossible to train without residual connections.

## Key Takeaways

- **Residuals enable depth**: Allow training networks with dozens of layers
- **Simple but powerful**: Just addition, but transforms what's possible
- **Gradient superhighway**: Direct path for gradients to flow backward
- **Identity mapping**: Easy to learn "do nothing" when that's optimal
- **Universal in Transformers**: Appears around every sub-layer
- **Works with layer norm**: Combined pattern is standard in modern architectures

Residual connections are one of the most important innovations in deep learning. They're simple conceptually but essential practically. Without them, the Transformer revolution—and everything built on top of it—would never have happened.

</WikiLayout>

export default ({ children }) => children;
