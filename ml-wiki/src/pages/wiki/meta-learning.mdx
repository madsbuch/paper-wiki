import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Meta-Learning",
  citations: [
    {
      paper: "Language Models are Few-Shot Learners (GPT-3)",
      authors: "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., et al.",
      year: "2020",
      pages: "6"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

**Meta-learning**, in the context of language models, refers to the ability of models to learn how to learn - developing capabilities during pre-training that enable rapid adaptation to new tasks at inference time without weight updates [Brown et al., 2020, p. 6].

## Structure

Meta-learning in language models has an **outer-loop-inner-loop structure** [Brown et al., 2020, p. 6]:

### Outer Loop
- **Language model pre-training** on a broad distribution of tasks
- Tasks are implicit in the pre-training data
- Weights are updated via gradient descent during unsupervised pre-training
- The model learns general patterns and task recognition abilities

### Inner Loop
- **Rapid adaptation** to a new task at inference time
- Happens through in-context examples
- **No weight updates** - adaptation occurs purely through the forward pass
- The model recognizes the task pattern from examples and applies learned knowledge

## Connection to In-Context Learning

The GPT-3 paper shows that larger models are **more proficient meta-learners** [Brown et al., 2020, p. 6]. Evidence for this includes:

1. The gap between zero-shot, one-shot, and few-shot performance grows with model capacity
2. Larger models show stronger in-context learning abilities
3. The 175B parameter model demonstrates much better task adaptation than smaller models

## Evidence of Meta-Learning

The finding that "the gap between zero-, one-, and few-shot performance often grows with model capacity" suggests that larger models better learn the meta-skill of adapting to tasks from examples [Brown et al., 2020, p. 6].

## Relationship to Broader ML

Few-shot learning in language models is related to few-shot learning in other ML contexts [Brown et al., 2020, p. 6]. Both involve:

1. **Learning from a broad distribution**: Exposure to many different types of tasks
2. **Rapid adaptation**: Quickly adjusting to new tasks with minimal examples

However, language model meta-learning is unique in that:
- The task distribution is implicit in natural language pre-training data
- Adaptation happens through context rather than explicit meta-learning algorithms
- No architectural changes or special meta-learning procedures are required

---

## The Master Craftsperson Story

Imagine two types of expertise:

**Specialist (Traditional Learning)**: Spent 10,000 hours making one specific type of chair. Can make that chair perfectly but struggles when asked to make a different furniture piece. Must retrain extensively for each new type of furniture.

**Master Craftsperson (Meta-Learner)**: Spent 10,000 hours making thousands of different furniture pieces - chairs, tables, cabinets, beds, shelves. Now, when shown 2-3 examples of a new furniture type they've never made before, they can create it reasonably well.

**The Meta-Skill**: The master hasn't just learned to make furniture - they've learned *how to learn* new furniture types. They've internalized:
- How to recognize patterns from examples
- What features matter for different furniture types
- How to adapt their general woodworking knowledge to new contexts

This is meta-learning: during the "outer loop" (years of diverse practice), they developed the "inner loop" skill of rapidly adapting to new furniture types from just a few examples.

In GPT-3, the outer loop is pre-training on diverse text, and the inner loop is in-context learning from a few demonstrations.

## Related Concepts

- [In-Context Learning](/wiki/in-context-learning)
- [Few-Shot Learning](/wiki/few-shot-learning)

</WikiLayout>

export default ({ children }) => children;
