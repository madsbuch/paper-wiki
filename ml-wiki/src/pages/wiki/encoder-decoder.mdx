import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "Encoder-Decoder Architecture",
  citations: [
    {
      paper: "Attention Is All You Need",
      authors: "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.",
      year: "2017",
      pages: "2-3"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

Most competitive neural sequence transduction models have an **encoder-decoder structure**. The encoder maps an input sequence of symbol representations (x₁, ..., xₙ) to a sequence of continuous representations z = (z₁, ..., zₙ). Given z, the decoder then generates an output sequence (y₁, ..., yₘ) of symbols one element at a time. At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next [Vaswani et al., 2017, p. 2].

## The Two-Stage Process

### Stage 1: Encoding

The **encoder** processes the entire input sequence and creates a continuous representation:
- Input: Sequence of tokens/symbols (e.g., words in a sentence)
- Output: Sequence of continuous vectors that capture the meaning and context
- Processing: All input positions are processed in parallel

The encoder's job is to understand and represent the input in a form the decoder can use.

### Stage 2: Decoding

The **decoder** generates the output sequence one token at a time:
- Input: The encoder's continuous representation + previously generated output tokens
- Output: Next token in the sequence
- Processing: Sequential and auto-regressive

## Auto-Regressive Generation

A crucial property of the decoder is that it's **auto-regressive**: when generating position i, it can only depend on the known outputs at positions less than i [Vaswani et al., 2017, p. 3].

This means:
- Generating the 1st output token uses only the encoder's representation
- Generating the 2nd token uses the encoder representation + the 1st token
- Generating the 3rd token uses the encoder representation + the 1st and 2nd tokens
- And so on...

To enforce this, the decoder's self-attention is modified with **masking** to prevent positions from attending to subsequent positions [Vaswani et al., 2017, p. 3].

## The Transformer's Implementation

### Encoder Structure

The Transformer encoder consists of a stack of N = 6 identical layers. Each layer has [Vaswani et al., 2017, p. 3]:
1. **Multi-head self-attention mechanism**
2. **Position-wise feed-forward network**
3. **Residual connections** around each sub-layer
4. **Layer normalization** after each sub-layer

All sub-layers produce outputs of dimension d_model = 512.

### Decoder Structure

The Transformer decoder also has N = 6 identical layers. Each layer has [Vaswani et al., 2017, p. 3]:
1. **Masked multi-head self-attention** (over previous decoder outputs)
2. **Multi-head attention over encoder output** (encoder-decoder attention)
3. **Position-wise feed-forward network**
4. **Residual connections** and **layer normalization**

The key difference: the decoder has an extra attention sub-layer that attends to the encoder's output, allowing it to focus on relevant parts of the input when generating each output token.

## Why This Architecture?

The encoder-decoder structure with attention provides several benefits:

1. **Separation of Concerns**: The encoder focuses on understanding the input, the decoder on generating the output
2. **Variable-Length I/O**: Can handle input and output sequences of different lengths
3. **Contextual Generation**: Decoder can selectively attend to relevant input positions
4. **Parallel Encoding**: Input can be processed in parallel
5. **Flexible Generation**: Output generation can condition on full input context

## Applications

Encoder-decoder architectures excel at **sequence transduction** tasks, including:
- Machine translation (e.g., English → German)
- Text summarization (long text → short summary)
- Question answering (question + context → answer)
- Speech recognition (audio → text)
- Image captioning (image → text description)

The WMT 2014 English-to-German translation results demonstrate the power of this architecture: the Transformer achieved 28.4 BLEU, improving over previous best results by over 2.0 BLEU [Vaswani et al., 2017, p. 1].

---

## The Translator Analogy: A Story

Imagine you're a professional translator working with two specialists: a reader (encoder) and a writer (decoder).

### The Reader (Encoder)

The **reader** receives a document in German and carefully studies the entire text:
- They read every sentence, understanding context, idioms, and relationships
- They make detailed notes about what each part means
- They identify which words relate to which concepts
- They create a comprehensive internal representation of the document's meaning

Crucially, the reader examines the **entire document at once**, making connections between the beginning, middle, and end. They understand that the pronoun "sie" in paragraph 5 refers back to "die Frau" mentioned in paragraph 2.

### The Writer (Decoder)

The **writer** creates the English translation **one sentence at a time**:
- They start with the reader's notes (encoder output)
- For each new English sentence they write:
  - They review what they've already written (previous output tokens)
  - They consult the reader's notes, focusing on the relevant German passages
  - They write the next sentence that flows naturally from what came before

**The Auto-Regressive Nature**: The writer can't see "future" English sentences they haven't written yet—they can only look at what's already been written. This is enforced by masking in the decoder's self-attention. They build the translation incrementally, where each sentence depends on the previous ones.

**The Cross-Attention Link**: When writing sentence 5 in English, the writer might need to focus heavily on German paragraph 3 (where the relevant information is). This selective focus is the encoder-decoder attention mechanism—the writer queries the reader's notes to find what's most relevant for the current output.

### Why Two Separate Roles?

You might ask: why not have one person do both jobs?

The separation provides key advantages:
- **Parallel Understanding**: The reader can process the entire German document simultaneously, understanding all contexts at once
- **Sequential Fluency**: The writer generates English one piece at a time, ensuring grammatical flow and coherence
- **Flexible Attention**: The writer can selectively consult any part of the reader's notes as needed, focusing on different input sections for different output sections

This is the encoder-decoder architecture: two complementary processes that together transform one sequence into another, combining global understanding with sequential generation.

</WikiLayout>

export default ({ children }) => children;
