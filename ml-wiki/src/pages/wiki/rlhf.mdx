import WikiLayout from "../../components/WikiLayout";

export const meta = {
  title: "RLHF",
  description: "Reinforcement Learning from Human Feedback - using human preferences as a reward signal to align language models.",
  category: "Training Method",
  difficulty: "Advanced",
  citations: [
    {
      paper: "Training language models to follow instructions with human feedback",
      authors: "Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al.",
      year: "2022",
      pages: "1-3, 6"
    }
  ]
};

<WikiLayout title={meta.title} citations={meta.citations}>

## Overview

RLHF (Reinforcement Learning from Human Feedback) is a technique that uses human preferences as a reward signal to train language models to behave in helpful, honest, and harmless ways [Ouyang et al., 2022, p. 2].

## The Three-Step Process

InstructGPT pioneered a three-step RLHF approach [Ouyang et al., 2022, p. 6]:

### Step 1: Supervised Fine-Tuning (SFT)
Collect demonstrations of desired behavior and fine-tune the model on them. Labelers write high-quality responses to prompts, and the model learns to imitate these responses [Ouyang et al., 2022, p. 6].

### Step 2: Reward Model Training
Collect comparison data where labelers rank multiple model outputs. Train a reward model to predict which outputs humans prefer [Ouyang et al., 2022, p. 8].

### Step 3: RL Optimization
Use the reward model as a reward function and fine-tune the SFT model using PPO (Proximal Policy Optimization) to maximize the predicted reward [Ouyang et al., 2022, p. 9].

## Why RLHF Works

RLHF solves a key problem: it's easier for humans to compare and rank outputs than to write perfect demonstrations from scratch. Comparison data scales better than demonstration data [Ouyang et al., 2022, p. 6].

## The Results

InstructGPT models trained with RLHF:
- Were preferred to 100x larger GPT-3 models
- Made up facts less often
- Generated less toxic output
- Better followed user intentions
[Ouyang et al., 2022, p. 1]

## A Training Analogy

Think of RLHF like training a chef:
1. **SFT**: Show them how to cook a few dishes (demonstrations)
2. **Reward Model**: Teach them what good food tastes like (human preferences)
3. **RL**: Let them practice and improve using taste as feedback (optimization)

## Challenges

RLHF faces important challenges:
- Reward model quality depends on human labeler agreement
- Can over-optimize on the reward model
- Expensive to collect human feedback
- Alignment to whose values?

## Impact on Modern AI

RLHF became the foundation for ChatGPT and modern instruction-following models, establishing the standard approach for making AI systems helpful, honest, and harmless [Ouyang et al., 2022, p. 1].

---

**Related Concepts:** [Reward Modeling](/wiki/reward-modeling) · [PPO](/wiki/ppo) · [Instruction Following](/wiki/instruction-following) · [AI Alignment](/wiki/ai-alignment)

</WikiLayout>

export default ({ children }) => children;
